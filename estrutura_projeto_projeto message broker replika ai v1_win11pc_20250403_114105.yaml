.:
  .env.example:
    caminho_completo: .\.env.example
    numero_de_linhas: 32
    tamanho: 0.00 MB
  .flake8:
    caminho_completo: .\.flake8
    numero_de_linhas: 51
    tamanho: 0.00 MB
  .gitignore:
    caminho_completo: .\.gitignore
    numero_de_linhas: 60
    tamanho: 0.00 MB
  .pre-commit-config.yaml:
    caminho_completo: .\.pre-commit-config.yaml
    numero_de_linhas: 79
    tamanho: 0.00 MB
    yaml_info:
      numero_de_linhas: 79
      tamanho: 0.00 MB
  ARQUITETURA.md:
    caminho_completo: .\ARQUITETURA.md
    numero_de_linhas: 102
    tamanho: 0.00 MB
  CHANGELOG.md:
    caminho_completo: .\CHANGELOG.md
    numero_de_linhas: 93
    tamanho: 0.00 MB
  CONTRIBUTING.md:
    caminho_completo: .\CONTRIBUTING.md
    numero_de_linhas: 147
    tamanho: 0.00 MB
  LICENSE:
    caminho_completo: .\LICENSE
    numero_de_linhas: 21
    tamanho: 0.00 MB
  NOTAS.md:
    caminho_completo: .\NOTAS.md
    numero_de_linhas: 135
    tamanho: 0.00 MB
  README.md:
    caminho_completo: .\README.md
    numero_de_linhas: 95
    tamanho: 0.00 MB
  coleta-mensagem-v1.py:
    caminho_completo: .\coleta-mensagem-v1.py
    classes: []
    functions:
    - docstring: Cria o diretório de saída se não existir.
      end_lineno: 42
      lineno: 34
      name: setup_output_directory
    - docstring: Gera um nome de arquivo JSON único.
      end_lineno: 49
      lineno: 44
      name: generate_output_filename
    - docstring: Salva os dados coletados em um arquivo JSON.
      end_lineno: 62
      lineno: 51
      name: save_data_to_json
    - docstring: Função chamada ao receber sinal de parada (como Ctrl+C).
      end_lineno: 70
      lineno: 64
      name: handle_shutdown
    - docstring: Faz login usando a sessão para obter um token e configura na sessão.
      end_lineno: 99
      lineno: 72
      name: get_access_token
    - docstring: Verifica se a fila alvo existe.
      end_lineno: 121
      lineno: 101
      name: check_queue_exists
    - docstring: Tenta consumir uma mensagem e, se bem-sucedido, faz o ACK.
      end_lineno: 200
      lineno: 123
      name: consume_and_ack_message
    imports:
    - asname: null
      name: requests
    - asname: null
      name: json
    - asname: null
      name: warnings
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: datetime
    - asname: null
      name: os
    - asname: null
      name: hashlib
    - asname: null
      name: signal
    numero_de_linhas: 303
    source_code: "# -*- coding: utf-8 -*-\nimport requests\nimport json\nimport warnings\n\
      import sys\nimport time\nimport datetime\nimport os\nimport hashlib\nimport\
      \ signal # Para lidar com Ctrl+C de forma mais explícita se necessário\n\n\n\
      \n# --- Configurações ---\nBASE_URL = \"https://localhost:8777\"\nQUEUE_NAME\
      \ = \"minha-fila-teste-stress\" # <<< IMPORTANTE: Use a mesma fila do teste\
      \ de stress\nUSERNAME = \"admin\"\nPASSWORD = \"admin\"\nOUTPUT_DIR = \"test-json-data-collector-validation\"\
      \ # Pasta para salvar os JSONs\nEMPTY_QUEUE_DELAY_SECONDS = 0.5 # Tempo de espera\
      \ se a fila estiver vazia\nREQUEST_TIMEOUT = 10 # Timeout para requisições API\n\
      # --- Fim das Configurações ---\n\n# Ignorar avisos sobre certificados SSL autoassinados\n\
      warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n\n\
      # --- Globais ---\ncollected_message_contents = [] # Lista para armazenar o\
      \ conteúdo das mensagens\nsession = requests.Session() # Usar sessão para reutilizar\
      \ conexões\nsession.verify = False # Ignora verificação SSL para toda a sessão\n\
      keep_running = True # Flag para controlar o loop principal\n# --- Fim Globais\
      \ ---\n\ndef setup_output_directory():\n    \"\"\"Cria o diretório de saída\
      \ se não existir.\"\"\"\n    try:\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\n\
      \        print(f\"\U0001F4C2 Diretório de saída '{OUTPUT_DIR}' verificado/criado.\"\
      )\n        return True\n    except OSError as e:\n        print(f\"❌ Erro crítico:\
      \ Não foi possível criar o diretório de saída '{OUTPUT_DIR}': {e}\")\n     \
      \   return False\n\ndef generate_output_filename() -> str:\n    \"\"\"Gera um\
      \ nome de arquivo JSON único.\"\"\"\n    timestamp = datetime.datetime.now().strftime(\"\
      %Y%m%d_%H%M%S\")\n    unique_hash = hashlib.sha1(str(os.getpid()).encode() +\
      \ str(time.time()).encode()).hexdigest()[:8]\n    filename = f\"collected_data_{QUEUE_NAME}_{timestamp}_{unique_hash}.json\"\
      \n    return os.path.join(OUTPUT_DIR, filename)\n\ndef save_data_to_json(filename:\
      \ str):\n    \"\"\"Salva os dados coletados em um arquivo JSON.\"\"\"\n    global\
      \ collected_message_contents\n    print(f\"\\n\U0001F4BE Salvando {len(collected_message_contents)}\
      \ mensagens coletadas em '{filename}'...\")\n    try:\n        with open(filename,\
      \ 'w', encoding='utf-8') as f:\n            json.dump(collected_message_contents,\
      \ f, indent=2, ensure_ascii=False)\n        print(f\"✅ Dados salvos com sucesso.\"\
      )\n    except IOError as e:\n        print(f\"❌ Erro ao salvar dados no arquivo\
      \ '{filename}': {e}\")\n    except Exception as e:\n        print(f\"\U0001F4A5\
      \ Erro inesperado ao salvar JSON: {e}\")\n\ndef handle_shutdown(signum=None,\
      \ frame=None):\n    \"\"\"Função chamada ao receber sinal de parada (como Ctrl+C).\"\
      \"\"\n    global keep_running\n    if not keep_running: # Evita chamadas múltiplas\n\
      \        return\n    print(\"\\n\U0001F6A6 Recebido sinal de parada. Iniciando\
      \ desligamento gracioso...\")\n    keep_running = False # Sinaliza para o loop\
      \ principal parar\n\ndef get_access_token(base_url, username, password):\n \
      \   \"\"\"Faz login usando a sessão para obter um token e configura na sessão.\"\
      \"\"\n    login_url = f\"{base_url}/login\"\n    try:\n        print(f\"\U0001F511\
      \ Tentando fazer login como '{username}'...\")\n        response = session.post(\n\
      \            login_url,\n            data={\"username\": username, \"password\"\
      : password},\n            timeout=REQUEST_TIMEOUT\n        )\n        response.raise_for_status()\n\
      \        token_data = response.json()\n        access_token = token_data.get(\"\
      access_token\")\n        if not access_token:\n            print(\"❌ Token não\
      \ encontrado na resposta do login.\")\n            return False\n        session.headers.update({\"\
      Authorization\": f\"Bearer {access_token}\"})\n        print(\"✅ Login bem-sucedido\
      \ e token configurado na sessão.\")\n        return True\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"❌ Erro de conexão ou HTTP ao tentar fazer login: {e}\"\
      )\n        if hasattr(e, 'response') and e.response is not None:\n         \
      \    try: print(f\"   Detalhe API: {e.response.json()}\")\n             except\
      \ json.JSONDecodeError: print(f\"   Resposta: {e.response.text[:200]}...\")\n\
      \        return False\n    except json.JSONDecodeError:\n        print(f\"❌\
      \ Erro ao decodificar a resposta JSON do login.\")\n        return False\n\n\
      def check_queue_exists(base_url, queue_name):\n    \"\"\"Verifica se a fila\
      \ alvo existe.\"\"\"\n    get_queue_url = f\"{base_url}/queues/{queue_name}\"\
      \n    try:\n        print(f\"ℹ️  Verificando se a fila '{queue_name}' existe...\"\
      )\n        response_get = session.get(get_queue_url, timeout=REQUEST_TIMEOUT)\n\
      \        if response_get.status_code == 200:\n            print(f\"\U0001F44D\
      \ Fila '{queue_name}' encontrada.\")\n            return True\n        elif\
      \ response_get.status_code == 404:\n            print(f\"❌ Erro Crítico: Fila\
      \ '{queue_name}' não encontrada. Verifique o nome.\")\n            return False\n\
      \        else:\n            response_get.raise_for_status() # Lança erro para\
      \ outros status\n            return False # Não deve chegar aqui\n    except\
      \ requests.exceptions.RequestException as e:\n        print(f\"❌ Erro ao verificar\
      \ a fila '{queue_name}': {e}\")\n        if hasattr(e, 'response') and e.response\
      \ is not None:\n             try: print(f\"   Detalhe API: {e.response.json()}\"\
      )\n             except json.JSONDecodeError: print(f\"   Resposta: {e.response.text[:200]}...\"\
      )\n        return False\n\ndef consume_and_ack_message(base_url, queue_name):\n\
      \    \"\"\"Tenta consumir uma mensagem e, se bem-sucedido, faz o ACK.\"\"\"\n\
      \    global collected_message_contents\n    consume_url = f\"{base_url}/queues/{queue_name}/messages/consume\"\
      \n    message_data = None\n\n    # 1. Tentar Consumir\n    try:\n        response_consume\
      \ = session.get(consume_url, timeout=REQUEST_TIMEOUT)\n\n        if response_consume.status_code\
      \ == 200:\n            message_data = response_consume.json()\n            if\
      \ message_data is None:\n                # Fila vazia, não é um erro, apenas\
      \ informativo\n                # print(\"E\", end=\"\", flush=True) # 'E' para\
      \ Empty\n                return \"empty\" # Sinaliza que a fila está vazia\n\
      \            # Temos uma mensagem!\n            message_id = message_data.get('message_id')\n\
      \            content = message_data.get('content')\n            if message_id\
      \ is None or content is None:\n                print(f\"\\n❗️ Resposta de consumo\
      \ inválida recebida: {message_data}\")\n                return \"error\" # Erro\
      \ inesperado no formato da resposta\n\n        elif response_consume.status_code\
      \ == 404: # Fila não existe mais?\n             print(f\"\\n❌ Erro: Fila '{queue_name}'\
      \ não encontrada durante consumo. Foi deletada?\")\n             return \"fatal_error\"\
      \ # Erro que deve parar o consumidor\n        else:\n             response_consume.raise_for_status()\
      \ # Levanta erro para outros status HTTP\n\n    except requests.exceptions.Timeout:\n\
      \        print(\"T\", end=\"\", flush=True) # Timeout\n        return \"error\"\
      \n    except requests.exceptions.RequestException as e:\n        print(f\"\\\
      n❌ Erro de rede/HTTP ao consumir: {e}\")\n        if hasattr(e, 'response')\
      \ and e.response is not None:\n             try: print(f\"   Detalhe API: {e.response.json()}\"\
      )\n             except json.JSONDecodeError: print(f\"   Resposta: {e.response.text[:100]}...\"\
      )\n        return \"error\"\n    except json.JSONDecodeError:\n        print(f\"\
      \\n❌ Erro ao decodificar JSON da resposta de consumo.\")\n        return \"\
      error\"\n    except Exception as e:\n        print(f\"\\n\U0001F4A5 Erro inesperado\
      \ durante consumo: {e}\")\n        return \"error\"\n\n    # 2. Tentar Acknowledger\
      \ (ACK) - Somente se consumimos com sucesso\n    if message_data and message_id:\n\
      \        ack_url = f\"{base_url}/messages/{message_id}/ack\"\n        try:\n\
      \            response_ack = session.post(ack_url, timeout=REQUEST_TIMEOUT)\n\
      \            response_ack.raise_for_status() # Levanta erro para 4xx/5xx no\
      \ ACK\n\n            # ACK bem-sucedido!\n            collected_message_contents.append(content)\
      \ # Salva o conteúdo na lista\n            print(\".\", end=\"\", flush=True)\
      \ # \".\" para sucesso\n            return \"success\"\n\n        except requests.exceptions.Timeout:\n\
      \            print(\"A\", end=\"\", flush=True) # Ack Timeout\n            #\
      \ O que fazer aqui? A mensagem foi consumida mas não ack'd.\n            # Poderia\
      \ tentar NACK ou apenas logar por enquanto.\n            print(f\"\\n⚠️ Timeout\
      \ ao tentar ACK msg {message_id}. Mensagem pode ser reprocessada por outro consumidor.\"\
      )\n            return \"ack_error\"\n        except requests.exceptions.RequestException\
      \ as e:\n            status_code = getattr(e.response, 'status_code', 'N/A')\n\
      \            print(\"F\", end=\"\", flush=True) # Failed Ack\n            print(f\"\
      \\n❌ Falha ({status_code}) ao tentar ACK msg {message_id}: {e}\")\n        \
      \    if hasattr(e, 'response') and e.response is not None:\n               \
      \ try: print(f\"   Detalhe API: {e.response.json()}\")\n                except\
      \ json.JSONDecodeError: print(f\"   Resposta: {e.response.text[:100]}...\")\n\
      \            # Mensagem foi consumida mas não ack'd.\n            return \"\
      ack_error\"\n        except Exception as e:\n            print(f\"\\n\U0001F4A5\
      \ Erro inesperado durante ACK da msg {message_id}: {e}\")\n            return\
      \ \"ack_error\"\n\n    # Se chegamos aqui, algo deu errado antes do ACK\n  \
      \  return \"error\"\n\n# --- Execução Principal ---\nif __name__ == \"__main__\"\
      :\n    print(\"--- Consumidor e Coletor de Dados da Fila ---\")\n    print(f\"\
      Alvo: {BASE_URL}\")\n    print(f\"Fila: {QUEUE_NAME}\")\n    print(f\"Pasta\
      \ de Saída: {OUTPUT_DIR}\")\n\n    # Configura manipulador de sinal para Ctrl+C\n\
      \    signal.signal(signal.SIGINT, handle_shutdown)\n    signal.signal(signal.SIGTERM,\
      \ handle_shutdown)\n\n    if not setup_output_directory():\n        sys.exit(1)\n\
      \n    # 1. Autenticar\n    if not get_access_token(BASE_URL, USERNAME, PASSWORD):\n\
      \        print(\"\\n--- Falha no Login. Abortando. ---\")\n        session.close()\n\
      \        sys.exit(1)\n\n    # 2. Verificar se a fila existe\n    if not check_queue_exists(BASE_URL,\
      \ QUEUE_NAME):\n        print(f\"\\n--- Fila '{QUEUE_NAME}' não existe ou inacessível.\
      \ Abortando. ---\")\n        session.close()\n        sys.exit(1)\n\n    print(f\"\
      \\n--- Iniciando consumo da fila '{QUEUE_NAME}' ---\")\n    print(\"Pressione\
      \ Ctrl+C para parar e salvar os dados.\")\n    print(\"Legenda: [.] Sucesso\
      \ | [E] Fila Vazia (pausando) | [T] Timeout Consumo | [A] Timeout ACK | [F]\
      \ Falha ACK | [X] Erro\")\n\n    processed_count = 0\n    error_count = 0\n\
      \    start_time = time.time()\n\n    try:\n        while keep_running:\n   \
      \         result = consume_and_ack_message(BASE_URL, QUEUE_NAME)\n\n       \
      \     if result == \"success\":\n                processed_count += 1\n    \
      \            # Adiciona nova linha a cada 50 sucessos para melhor visualização\n\
      \                if processed_count % 50 == 0:\n                    elapsed\
      \ = time.time() - start_time\n                    rate = processed_count / elapsed\
      \ if elapsed > 0 else 0\n                    print(f\" | {processed_count} msgs\
      \ processadas ({rate:.1f} msg/s)\")\n\n            elif result == \"empty\"\
      :\n                print(\"E\", end=\"\", flush=True)\n                # Pausa\
      \ antes de tentar novamente se a fila estiver vazia\n                # Usa wait\
      \ do Event se estiver usando threading, ou sleep simples\n                try:\n\
      \                   time.sleep(EMPTY_QUEUE_DELAY_SECONDS)\n                except\
      \ InterruptedError: # Pode acontecer se Ctrl+C for pressionado durante o sleep\n\
      \                   handle_shutdown()\n                   break # Sai do loop\
      \ se interrompido\n\n            elif result == \"ack_error\":\n           \
      \     error_count += 1\n                # Pausa curta após erro de ACK para\
      \ evitar spam\n                time.sleep(0.2)\n\n            elif result ==\
      \ \"error\":\n                print(\"X\", end=\"\", flush=True)\n         \
      \       error_count += 1\n                # Pausa um pouco maior após erro genérico\
      \ ou de rede\n                time.sleep(1.0)\n\n            elif result ==\
      \ \"fatal_error\":\n                print(\"\\n⛔ Erro fatal detectado. Parando\
      \ o consumidor.\")\n                keep_running = False # Para o loop\n   \
      \             break\n\n            # Verificação adicional para o caso de handle_shutdown\
      \ ter sido chamado por um sinal\n            if not keep_running:\n        \
      \        break\n\n    # except KeyboardInterrupt: # Redundante se o signal handler\
      \ funcionar bem\n    #     handle_shutdown()\n\n    finally:\n        # Esta\
      \ parte será executada quando o loop terminar (normalmente ou por sinal)\n \
      \       print(\"\\n--- Finalizando ---\")\n        output_filename = generate_output_filename()\n\
      \        save_data_to_json(output_filename)\n\n        # Imprimir estatísticas\
      \ finais\n        end_time = time.time()\n        total_time = end_time - start_time\n\
      \        print(\"\\n--- Resumo da Coleta ---\")\n        print(f\"Tempo total\
      \ de execução: {total_time:.2f} segundos\")\n        print(f\"Total de mensagens\
      \ processadas e salvas: {processed_count}\")\n        print(f\"Total de erros\
      \ (consumo/ack/outros): {error_count}\")\n        if total_time > 0 and processed_count\
      \ > 0:\n            average_rate = processed_count / total_time\n          \
      \  print(f\"Taxa média de processamento: {average_rate:.2f} mensagens/segundo\"\
      )\n        print(f\"Dados salvos em: {output_filename}\")\n        print(\"\
      --------------------------\")\n\n        # Fecha a sessão de requests\n    \
      \    session.close()\n        print(\"\U0001F50C Sessão HTTP fechada.\")\n \
      \       sys.exit(0)"
    tamanho: 0.01 MB
  coletamensagemv1.py:
    caminho_completo: .\coletamensagemv1.py
    classes: []
    functions:
    - docstring: Faz login usando a sessão para obter um token.
      end_lineno: 58
      lineno: 28
      name: get_access_token
    - docstring: Envia um ACK para o servidor para confirmar o processamento.
      end_lineno: 97
      lineno: 60
      name: acknowledge_message
    - docstring: Tenta consumir uma mensagem e envia ACK se bem-sucedido.
      end_lineno: 188
      lineno: 99
      name: consume_and_ack_message
    imports:
    - asname: null
      name: requests
    - asname: null
      name: json
    - asname: null
      name: warnings
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: datetime
    numero_de_linhas: 260
    source_code: "# -*- coding: utf-8 -*-\nimport requests\nimport json\nimport warnings\n\
      import sys\nimport time\nimport datetime\n\n# --- Configurações ---\nBASE_URL\
      \ = \"https://localhost:8777\"\nQUEUE_NAME = \"minha-fila-teste-stress\" # <<<\
      \ A MESMA FILA USADA NO SCRIPT DE ENVIO >>>\nUSERNAME = \"admin\"\nPASSWORD\
      \ = \"admin\"\nPOLL_INTERVAL = 1 # Segundos para esperar se a fila estiver vazia\
      \ (reduzido para testes)\nREQUEST_TIMEOUT = 15 # Timeout para requisições GET/POST\n\
      # --- Fim das Configurações ---\n\n# Ignorar avisos sobre certificados SSL autoassinados\n\
      warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n\n\
      # --- Globais ---\nmessages_processed = 0\nconsume_attempts = 0\nack_failures\
      \ = 0\nlast_message_time = None\n# --- Fim Globais ---\n\ndef get_access_token(session,\
      \ base_url, username, password):\n    \"\"\"Faz login usando a sessão para obter\
      \ um token.\"\"\"\n    login_url = f\"{base_url}/login\"\n    try:\n       \
      \ print(f\"\U0001F511 Tentando fazer login como '{username}' em {login_url}...\"\
      )\n        response = session.post(\n            login_url,\n            data={\"\
      username\": username, \"password\": password},\n            verify=False,\n\
      \            timeout=20 # Timeout maior para login\n        )\n        response.raise_for_status()\n\
      \        token_data = response.json()\n        print(\"✅ Login bem-sucedido!\"\
      )\n        access_token = token_data.get(\"access_token\")\n        if access_token:\n\
      \            session.headers.update({\"Authorization\": f\"Bearer {access_token}\"\
      })\n            print(\"\U0001F511 Token de acesso configurado na sessão.\"\
      )\n            return access_token\n        else:\n             print(\"❌ Token\
      \ não encontrado na resposta do login.\")\n             return None\n    except\
      \ requests.exceptions.RequestException as e:\n        print(f\"❌ Erro de conexão\
      \ ou HTTP ao tentar fazer login: {e}\")\n        if hasattr(e, 'response') and\
      \ e.response is not None:\n            try: print(f\"   Detalhe da API: {e.response.status_code}\
      \ {e.response.reason} - {e.response.json()}\")\n            except json.JSONDecodeError:\
      \ print(f\"   Resposta (não JSON): {e.response.status_code} {e.response.reason}\
      \ - {e.response.text[:200]}...\")\n        return None\n    except json.JSONDecodeError:\n\
      \        print(f\"❌ Erro ao decodificar a resposta JSON do login.\")\n     \
      \   return None\n\ndef acknowledge_message(session, base_url, message_id):\n\
      \    \"\"\"Envia um ACK para o servidor para confirmar o processamento.\"\"\"\
      \n    global ack_failures\n    ack_url = f\"{base_url}/messages/{message_id}/ack\"\
      \n    try:\n        # print(f\"  -> Enviando ACK para {ack_url}...\") # Debug\
      \ verboso\n        response_ack = session.post(\n            ack_url,\n    \
      \        verify=False,\n            timeout=REQUEST_TIMEOUT\n        )\n\n \
      \       if response_ack.status_code == 200:\n            # print(f\"  ✅ ACK\
      \ bem-sucedido para mensagem {message_id}.\") # Debug verboso\n            return\
      \ True\n        elif response_ack.status_code == 404:\n             print(f\"\
      \  ⚠️ ACK falhou (404): Mensagem {message_id} não encontrada (provavelmente\
      \ já processada/deletada).\")\n             # Consideramos sucesso pois a mensagem\
      \ não está mais pendente para nós\n             return True\n        elif response_ack.status_code\
      \ == 409:\n             print(f\"  ⚠️ ACK falhou (409): Mensagem {message_id}\
      \ não estava no estado 'processing'. Status: {response_ack.json().get('detail',\
      \ '')}\")\n             ack_failures += 1\n             return False # Falha\
      \ real, a mensagem pode ter sido NACK'd ou ainda está pendente\n        else:\n\
      \            response_ack.raise_for_status() # Levanta erro para outros status\
      \ inesperados\n\n    except requests.exceptions.Timeout:\n        print(f\"\
      \  ⏳ Timeout ao enviar ACK para mensagem {message_id}.\")\n        ack_failures\
      \ += 1\n        return False\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"  ❌ Erro ao enviar ACK para mensagem {message_id}:\
      \ {e}\")\n        ack_failures += 1\n        if hasattr(e, 'response') and e.response\
      \ is not None:\n             try: print(f\"     Detalhe da API: {e.response.status_code}\
      \ {e.response.reason} - {e.response.json()}\")\n             except json.JSONDecodeError:\
      \ print(f\"     Resposta (não JSON): {e.response.status_code} {e.response.reason}\
      \ - {e.response.text[:200]}...\")\n        return False\n    return False #\
      \ Se chegou aqui por algum motivo\n\ndef consume_and_ack_message(session, base_url,\
      \ queue_name):\n    \"\"\"Tenta consumir uma mensagem e envia ACK se bem-sucedido.\"\
      \"\"\n    global messages_processed, last_message_time, consume_attempts\n \
      \   consume_url = f\"{base_url}/queues/{queue_name}/messages/consume\" # <<<\
      \ URL CORRIGIDA >>>\n    message_data = None\n    message_id = None\n    consume_attempts\
      \ += 1\n\n    # 1. Tentar obter (consumir) uma mensagem\n    try:\n        #\
      \ print(f\" Tentando consumir de {consume_url}...\") # Debug verboso\n     \
      \   response_get = session.get(\n            consume_url,\n            verify=False,\n\
      \            timeout=REQUEST_TIMEOUT\n        )\n\n        # --- Tratamento\
      \ da Resposta do Consumo ---\n        if response_get.status_code == 200:\n\
      \            # Tenta decodificar o JSON. Se o corpo for 'null', json() pode\
      \ retornar None ou dar erro\n            try:\n                message_data\
      \ = response_get.json()\n            except json.JSONDecodeError:\n        \
      \         # Isso pode acontecer se a resposta for 'null' literal ou outro texto\
      \ não-JSON\n                 if response_get.text == 'null':\n             \
      \        message_data = None # Tratar 'null' como fila vazia\n             \
      \    else:\n                    print(f\"❌ Erro ao decodificar JSON da resposta\
      \ de consumo (não era 'null'): {response_get.text[:100]}...\")\n           \
      \         return False # Falha inesperada\n\n            # --- Checa se a mensagem\
      \ foi recebida (message_data não é None) ---\n            if message_data:\n\
      \                message_id = message_data.get(\"message_id\") # <<< NOME DO\
      \ CAMPO CORRIGIDO >>>\n                content = message_data.get(\"content\"\
      , \"*Conteúdo não encontrado*\")\n                status = message_data.get(\"\
      status\", \"*Status não encontrado*\")\n                queue_recv = message_data.get(\"\
      queue\", queue_name) # Usa o nome da fila retornado se disponível\n\n      \
      \          if not message_id:\n                     print(f\"❌ Resposta de consumo\
      \ recebida, mas sem 'message_id'. Dados: {message_data}\")\n               \
      \      return False # Algo deu errado na API\n\n                print(f\"\U0001F4E9\
      \ [{datetime.datetime.now().strftime('%H:%M:%S')}] Msg Recebida (ID: {message_id},\
      \ Fila: {queue_recv}, Status: {status}): '{content[:80]}{'...' if len(content)>80\
      \ else ''}'\")\n\n                # --- 2. Enviar ACK imediatamente (após receber\
      \ a mensagem) ---\n                # Em uma aplicação real, o processamento\
      \ do 'content' iria aqui\n                ack_successful = acknowledge_message(session,\
      \ base_url, message_id)\n\n                if ack_successful:\n            \
      \        messages_processed += 1\n                    last_message_time = time.time()\n\
      \                    return True # Consumo E ACK bem-sucedidos\n           \
      \     else:\n                    # O ACK falhou. A mensagem permanecerá como\
      \ 'processing' no servidor.\n                    # Poderíamos tentar um NACK\
      \ aqui, mas por simplicidade vamos apenas registrar\n                    print(f\"\
      ‼️ Falha ao enviar ACK para msg {message_id} após consumo bem-sucedido.\")\n\
      \                    return False # Processamento completo falhou devido ao\
      \ ACK\n\n            else:\n                # Status 200 mas message_data é\
      \ None (corpo 'null'), significa fila vazia\n                # print(f\" Fila\
      \ '{queue_name}' vazia (200 OK com null).\") # Debug verboso\n             \
      \   return False # Indica que não havia mensagem\n\n        elif response_get.status_code\
      \ == 404:\n            # Este 404 geralmente significa que a *fila* em si não\
      \ foi encontrada\n            print(f\"❓ Fila '{queue_name}' não encontrada\
      \ no servidor (Erro 404 no GET .../consume). Verifique o nome da fila.\")\n\
      \            # Espera um pouco mais se a fila não for encontrada para não spammar\
      \ logs\n            time.sleep(POLL_INTERVAL * 5)\n            return False\
      \ # Indica que a fila não existe\n\n        else:\n            # Outros erros\
      \ HTTP inesperados\n            print(f\"‼️ Erro HTTP inesperado ao consumir\
      \ de '{queue_name}': {response_get.status_code} {response_get.reason}\")\n \
      \           response_get.raise_for_status() # Levanta erro para análise detalhada\n\
      \n    except requests.exceptions.Timeout:\n        print(f\"⏳ Timeout ao tentar\
      \ consumir mensagem da fila '{queue_name}'.\")\n        return False # Falha\
      \ temporária, tentar novamente depois\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"❌ Erro de requisição ao consumir mensagem da fila\
      \ '{queue_name}': {e}\")\n        if hasattr(e, 'response') and e.response is\
      \ not None:\n             try: print(f\"   Detalhe da API: {e.response.status_code}\
      \ {e.response.reason} - {e.response.json()}\")\n             except json.JSONDecodeError:\
      \ print(f\"   Resposta (não JSON): {e.response.status_code} {e.response.reason}\
      \ - {e.response.text[:200]}...\")\n        return False # Falha, tentar novamente\
      \ depois\n    except Exception as e:\n         print(f\"\U0001F4A5 Erro inesperado\
      \ na função consume_and_ack_message: {type(e).__name__}: {e}\")\n         #\
      \ Considerar logar traceback aqui para depuração\n         return False\n\n\
      \    # Retorno padrão caso nenhum caminho anterior retorne\n    return False\n\
      \n\n# --- Execução Principal ---\nif __name__ == \"__main__\":\n    print(f\"\
      --- Consumidor de Mensagens (com ACK) da Fila '{QUEUE_NAME}' ---\")\n    print(f\"\
      Alvo: {BASE_URL}\")\n    print(f\"Intervalo de polling (fila vazia): {POLL_INTERVAL}s\"\
      )\n\n    session = requests.Session()\n    session.verify = False # Ignora verificação\
      \ SSL para toda a sessão\n\n    # 1. Obter token de acesso e configurar na sessão\n\
      \    token = get_access_token(session, BASE_URL, USERNAME, PASSWORD)\n    if\
      \ not token:\n        print(\"\\n--- Falha no Login. Abortando. ---\")\n   \
      \     sys.exit(1)\n\n    print(f\"\\n--- Iniciando consumo da fila '{QUEUE_NAME}'\
      \ ---\")\n    print(\"Pressione Ctrl+C para parar...\")\n\n    start_time =\
      \ time.time()\n    running = True\n\n    try:\n        while running:\n    \
      \        try:\n                # Tenta consumir E fazer o ACK\n            \
      \    message_processed_successfully = consume_and_ack_message(session, BASE_URL,\
      \ QUEUE_NAME)\n\n                if not message_processed_successfully:\n  \
      \                  # Se não processou mensagem (fila vazia, erro, falha no ACK),\
      \ esperar\n                    # print(\".\", end=\"\", flush=True) # Indicador\
      \ visual de polling\n                    time.sleep(POLL_INTERVAL)\n       \
      \         # else:\n                    # Se consumiu e ACK foi OK, tenta ler\
      \ a próxima imediatamente (sem sleep)\n                    # Pequeno sleep opcional\
      \ para não sobrecarregar CPU em loop muito rápido\n                    # time.sleep(0.01)\n\
      \                    pass # Tenta consumir a próxima imediatamente\n\n     \
      \       except Exception as e:\n                # Captura exceções inesperadas\
      \ no loop principal\n                print(f\"\\n\U0001F4A5 Erro inesperado\
      \ no loop principal: {e}\")\n                print(\"Aguardando antes de tentar\
      \ novamente...\")\n                time.sleep(POLL_INTERVAL * 2) # Espera um\
      \ pouco mais após um erro grave\n\n    except KeyboardInterrupt:\n        print(\"\
      \\n\\n\U0001F6D1 Interrupção pelo usuário recebida. Parando o consumidor...\"\
      )\n        running = False\n\n    finally:\n        # Fecha a sessão de requests\
      \ para liberar conexões\n        print(\"Fechando a sessão HTTP...\")\n    \
      \    session.close()\n\n        # Imprimir estatísticas finais\n        end_time\
      \ = time.time()\n        total_time = end_time - start_time\n\n        print(\"\
      \\n--- Resumo Final do Consumo ---\")\n        print(f\"Tempo total de execução:\
      \ {total_time:.2f} segundos\")\n        print(f\"Tentativas de consumo: {consume_attempts}\"\
      )\n        print(f\"Mensagens processadas (consumo + ACK OK): {messages_processed}\"\
      )\n        print(f\"Falhas no ACK (após consumo OK): {ack_failures}\")\n   \
      \     if messages_processed > 0 and total_time > 0:\n            average_rate\
      \ = messages_processed / total_time\n            print(f\"Taxa média de consumo\
      \ efetivo: {average_rate:.2f} mensagens/segundo\")\n        if last_message_time:\n\
      \             print(f\"Última mensagem processada com sucesso em: {datetime.datetime.fromtimestamp(last_message_time).strftime('%Y-%m-%d\
      \ %H:%M:%S')}\")\n        else:\n             print(\"Nenhuma mensagem foi processada\
      \ com sucesso durante esta execução.\")\n        print(\"---------------------------------\"\
      )\n        sys.exit(0)"
    tamanho: 0.01 MB
  dbfixv1.py:
    caminho_completo: .\dbfixv1.py
    classes: []
    functions:
    - docstring: Conecta diretamente ao SQLite e adiciona a coluna 'updated_at'.
      end_lineno: 75
      lineno: 12
      name: add_updated_at_column_direct
    imports:
    - asname: null
      name: sqlite3
    - asname: null
      name: os
    - asname: null
      name: sys
    numero_de_linhas: 91
    source_code: "# -*- coding: utf-8 -*-\nimport sqlite3\nimport os\nimport sys\n\
      \n# --- Configurações (Ajuste se necessário) ---\nDB_DIR = 'databases'\nDB_FILENAME\
      \ = 'message_broker_v3.db'\nDB_PATH = os.path.abspath(os.path.join(DB_DIR, DB_FILENAME))\n\
      # --- Fim das Configurações ---\n\ndef add_updated_at_column_direct():\n   \
      \ \"\"\"Conecta diretamente ao SQLite e adiciona a coluna 'updated_at'.\"\"\"\
      \n\n    print(f\"--- Iniciando Script de Correção Direta (sqlite3): Adicionar\
      \ Coluna 'updated_at' ---\")\n    print(f\"Banco de dados alvo: {DB_PATH}\"\
      )\n\n    if not os.path.exists(DB_PATH):\n        print(f\"❌ Erro: Arquivo de\
      \ banco de dados não encontrado em '{DB_PATH}'.\")\n        print(\"   O script\
      \ da API precisa ser executado pelo menos uma vez para criar o DB inicial.\"\
      )\n        sys.exit(1)\n\n    # Comando SQL para adicionar a coluna (SQLite)\n\
      \    # Adicionamos como NULLABLE para não dar erro em linhas existentes\n  \
      \  # e IF NOT EXISTS para segurança, embora a verificação de erro seja mais\
      \ robusta\n    sql_command = \"ALTER TABLE queues ADD COLUMN updated_at DATETIME\
      \ NULL;\"\n    # SQL para verificar se a coluna já existe (específico do SQLite)\n\
      \    sql_check_column = \"SELECT COUNT(*) FROM pragma_table_info('queues') WHERE\
      \ name='updated_at';\"\n\n\n    conn = None # Inicializa a conexão fora do try\
      \ para poder usar no finally\n    try:\n        print(\"\U0001F50C Conectando\
      \ diretamente ao banco de dados SQLite...\")\n        conn = sqlite3.connect(DB_PATH)\n\
      \        cursor = conn.cursor()\n        print(\"   Conectado.\")\n\n      \
      \  # 1. Verificar se a coluna já existe\n        print(\"\U0001F50D Verificando\
      \ se a coluna 'updated_at' já existe...\")\n        cursor.execute(sql_check_column)\n\
      \        result = cursor.fetchone()\n        column_exists = result[0] > 0\n\
      \n        if column_exists:\n            print(\"✅ A coluna 'updated_at' já\
      \ existe na tabela 'queues'. Nenhuma alteração necessária.\")\n        else:\n\
      \            # 2. Se não existe, tentar adicionar\n            print(\" कॉलम\
      \ 'updated_at' não encontrada. Tentando adicionar...\") # (Coluna em Hindi,\
      \ mantido por consistência se foi erro de digitação)\n            print(f\"\U0001F680\
      \ Executando comando SQL: {sql_command}\")\n            cursor.execute(sql_command)\n\
      \            # Commit é necessário para ALTER TABLE no sqlite3\n           \
      \ conn.commit()\n            print(\"✅ Sucesso! Coluna 'updated_at' adicionada\
      \ à tabela 'queues'.\")\n\n    except sqlite3.Error as e:\n        # O erro\
      \ \"duplicate column name\" pode ocorrer se a verificação falhar por algum motivo\n\
      \        if \"duplicate column name: updated_at\" in str(e).lower():\n     \
      \        print(f\"ℹ️  Aviso: A coluna 'updated_at' já existe (detectado via\
      \ erro). Nenhuma alteração feita.\")\n        else:\n            print(f\"❌\
      \ Erro do SQLite ao tentar modificar a tabela:\")\n            print(f\"   Tipo\
      \ de Erro: {type(e).__name__}\")\n            print(f\"   Mensagem: {e}\")\n\
      \            print(\"   Verifique se a tabela 'queues' existe e se o comando\
      \ SQL está correto.\")\n            # Rollback pode ser útil se outras operações\
      \ estivessem na transação\n            if conn:\n                conn.rollback()\n\
      \    except Exception as e:\n        print(f\"❌ Ocorreu um erro inesperado:\"\
      )\n        print(e)\n    finally:\n        if conn:\n            print(\"\U0001F512\
      \ Fechando a conexão com o banco de dados SQLite...\")\n            conn.close()\n\
      \            print(\"   Conexão fechada.\")\n        print(\"--- Script de Correção\
      \ Direta Concluído ---\")\n\n# Executa a função\nif __name__ == \"__main__\"\
      :\n    print(\"*********************************************************************\"\
      )\n    print(\"* IMPORTANTE:                                               \
      \        *\")\n    print(\"* 1. PARE o servidor da API ANTES de executar este\
      \ script.          *\")\n    print(\"* 2. FAÇA UM BACKUP do arquivo .db como\
      \ precaução.                  *\")\n    print(\"*    Arquivo: databases/message_broker_v3.db\
      \                        *\")\n    print(\"*********************************************************************\"\
      )\n    try:\n      input(\"Pressione Enter para continuar ou Ctrl+C para cancelar...\"\
      )\n    except KeyboardInterrupt:\n        print(\"\\nOperação cancelada pelo\
      \ usuário.\")\n        sys.exit(0)\n\n    add_updated_at_column_direct()"
    tamanho: 0.00 MB
  dbfixv2.py:
    caminho_completo: .\dbfixv2.py
    classes: []
    functions:
    - docstring: Conecta diretamente ao SQLite e aplica as correções de esquema necessárias.
      end_lineno: 95
      lineno: 20
      name: apply_schema_corrections
    imports:
    - asname: null
      name: sqlite3
    - asname: null
      name: os
    - asname: null
      name: sys
    numero_de_linhas: 112
    source_code: "# -*- coding: utf-8 -*-\nimport sqlite3\nimport os\nimport sys\n\
      \n# --- Configurações (Ajuste se necessário) ---\nDB_DIR = 'databases'\nDB_FILENAME\
      \ = 'message_broker_v3.db'\nDB_PATH = os.path.abspath(os.path.join(DB_DIR, DB_FILENAME))\n\
      # --- Fim das Configurações ---\n\n# Lista de correções a serem aplicadas: (table_name,\
      \ column_name, column_type_sql)\ncorrections = [\n    ('queues',   'updated_at',\
      \ 'DATETIME NULL'),\n    ('messages', 'updated_at', 'DATETIME NULL'),\n    #\
      \ Adicione outras correções aqui se necessário no futuro\n    # Ex: ('messages',\
      \ 'retry_count', 'INTEGER DEFAULT 0')\n]\n\ndef apply_schema_corrections():\n\
      \    \"\"\"Conecta diretamente ao SQLite e aplica as correções de esquema necessárias.\"\
      \"\"\n\n    print(f\"--- Iniciando Script de Correção de Esquema (sqlite3) ---\"\
      )\n    print(f\"Banco de dados alvo: {DB_PATH}\")\n\n    if not os.path.exists(DB_PATH):\n\
      \        print(f\"❌ Erro: Arquivo de banco de dados não encontrado em '{DB_PATH}'.\"\
      )\n        print(\"   Execute o script da API principal primeiro para criar\
      \ o DB.\")\n        sys.exit(1)\n\n    conn = None\n    all_successful = True\
      \ # Flag para rastrear o sucesso geral\n\n    try:\n        print(\"\U0001F50C\
      \ Conectando diretamente ao banco de dados SQLite...\")\n        conn = sqlite3.connect(DB_PATH)\n\
      \        cursor = conn.cursor()\n        print(\"   Conectado.\")\n\n      \
      \  print(\"\\n--- Verificando e Aplicando Correções ---\")\n        for table_name,\
      \ column_name, column_type in corrections:\n            print(f\"\\n -> Verificando\
      \ Tabela: '{table_name}', Coluna: '{column_name}'\")\n\n            # SQL para\
      \ verificar se a coluna já existe\n            sql_check_column = f\"SELECT\
      \ COUNT(*) FROM pragma_table_info('{table_name}') WHERE name='{column_name}';\"\
      \n            # Comando SQL para adicionar a coluna\n            sql_add_column\
      \ = f\"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type};\"\n\n\
      \            try:\n                cursor.execute(sql_check_column)\n      \
      \          result = cursor.fetchone()\n                column_exists = result[0]\
      \ > 0\n\n                if column_exists:\n                    print(f\"  \
      \  ✅ Coluna '{column_name}' já existe em '{table_name}'.\")\n              \
      \  else:\n                    print(f\"    ⚠️ Coluna '{column_name}' não encontrada\
      \ em '{table_name}'. Tentando adicionar...\")\n                    print(f\"\
      \       Executando: {sql_add_column}\")\n                    cursor.execute(sql_add_column)\n\
      \                    # Commit APÓS CADA ALTER TABLE bem-sucedido é mais seguro\n\
      \                    conn.commit()\n                    print(f\"    ✅ Sucesso!\
      \ Coluna '{column_name}' adicionada a '{table_name}'.\")\n\n            except\
      \ sqlite3.Error as e_inner:\n                 # Verifica erro específico de\
      \ coluna duplicada\n                if f\"duplicate column name: {column_name}\"\
      \ in str(e_inner).lower():\n                     print(f\"    ℹ️ Aviso: Coluna\
      \ '{column_name}' já existe em '{table_name}' (detectado via erro).\")\n   \
      \             else:\n                    print(f\"    ❌ Erro do SQLite ao processar\
      \ '{table_name}'.'{column_name}':\")\n                    print(f\"       {type(e_inner).__name__}:\
      \ {e_inner}\")\n                    all_successful = False\n               \
      \     # Interrompe em caso de erro inesperado para esta correção específica\n\
      \                    break\n\n    except sqlite3.Error as e_outer:\n       \
      \ print(f\"\\n❌ Erro Crítico do SQLite durante a conexão ou operação geral:\"\
      )\n        print(f\"   {type(e_outer).__name__}: {e_outer}\")\n        all_successful\
      \ = False\n        if conn:\n            conn.rollback() # Desfaz qualquer alteração\
      \ pendente na transação atual\n    except Exception as e_generic:\n        print(f\"\
      \\n❌ Ocorreu um erro inesperado:\")\n        print(e_generic)\n        all_successful\
      \ = False\n    finally:\n        if conn:\n            print(\"\\n\U0001F512\
      \ Fechando a conexão com o banco de dados SQLite...\")\n            conn.close()\n\
      \            print(\"   Conexão fechada.\")\n\n        print(\"\\n--- Script\
      \ de Correção de Esquema Concluído ---\")\n        if all_successful:\n    \
      \        print(\"\U0001F389 Todas as verificações/correções foram concluídas\
      \ (ou não foram necessárias).\")\n        else:\n            print(\"\U0001F6D1\
      \ Ocorreram erros durante o processo. Verifique os logs acima.\")\n\n\n# Executa\
      \ a função\nif __name__ == \"__main__\":\n    print(\"*********************************************************************\"\
      )\n    print(\"* IMPORTANTE:                                               \
      \        *\")\n    print(\"* 1. PARE o servidor da API ANTES de executar este\
      \ script.          *\")\n    print(\"* 2. FAÇA UM BACKUP do arquivo .db como\
      \ precaução.                  *\")\n    print(\"*    Arquivo: databases/message_broker_v3.db\
      \                        *\")\n    print(\"*********************************************************************\"\
      )\n    try:\n      input(\"Pressione Enter para continuar ou Ctrl+C para cancelar...\"\
      )\n    except KeyboardInterrupt:\n        print(\"\\nOperação cancelada pelo\
      \ usuário.\")\n        sys.exit(0)\n\n    apply_schema_corrections()"
    tamanho: 0.00 MB
  docgenv2.py:
    caminho_completo: .\docgenv2.py
    classes: []
    functions:
    - docstring: null
      end_lineno: 36
      lineno: 32
      name: get_file_size
    - docstring: null
      end_lineno: 44
      lineno: 39
      name: count_lines
    - docstring: null
      end_lineno: 53
      lineno: 47
      name: read_file_content
    - docstring: null
      end_lineno: 102
      lineno: 56
      name: analyze_python_code
    - docstring: null
      end_lineno: 130
      lineno: 105
      name: get_sqlite_info
    - docstring: null
      end_lineno: 142
      lineno: 133
      name: get_python_info
    - docstring: null
      end_lineno: 151
      lineno: 145
      name: get_json_info
    - docstring: null
      end_lineno: 160
      lineno: 154
      name: get_yaml_info
    - docstring: null
      end_lineno: 166
      lineno: 164
      name: format_size
    - docstring: null
      end_lineno: 170
      lineno: 169
      name: stylize_header
    - docstring: null
      end_lineno: 186
      lineno: 179
      name: configurar_geracao
    - docstring: null
      end_lineno: 197
      lineno: 189
      name: enviar_mensagem
    - docstring: null
      end_lineno: 238
      lineno: 200
      name: scan_directory
    - docstring: null
      end_lineno: 358
      lineno: 241
      name: gerar_relatorio_ia
    - docstring: null
      end_lineno: 377
      lineno: 361
      name: main
    imports:
    - asname: null
      name: os
    - asname: null
      name: platform
    - asname: null
      name: sqlite3
    - asname: null
      name: json
    - asname: null
      name: yaml
    - asname: null
      name: time
    - module: datetime
      names:
      - datetime
    - asname: genai
      name: google.generativeai
    - module: colorama
      names:
      - Fore
      - Style
      - init
    - asname: null
      name: ast
    numero_de_linhas: 386
    source_code: "import os\nimport platform\nimport sqlite3\nimport json\nimport\
      \ yaml\nimport time\nfrom datetime import datetime\nimport google.generativeai\
      \ as genai\nfrom colorama import Fore, Style, init\nimport ast\n\n# Inicializa\
      \ o Colorama para colorir os logs\ninit(autoreset=True)\n\n# Configuração da\
      \ API de IA\nAPI_KEY = 'AIzaSyC7dAwSyLKaVO2E-PA6UaacLZ4aLGtrXbY'  # Chave da\
      \ API fornecida\ngenai.configure(api_key=API_KEY)\nNOME_MODELO = \"gemini-2.0-flash\"\
      \  # Modelo atualizado\n\n# Arquivos e diretórios a serem ignorados\nIGNORE_LIST\
      \ = [\n    \"docgenv1.py\",  # Ignora o próprio script\n    \".git\",  # Ignora\
      \ diretórios .git\n    \".venv\",  # Ignora ambientes virtuais\n    \"venv\"\
      ,  # Ignora ambientes virtuais\n    \"__pycache__\", # Ignora diretórios de\
      \ cache\n    \"node_modules\",  # Ignora diretórios node_modules\n    \".md\"\
      \ # Ignora arquivos Markdown\n]\n\n# Função para calcular o tamanho de um arquivo\n\
      def get_file_size(file_path):\n    try:\n        return os.path.getsize(file_path)\n\
      \    except OSError:\n        return -1  # Retorna -1 se o arquivo não existir\
      \ ou ocorrer um erro\n\n# Função para calcular o número de linhas em um arquivo\
      \ de texto\ndef count_lines(file_path):\n    try:\n        with open(file_path,\
      \ 'r', encoding='utf-8') as f:\n            return sum(1 for _ in f)\n    except\
      \ Exception:\n        return -1\n\n# Função para ler o conteúdo de um arquivo\
      \ de texto (com limite)\ndef read_file_content(file_path, max_lines=500): #\
      \ Aumentei o limite de linhas\n    try:\n        with open(file_path, 'r', encoding='utf-8')\
      \ as f:\n            lines = [next(f) for _ in range(max_lines)]  # Lê até max_lines\
      \ linhas\n            return \"\".join(lines)\n    except Exception:\n     \
      \   return None\n\n# Função para analisar o código Python e extrair informações\
      \ relevantes\ndef analyze_python_code(file_path):\n    try:\n        with open(file_path,\
      \ 'r', encoding='utf-8') as f:\n            source_code = f.read()\n       \
      \ \n        tree = ast.parse(source_code)\n        \n        functions = []\n\
      \        classes = []\n        imports = []\n\n        for node in ast.walk(tree):\n\
      \            if isinstance(node, ast.FunctionDef):\n                functions.append({\n\
      \                    \"name\": node.name,\n                    \"docstring\"\
      : ast.get_docstring(node),\n                    \"lineno\": node.lineno,\n \
      \                   \"end_lineno\": node.end_lineno if hasattr(node, 'end_lineno')\
      \ else None\n                })\n            elif isinstance(node, ast.ClassDef):\n\
      \                classes.append({\n                    \"name\": node.name,\n\
      \                    \"docstring\": ast.get_docstring(node),\n             \
      \       \"lineno\": node.lineno,\n                    \"end_lineno\": node.end_lineno\
      \ if hasattr(node, 'end_lineno') else None\n                })\n           \
      \ elif isinstance(node, ast.Import):\n                for alias in node.names:\n\
      \                    imports.append({\n                        \"name\": alias.name,\n\
      \                        \"asname\": alias.asname\n                    })\n\
      \            elif isinstance(node, ast.ImportFrom):\n                imports.append({\n\
      \                    \"module\": node.module,\n                    \"names\"\
      : [alias.name for alias in node.names]\n                })\n        \n     \
      \   return {\n            \"functions\": functions,\n            \"classes\"\
      : classes,\n            \"imports\": imports,\n            \"source_code\":\
      \ source_code # Mantém o código fonte completo\n        }\n\n    except Exception\
      \ as e:\n        return {\"error\": str(e)}\n\n# Função para obter informações\
      \ de um arquivo .db (SQLite)\ndef get_sqlite_info(file_path, max_rows=5): #\
      \ Aumentei o número de exemplos\n    try:\n        conn = sqlite3.connect(file_path)\n\
      \        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT name FROM\
      \ sqlite_master WHERE type='table';\")\n        tables = [row[0] for row in\
      \ cursor.fetchall()]\n        \n        table_data = {}\n        for table in\
      \ tables:\n            cursor.execute(f\"PRAGMA table_info('{table}');\")\n\
      \            columns = [(row[1], row[2]) for row in cursor.fetchall()] # (nome,\
      \ tipo)\n            \n            try:\n                cursor.execute(f\"\
      SELECT * FROM '{table}' LIMIT {max_rows};\")\n                rows = cursor.fetchall()\n\
      \            except sqlite3.OperationalError as e:\n                print(f\"\
      {Fore.YELLOW}Aviso: Não foi possível selecionar dados da tabela '{table}': {e}{Style.RESET_ALL}\"\
      )\n                rows = []  # Define rows como uma lista vazia em caso de\
      \ erro\n\n            table_data[table] = {\"columns\": columns, \"rows\": rows}\n\
      \        \n        conn.close()\n        return table_data\n    except Exception\
      \ as e:\n        return {\"error\": str(e)}\n\n# Função para obter informações\
      \ de um arquivo .py\ndef get_python_info(file_path):\n    try:\n        with\
      \ open(file_path, 'r', encoding='utf-8') as f:\n            source_code = f.read()\n\
      \        analysis_result = analyze_python_code(file_path)\n        # Garante\
      \ que o código fonte seja armazenado corretamente\n        analysis_result['source_code']\
      \ = source_code\n        return analysis_result\n    except Exception as e:\n\
      \        return {\"error\": str(e)}\n\n# Função para obter informações de um\
      \ arquivo .json\ndef get_json_info(file_path):\n     try:\n        file_size\
      \ = get_file_size(file_path)\n        line_count = count_lines(file_path)\n\
      \        return {\"tamanho\": format_size(file_size), \"numero_de_linhas\":\
      \ line_count}\n     except Exception as e:\n        return {\"error\": str(e)}\n\
      \n# Função para obter informações de um arquivo .yaml\ndef get_yaml_info(file_path):\n\
      \    try:\n        file_size = get_file_size(file_path)\n        line_count\
      \ = count_lines(file_path)\n        return {\"tamanho\": format_size(file_size),\
      \ \"numero_de_linhas\": line_count}\n    except Exception as e:\n        return\
      \ {\"error\": str(e)}\n\n\n# Função para formatar o tamanho em MB\ndef format_size(size_in_bytes):\n\
      \    size_mb = size_in_bytes / (1024 * 1024)\n    return f\"{size_mb:.2f} MB\"\
      \n\n# Função para estilizar o cabeçalho de cada unidade\ndef stylize_header(header):\n\
      \    return f\"\U0001F4C1 --- {header} --- \U0001F4C1\\n\"\n\n# Configurações\
      \ de geração da IA (movido para o escopo global para facilitar a modificação)\n\
      AI_TEMPERATURE = 0.9  # Aumentei para respostas mais criativas e variadas\n\
      AI_TOP_P = 0.99       # Aumentei para considerar um conjunto maior de palavras\
      \ possíveis\nAI_TOP_K = 80         # Aumentei para uma amostragem mais ampla\n\
      AI_MAX_TOKENS = 32768  # Aumentei consideravelmente o limite máximo de tokens\n\
      \n# Função para gerar o payload da IA para relatório Markdown\ndef configurar_geracao(temperatura=AI_TEMPERATURE,\
      \ top_p=AI_TOP_P, top_k=AI_TOP_K, max_tokens=AI_MAX_TOKENS):\n    return {\n\
      \        \"temperature\": temperatura,\n        \"top_p\": top_p,\n        \"\
      top_k\": top_k,\n        \"max_output_tokens\": max_tokens,\n        \"response_mime_type\"\
      : \"text/plain\",\n    }\n\n# Função para enviar mensagens para a IA com logging\
      \ aprimorado\ndef enviar_mensagem(sessao, mensagem):\n    try:\n        print(f\"\
      {Fore.YELLOW}\U0001F9E0 Enviando mensagem para a IA...\")  # Log colorido\n\
      \        resposta = sessao.send_message([mensagem])\n        print(f\"{Fore.GREEN}✅\
      \ Resposta recebida!\")  # Sucesso\n        return resposta.text\n    except\
      \ Exception as e:\n        print(f\"{Fore.RED}❗Erro ao enviar mensagem para\
      \ o modelo: {e}\")\n        return \"\"\n\n# Função para varrer diretórios e\
      \ arquivos\ndef scan_directory(root_path=\".\"):\n    report = {}\n    for root,\
      \ dirs, files in os.walk(root_path):\n        # Ignora diretórios da lista de\
      \ ignorados\n        dirs[:] = [d for d in dirs if d not in IGNORE_LIST]\n\n\
      \        root_report = {}\n        for file in files:\n            if file in\
      \ IGNORE_LIST:\n                continue\n            \n            file_path\
      \ = os.path.join(root, file)\n            file_size = get_file_size(file_path)\n\
      \            line_count = count_lines(file_path)\n\n            file_info =\
      \ {\n                \"tamanho\": format_size(file_size),\n                \"\
      numero_de_linhas\": line_count,\n                \"caminho_completo\": file_path\
      \ # Adicionado caminho completo\n            }\n\n            if file.endswith(\"\
      .py\"):\n                python_info = get_python_info(file_path)\n        \
      \        if python_info:\n                    file_info.update(python_info)\n\
      \            elif file.endswith(\".db\"):\n                sqlite_info = get_sqlite_info(file_path)\n\
      \                file_info[\"sqlite_info\"] = sqlite_info\n            elif\
      \ file.endswith(\".json\"):\n                json_info = get_json_info(file_path)\n\
      \                file_info[\"json_info\"] = json_info\n            elif file.endswith(\"\
      .yaml\") or file.endswith(\".yml\"):\n                yaml_info = get_yaml_info(file_path)\n\
      \                file_info[\"yaml_info\"] = yaml_info\n            \n      \
      \      root_report[file] = file_info\n\n        report[root] = root_report\n\
      \    return report\n\n# Função para gerar relatório em YAML e enviar para IA\n\
      def gerar_relatorio_ia(data, project_name):\n    try:\n        sessao_chat =\
      \ genai.GenerativeModel(\n            model_name=NOME_MODELO,\n            generation_config=configurar_geracao(),\n\
      \        ).start_chat(history=[])\n    except Exception as e:\n        print(f\"\
      {Fore.RED}❗Erro ao iniciar sessão de chat: {e}\")\n        return\n\n    prompt\
      \ = f\"\"\"\n    nao repita dados dos bancos, foque mais em descrever o projeto,\
      \ o que faz, como faz, etc quem faz, etc, o projeto no espectro amplo é o objetivo\n\
      \    documente o projeto bem comercial e executivo\n    nao precisa cobrir cada\
      \ arquivo cada coisa, documente no amplo, foque no pareto 80 - 20  - foque no\
      \ 20 relevante dos 80 irrrelevante ignore\n    \n    naoseja minuciosoo, abranja\
      \ tudo de forma geral, o projeto como todo, foque no que é relevante\n    NUNCA\
      \ GERE CODIGO NA SUA RESPOSTA, NAO ANALISE O CODIGO A PONTO DE REPETIR, NUNCA\
      \ CRIE SNIPPETS E CODIGO FONTE NA RESPOSTA\n    seu objeitov nao é criar nem\
      \ ficar citando ocdoigo, mas sim documentar o projeto, documente como produto\
      \ e projeto\n    sempre criado por elias andrade - replika ia solutions- sempre\
      \ documente o projeto em si, nao fique repetindo codigo\n    respondoa mais\
      \ longo, detlahado, aprofundado, use muitos icones, emojis, documente o projeto,\
      \ o que faz, como faz, o que faz o que, etc.\n    \n    Você é um especialista\
      \ sênior em documentação de projetos de software e agora também um arquiteto\
      \ de sistemas.\n    Sua missão é analisar a estrutura e o conteúdo de um projeto\
      \ e gerar um README.md completo e detalhado.\n    Você deve fornecer uma visão\
      \ geral abrangente do projeto para facilitar a compreensão e manutenção futura.\n\
      \n    Com base nos dados fornecidos, gere um README.md completo e detalhado\
      \ para o projeto. Use Markdown para formatar o README.md.\n\n    O README.md\
      \ deve incluir as seguintes seções (use títulos e subtítulos apropriados):\n\
      \n    1.  **Título do Projeto:** {project_name} (Utilize emojis relacionados\
      \ ao nome, se aplicável)\n\n    2.  **Descrição Geral:**\n        *   Forneça\
      \ uma descrição concisa e de alto nível do projeto, incluindo seu propósito\
      \ principal e funcionalidades.\n        *   Identifique os principais componentes\
      \ e tecnologias utilizadas.\n        *   Resuma o problema que o projeto resolve\
      \ ou a necessidade que atende.\n\n    3.  **Estrutura do Projeto (com muitos\
      \ emojis):**\n        *   Apresente uma lista detalhada de todos os diretórios\
      \ e arquivos.\n        *   Para cada diretório, descreva seu papel e responsabilidades\
      \ dentro do projeto.\n        *   Para cada arquivo, inclua:\n            *\
      \   Nome do arquivo (com emoji indicando o tipo, ex: \U0001F40D para .py, ⚙️\
      \ para config)\n            *   Caminho completo (importante para localização)\n\
      \            *   Tamanho do arquivo\n            *   Número de linhas\n    \
      \        *   Descrição da função do arquivo (inferida do nome, comentários no\
      \ código, etc.)\n\n    4.  **Detalhes Técnicos e Arquiteturais:**\n        *\
      \   **Código Fonte (Python):**\n            *   Apresente o código fonte completo\
      \ de cada arquivo Python.\n            *   Analise o código e destaque os principais\
      \ componentes:\n                *   Funções (nome, docstring, linhas de início\
      \ e fim)\n                *   Classes (nome, docstring, linhas de início e fim)\n\
      \                *   Imports (módulos importados, aliases)\n            *  \
      \ Explique a lógica por trás dos principais algoritmos e estruturas de dados.\n\
      \            *   Comente sobre padrões de design utilizados (se houver).\n \
      \            *  Formate o código de forma legível, removendo quebras de linha\
      \ desnecessárias e garantindo a codificação UTF-8 correta.\n        *   **Bancos\
      \ de Dados (SQLite):**\n            *   Diagrama ER (se possível, descreva a\
      \ estrutura em texto se não for possível gerar o diagrama).\n            * \
      \  Lista de tabelas com descrições claras.\n            *   Esquema de cada\
      \ tabela (nome das colunas, tipos de dados, chaves primárias/estrangeiras).\n\
      \            *   Exemplos de consultas SQL importantes.\n            *   Dados\
      \ de exemplo (até 5 linhas por tabela, com formatação de tabela Markdown).\n\
      \            *   Observações sobre otimizações de queries (se aplicável).\n\
      \        *   **Configurações (JSON/YAML):**\n            *   Descreva o propósito\
      \ de cada arquivo de configuração.\n            *   Liste as principais chaves\
      \ de configuração e seus significados.\n            *   (Não inclua o conteúdo\
      \ completo, apenas resumos e exemplos se necessários)\n            *   Caminho\
      \ completo (para facilidade de acesso)\n            *    Tamanho do arquivo\
      \ e número de linhas.\n\n    5.  **Como Executar e Configurar o Projeto:**\n\
      \        *   Instruções passo a passo para configurar o ambiente (ex: instalar\
      \ Python, dependências).\n        *   Exemplo de como executar o projeto (ex:\
      \ `python main.py`).\n        *   Explique como configurar o projeto (ex: editar\
      \ arquivos de configuração).\n        *   Liste as dependências externas e como\
      \ instalá-las (use `pip install -r requirements.txt` se aplicável).\n      \
      \  *   Explique como executar os testes (se houver testes automatizados).\n\n\
      \    6.  **Considerações Adicionais:**\n        *   **Arquitetura do Projeto:**\
      \ Discuta a arquitetura geral do projeto (ex: MVC, microserviços, etc.).\n \
      \       *   **Padrões de Codificação:** Mencione se o projeto segue algum padrão\
      \ de codificação específico (ex: PEP 8).\n        *   **Licença:** Informe a\
      \ licença sob a qual o projeto é distribuído.\n        *   **Contribuições:**\
      \ Explique como outros desenvolvedores podem contribuir para o projeto.\n  \
      \      *   **Próximos Passos:** Liste os próximos passos para o desenvolvimento\
      \ do projeto.\n        *   **Notas:** Inclua quaisquer notas adicionais que\
      \ sejam relevantes para o projeto.\n\n    7.  **Informações sobre o ambiente\
      \ que o gerou:**\n        *   Sistema Operacional\n        *   Data e Hora da\
      \ geração\n        *   Nome do computador\n\n    Formate o README.md de forma\
      \ clara, concisa e profissional. Use formatação Markdown (títulos, subtítulos,\
      \ listas, tabelas, blocos de código, links, imagens) para facilitar a leitura\
      \ e a compreensão. Inclua emojis relevantes para tornar o documento mais visualmente\
      \ atraente.\n\n    Seja extremamente detalhista e procure inferir o máximo de\
      \ informações possível sobre o projeto com base nos dados fornecidos.\n    O\
      \ objetivo é criar um README.md que seja útil tanto para desenvolvedores que\
      \ já conhecem o projeto quanto para aqueles que estão chegando agora.\n\n  \
      \  Aqui estão os dados do projeto em formato YAML:\n\n    ```yaml\n    {yaml.dump(data,\
      \ allow_unicode=True)}\n    ```\n    \n    nunca crie snippet de codigo, nunca\
      \ traga na sua resposta pedaços do codigo por que isso é redundante, nao é permitido\
      \ ter codigo fonte nas respostas\n    --\n    a documentacao é itnerna rpa mim,\
      \ pra replika ai, entao nao crie como se fosse um repo publico, é documento\
      \ interno - use muitos icones e emojis, cubra o projeto e fale altaemnte tecnico\n\
      \    entenda uqe nivel o projeto est,a parou, etc, o que ja funciona, oq ue\
      \ nao funciona ainda, o que ja foi criado, entenda nesse aspecto, muito icone\
      \ e emoji estilizado pro notion\n    documente o projeto a propriedade intelectual\
      \ e que nivel esta o projeto, etc, \n    \n    \"\"\"\n\n    resposta = enviar_mensagem(sessao_chat,\
      \ prompt)\n\n    if resposta:\n        markdown_filename = \"DOCUMENTACAO-PROJETO.md\"\
      \  # Nome fixo do arquivo README\n        with open(markdown_filename, 'w',\
      \ encoding='utf-8') as markdown_file:\n            markdown_file.write(resposta)\n\
      \        print(f\"{Fore.GREEN}\U0001F4C4 {markdown_filename} salvo com sucesso!\"\
      )\n    else:\n        print(f\"{Fore.RED}❗Erro ao gerar relatório com a IA.\"\
      )\n\n# Função principal para unir tudo\ndef main():\n    project_path = \".\"\
      \ # Diretório atual\n    project_name = os.path.basename(os.getcwd()) # Nome\
      \ do diretório atual\n    print(f\"{Fore.YELLOW}\U0001F504 Iniciando varredura\
      \ do projeto '{project_name}' em '{project_path}'...\")\n    project_data =\
      \ scan_directory(project_path)\n\n    if project_data:\n        # Salva o YAML\
      \ em arquivo para fins de backup\n        yaml_filename = f\"estrutura_projeto_{project_name}_{platform.node()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml\"\
      \n        with open(yaml_filename, 'w', encoding='utf-8') as yaml_file:\n  \
      \          yaml.dump(project_data, yaml_file, allow_unicode=True)\n        print(f\"\
      {Fore.GREEN}\U0001F4C2 Estrutura do projeto salva em: {yaml_filename}\")\n\n\
      \        # Envia os dados para a IA para gerar relatório final\n        gerar_relatorio_ia(project_data,\
      \ project_name)\n    else:\n        print(f\"{Fore.RED}\U0001F6AB Nenhum arquivo\
      \ ou diretório encontrado.\")\n\n# Execução do programa\nif __name__ == \"__main__\"\
      :\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(f\"\
      {Fore.RED}❗Processo interrompido pelo usuário.\")\n    except Exception as e:\n\
      \        print(f\"{Fore.RED}❗Erro inesperado: {e}\")"
    tamanho: 0.02 MB
  geramensagem-v2-loop.py:
    caminho_completo: .\geramensagem-v2-loop.py
    classes: []
    functions:
    - docstring: Faz login usando a sessão para obter um token.
      end_lineno: 64
      lineno: 34
      name: get_access_token
    - docstring: Verifica/cria a fila usando a sessão.
      end_lineno: 113
      lineno: 66
      name: check_or_create_queue
    - docstring: Função executada por cada thread worker.
      end_lineno: 154
      lineno: 115
      name: message_sender_worker
    - docstring: Thread separada para reportar o status periodicamente.
      end_lineno: 181
      lineno: 158
      name: status_reporter
    imports:
    - asname: null
      name: requests
    - asname: null
      name: json
    - asname: null
      name: warnings
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: datetime
    - asname: null
      name: threading
    - asname: null
      name: queue
    - module: concurrent.futures
      names:
      - ThreadPoolExecutor
    numero_de_linhas: 268
    source_code: "# -*- coding: utf-8 -*-\nimport requests\nimport json\nimport warnings\n\
      import sys\nimport time\nimport datetime\nimport threading\nimport queue # Embora\
      \ não usemos uma Queue do módulo, o conceito é similar\nfrom concurrent.futures\
      \ import ThreadPoolExecutor # Alternativa mais moderna para gerenciar threads\n\
      \n# --- Configurações ---\nBASE_URL = \"https://localhost:8777\"\nQUEUE_NAME\
      \ = \"minha-fila-teste-stress\" # Fila para teste de stress\nMESSAGE_BASE_CONTENT\
      \ = \"Stress Test Msg\"\nUSERNAME = \"admin\"\nPASSWORD = \"admin\"\nNUM_WORKERS\
      \ = 20  # <<< NÚMERO DE THREADS CONCORRENTES >>> Ajuste conforme necessário\n\
      REPORT_INTERVAL = 5 # Segundos entre relatórios de status\n# --- Fim das Configurações\
      \ ---\n\n# Ignorar avisos sobre certificados SSL autoassinados\nwarnings.filterwarnings(\"\
      ignore\", message=\"Unverified HTTPS request\")\n\n# --- Globais e Sincronização\
      \ ---\nmessage_counter = 0\nsuccess_count = 0\nfail_count = 0\ncounter_lock\
      \ = threading.Lock() # Para proteger acesso aos contadores\nstop_event = threading.Event()\
      \ # Para sinalizar parada para as threads\nstart_time = 0.0\n# --- Fim Globais\
      \ ---\n\ndef get_access_token(session, base_url, username, password):\n    \"\
      \"\"Faz login usando a sessão para obter um token.\"\"\"\n    login_url = f\"\
      {base_url}/login\"\n    try:\n        print(f\"\U0001F511 Tentando fazer login\
      \ como '{username}' em {login_url}...\")\n        response = session.post( #\
      \ Usa a sessão\n            login_url,\n            data={\"username\": username,\
      \ \"password\": password},\n            verify=False,\n            timeout=15\
      \ # Timeout um pouco maior para login\n        )\n        response.raise_for_status()\n\
      \        token_data = response.json()\n        print(\"✅ Login bem-sucedido!\"\
      )\n        # Define o header de autorização na sessão para todas as requisições\
      \ futuras\n        access_token = token_data.get(\"access_token\")\n       \
      \ if access_token:\n            session.headers.update({\"Authorization\": f\"\
      Bearer {access_token}\"})\n            return access_token # Retorna o token\
      \ caso seja necessário fora da sessão\n        else:\n             print(\"\
      ❌ Token não encontrado na resposta do login.\")\n             return None\n\
      \    except requests.exceptions.RequestException as e:\n        print(f\"❌ Erro\
      \ de conexão ou HTTP ao tentar fazer login: {e}\")\n        if hasattr(e, 'response')\
      \ and e.response is not None:\n            try: print(f\"   Detalhe da API:\
      \ {e.response.json()}\")\n            except json.JSONDecodeError: print(f\"\
      \   Resposta (não JSON): {e.response.text[:200]}...\")\n        return None\n\
      \    except json.JSONDecodeError:\n        print(f\"❌ Erro ao decodificar a\
      \ resposta JSON do login.\")\n        return None\n\ndef check_or_create_queue(session,\
      \ base_url, queue_name):\n    \"\"\"Verifica/cria a fila usando a sessão.\"\"\
      \"\n    get_queue_url = f\"{base_url}/queues/{queue_name}\"\n    create_queue_url\
      \ = f\"{base_url}/queues\"\n    # Headers já estão na sessão (Authorization)\n\
      \n    try:\n        print(f\"ℹ️  Verificando se a fila '{queue_name}' existe...\"\
      )\n        response_get = session.get(get_queue_url, verify=False, timeout=10)\
      \ # Usa sessão\n\n        if response_get.status_code == 200:\n            print(f\"\
      \U0001F44D Fila '{queue_name}' já existe.\")\n            return True\n\n  \
      \      elif response_get.status_code == 404:\n            print(f\"⚠️  Fila\
      \ '{queue_name}' não encontrada. Tentando criar...\")\n            payload =\
      \ {\"name\": queue_name}\n            # Adiciona Content-Type especificamente\
      \ para esta requisição POST JSON\n            headers_post = {\"Content-Type\"\
      : \"application/json\"}\n            response_post = session.post( # Usa sessão\n\
      \                create_queue_url,\n                headers=headers_post, #\
      \ Adiciona o Content-Type aqui\n                json=payload,\n            \
      \    verify=False,\n                timeout=10\n            )\n\n          \
      \  if response_post.status_code == 201:\n                print(f\"✅ Fila '{queue_name}'\
      \ criada com sucesso!\")\n                time.sleep(0.5)\n                return\
      \ True\n            elif response_post.status_code == 409:\n               \
      \  print(f\"⚠️  A fila '{queue_name}' foi criada por outro processo (Erro 409).\
      \ OK.\")\n                 return True\n            else:\n                response_post.raise_for_status()\
      \ # Levanta erro para outros status\n                return False # Não deve\
      \ chegar aqui\n\n        else:\n            response_get.raise_for_status()\
      \ # Levanta erro para outros status no GET\n            return False # Não deve\
      \ chegar aqui\n\n    except requests.exceptions.RequestException as e:\n   \
      \     print(f\"❌ Erro ao verificar ou criar a fila '{queue_name}': {e}\")\n\
      \        if hasattr(e, 'response') and e.response is not None:\n           \
      \  try: print(f\"   Detalhe da API: {e.response.json()}\")\n             except\
      \ json.JSONDecodeError: print(f\"   Resposta (não JSON): {e.response.text[:200]}...\"\
      )\n        return False\n\ndef message_sender_worker(session, base_url, queue_name,\
      \ worker_id):\n    \"\"\"Função executada por cada thread worker.\"\"\"\n  \
      \  global message_counter, success_count, fail_count\n    publish_url = f\"\
      {base_url}/queues/{queue_name}/messages\"\n    headers_post = {\"Content-Type\"\
      : \"application/json\"} # Content-Type para o POST\n\n    while not stop_event.is_set():\
      \ # Continua enquanto o evento de parada não for setado\n        with counter_lock:\
      \ # Adquire o lock para obter um ID único\n            current_msg_id = message_counter\n\
      \            message_counter += 1\n        # O lock é liberado aqui\n\n    \
      \    full_message = f\"{MESSAGE_BASE_CONTENT} Worker {worker_id} Msg #{current_msg_id}\
      \ ({datetime.datetime.now().isoformat()})\"\n        payload = {\"content\"\
      : full_message}\n\n        try:\n            response = session.post(\n    \
      \            publish_url,\n                headers=headers_post, # Adiciona\
      \ Content-Type (Auth já está na sessão)\n                json=payload,\n   \
      \             verify=False,\n                timeout=5 # Timeout curto para\
      \ não prender a thread por muito tempo\n            )\n            response.raise_for_status()\
      \ # Lança exceção para erros 4xx/5xx\n\n            # Se chegou aqui, foi sucesso\n\
      \            with counter_lock: # Adquire o lock para atualizar contador de\
      \ sucesso\n                success_count += 1\n            # Lock liberado\n\
      \n        except requests.exceptions.Timeout:\n            with counter_lock:\
      \ fail_count += 1\n            # print(f\"T{worker_id}:T\", end=\"\", flush=True)\
      \ # Timeout\n        except requests.exceptions.RequestException as e:\n   \
      \         with counter_lock: fail_count += 1\n            # Opcional: Logar\
      \ o erro, mas pode poluir muito\n            # status = e.response.status_code\
      \ if hasattr(e, 'response') and e.response else 'N/A'\n            # print(f\"\
      T{worker_id}:E{status}\", end=\"\", flush=True) # Erro com status\n        except\
      \ Exception as e: # Captura outras exceções inesperadas\n             with counter_lock:\
      \ fail_count += 1\n             # print(f\"T{worker_id}:X\", end=\"\", flush=True)\
      \ # Erro genérico\n        # REMOVEMOS O time.sleep() DAQUI PARA MÁXIMA VELOCIDADE\n\
      \ndef status_reporter():\n    \"\"\"Thread separada para reportar o status periodicamente.\"\
      \"\"\n    global start_time\n    last_report_time = time.time()\n    last_success_count\
      \ = 0\n\n    while not stop_event.wait(REPORT_INTERVAL): # Espera pelo intervalo\
      \ ou pelo evento de parada\n        now = time.time()\n        elapsed_interval\
      \ = now - last_report_time\n        \n        with counter_lock: # Lê contadores\
      \ com segurança\n            current_success = success_count\n            current_fail\
      \ = fail_count\n            current_total = message_counter\n\n        interval_success\
      \ = current_success - last_success_count\n        rate = interval_success /\
      \ elapsed_interval if elapsed_interval > 0 else 0\n        total_elapsed = now\
      \ - start_time\n        overall_rate = current_success / total_elapsed if total_elapsed\
      \ > 0 else 0\n\n        print(f\"\\n[Status {time.strftime('%H:%M:%S')}] Total:\
      \ {current_total} | Sucesso: {current_success} ({interval_success} no intervalo)\
      \ | Falhas: {current_fail} | Taxa Intervalo: {rate:.1f} msg/s | Taxa Média:\
      \ {overall_rate:.1f} msg/s\")\n        \n        last_report_time = now\n  \
      \      last_success_count = current_success\n\n\n# --- Execução Principal ---\n\
      if __name__ == \"__main__\":\n    print(\"--- Cliente de Teste de Stress (Máxima\
      \ Velocidade) ---\")\n    print(f\"Alvo: {BASE_URL}\")\n    print(f\"Fila: {QUEUE_NAME}\"\
      )\n    print(f\"Workers: {NUM_WORKERS}\")\n\n    # Cria uma única sessão para\
      \ ser compartilhada por todas as threads\n    # A sessão gerencia o pool de\
      \ conexões e headers comuns\n    session = requests.Session()\n    session.verify\
      \ = False # Ignora verificação SSL para toda a sessão\n\n    # 1. Obter token\
      \ de acesso e configurar na sessão\n    token = get_access_token(session, BASE_URL,\
      \ USERNAME, PASSWORD)\n    if not token:\n        print(\"\\n--- Falha no Login.\
      \ Abortando. ---\")\n        sys.exit(1)\n\n    print(\"\\n--- Preparação da\
      \ Fila ---\")\n    # 2. Verificar/Criar a fila usando a sessão\n    queue_is_ready\
      \ = check_or_create_queue(session, BASE_URL, QUEUE_NAME)\n    if not queue_is_ready:\n\
      \        print(f\"\\n--- Falha ao preparar a fila '{QUEUE_NAME}'. Abortando.\
      \ ---\")\n        session.close() # Fecha a sessão\n        sys.exit(1)\n\n\
      \    print(f\"\\n--- Iniciando {NUM_WORKERS} workers para stress na fila '{QUEUE_NAME}'\
      \ ---\")\n    print(\"Pressione Ctrl+C para parar...\")\n\n    start_time =\
      \ time.time() # Marca o tempo de início\n\n    threads = []\n    # Inicia a\
      \ thread de relatório\n    reporter_thread = threading.Thread(target=status_reporter,\
      \ daemon=True) # Daemon=True para não impedir saída\n    reporter_thread.start()\n\
      \n    # Cria e inicia as threads worker\n    for i in range(NUM_WORKERS):\n\
      \        thread = threading.Thread(target=message_sender_worker, args=(session,\
      \ BASE_URL, QUEUE_NAME, i + 1))\n        thread.start()\n        threads.append(thread)\n\
      \n    try:\n        # Mantém a thread principal viva esperando as workers terminarem\
      \ (o que só acontece com Ctrl+C)\n        # Ou poderíamos simplesmente esperar\
      \ pelo Ctrl+C aqui\n        while not stop_event.is_set():\n             time.sleep(0.5)\
      \ # Evita que a thread principal consuma 100% CPU apenas esperando\n\n    except\
      \ KeyboardInterrupt:\n        print(\"\\n\\n\U0001F6D1 Interrupção pelo usuário\
      \ recebida. Sinalizando parada para as threads...\")\n        stop_event.set()\
      \ # Sinaliza para todas as threads pararem\n\n    finally:\n        print(\"\
      Aguardando workers finalizarem...\")\n        # Espera todas as threads worker\
      \ terminarem\n        for thread in threads:\n            thread.join(timeout=10)\
      \ # Dá um tempo para as threads terminarem\n\n        # A thread reporter é\
      \ daemon, então não precisamos esperar por ela explicitamente se o programa\
      \ sair\n\n        # Fecha a sessão de requests para liberar conexões\n     \
      \   session.close()\n\n        # 5. Imprimir estatísticas finais\n        end_time\
      \ = time.time()\n        total_time = end_time - start_time\n        # Lê os\
      \ contadores finais (o lock não é estritamente necessário aqui,\n        # pois\
      \ as threads já pararam, mas é boa prática)\n        with counter_lock:\n  \
      \          final_total = message_counter\n            final_success = success_count\n\
      \            final_fail = fail_count\n\n        print(\"\\n--- Resumo Final\
      \ do Teste de Stress ---\")\n        print(f\"Tempo total de execução: {total_time:.2f}\
      \ segundos\")\n        print(f\"Total de mensagens tentadas: {final_total}\"\
      )\n        print(f\"Total de mensagens publicadas com sucesso: {final_success}\"\
      )\n        print(f\"Total de falhas na publicação: {final_fail}\")\n       \
      \ if total_time > 0:\n            average_rate = final_success / total_time\n\
      \            print(f\"Taxa média de publicação (sucesso): {average_rate:.2f}\
      \ mensagens/segundo\")\n        else:\n            print(\"Taxa média de publicação:\
      \ N/A (tempo de execução muito curto)\")\n        print(\"--------------------------------------\"\
      )\n        sys.exit(0)"
    tamanho: 0.01 MB
  geramensagem-v3-massive-loop.py:
    caminho_completo: .\geramensagem-v3-massive-loop.py
    classes: []
    functions:
    - docstring: Faz login usando a sessão para obter um token.
      end_lineno: 64
      lineno: 34
      name: get_access_token
    - docstring: Verifica/cria a fila usando a sessão.
      end_lineno: 113
      lineno: 66
      name: check_or_create_queue
    - docstring: Função executada por cada thread worker.
      end_lineno: 154
      lineno: 115
      name: message_sender_worker
    - docstring: Thread separada para reportar o status periodicamente.
      end_lineno: 181
      lineno: 158
      name: status_reporter
    imports:
    - asname: null
      name: requests
    - asname: null
      name: json
    - asname: null
      name: warnings
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: datetime
    - asname: null
      name: threading
    - asname: null
      name: queue
    - module: concurrent.futures
      names:
      - ThreadPoolExecutor
    numero_de_linhas: 268
    source_code: "# -*- coding: utf-8 -*-\nimport requests\nimport json\nimport warnings\n\
      import sys\nimport time\nimport datetime\nimport threading\nimport queue # Embora\
      \ não usemos uma Queue do módulo, o conceito é similar\nfrom concurrent.futures\
      \ import ThreadPoolExecutor # Alternativa mais moderna para gerenciar threads\n\
      \n# --- Configurações ---\nBASE_URL = \"https://localhost:8777\"\nQUEUE_NAME\
      \ = \"minha-fila-teste-stress\" # Fila para teste de stress\nMESSAGE_BASE_CONTENT\
      \ = \"Stress Test Msg\"\nUSERNAME = \"admin\"\nPASSWORD = \"admin\"\nNUM_WORKERS\
      \ = 60  # <<< NÚMERO DE THREADS CONCORRENTES >>> Ajuste conforme necessário\n\
      REPORT_INTERVAL = 5 # Segundos entre relatórios de status\n# --- Fim das Configurações\
      \ ---\n\n# Ignorar avisos sobre certificados SSL autoassinados\nwarnings.filterwarnings(\"\
      ignore\", message=\"Unverified HTTPS request\")\n\n# --- Globais e Sincronização\
      \ ---\nmessage_counter = 0\nsuccess_count = 0\nfail_count = 0\ncounter_lock\
      \ = threading.Lock() # Para proteger acesso aos contadores\nstop_event = threading.Event()\
      \ # Para sinalizar parada para as threads\nstart_time = 0.0\n# --- Fim Globais\
      \ ---\n\ndef get_access_token(session, base_url, username, password):\n    \"\
      \"\"Faz login usando a sessão para obter um token.\"\"\"\n    login_url = f\"\
      {base_url}/login\"\n    try:\n        print(f\"\U0001F511 Tentando fazer login\
      \ como '{username}' em {login_url}...\")\n        response = session.post( #\
      \ Usa a sessão\n            login_url,\n            data={\"username\": username,\
      \ \"password\": password},\n            verify=False,\n            timeout=15\
      \ # Timeout um pouco maior para login\n        )\n        response.raise_for_status()\n\
      \        token_data = response.json()\n        print(\"✅ Login bem-sucedido!\"\
      )\n        # Define o header de autorização na sessão para todas as requisições\
      \ futuras\n        access_token = token_data.get(\"access_token\")\n       \
      \ if access_token:\n            session.headers.update({\"Authorization\": f\"\
      Bearer {access_token}\"})\n            return access_token # Retorna o token\
      \ caso seja necessário fora da sessão\n        else:\n             print(\"\
      ❌ Token não encontrado na resposta do login.\")\n             return None\n\
      \    except requests.exceptions.RequestException as e:\n        print(f\"❌ Erro\
      \ de conexão ou HTTP ao tentar fazer login: {e}\")\n        if hasattr(e, 'response')\
      \ and e.response is not None:\n            try: print(f\"   Detalhe da API:\
      \ {e.response.json()}\")\n            except json.JSONDecodeError: print(f\"\
      \   Resposta (não JSON): {e.response.text[:200]}...\")\n        return None\n\
      \    except json.JSONDecodeError:\n        print(f\"❌ Erro ao decodificar a\
      \ resposta JSON do login.\")\n        return None\n\ndef check_or_create_queue(session,\
      \ base_url, queue_name):\n    \"\"\"Verifica/cria a fila usando a sessão.\"\"\
      \"\n    get_queue_url = f\"{base_url}/queues/{queue_name}\"\n    create_queue_url\
      \ = f\"{base_url}/queues\"\n    # Headers já estão na sessão (Authorization)\n\
      \n    try:\n        print(f\"ℹ️  Verificando se a fila '{queue_name}' existe...\"\
      )\n        response_get = session.get(get_queue_url, verify=False, timeout=10)\
      \ # Usa sessão\n\n        if response_get.status_code == 200:\n            print(f\"\
      \U0001F44D Fila '{queue_name}' já existe.\")\n            return True\n\n  \
      \      elif response_get.status_code == 404:\n            print(f\"⚠️  Fila\
      \ '{queue_name}' não encontrada. Tentando criar...\")\n            payload =\
      \ {\"name\": queue_name}\n            # Adiciona Content-Type especificamente\
      \ para esta requisição POST JSON\n            headers_post = {\"Content-Type\"\
      : \"application/json\"}\n            response_post = session.post( # Usa sessão\n\
      \                create_queue_url,\n                headers=headers_post, #\
      \ Adiciona o Content-Type aqui\n                json=payload,\n            \
      \    verify=False,\n                timeout=10\n            )\n\n          \
      \  if response_post.status_code == 201:\n                print(f\"✅ Fila '{queue_name}'\
      \ criada com sucesso!\")\n                time.sleep(0.5)\n                return\
      \ True\n            elif response_post.status_code == 409:\n               \
      \  print(f\"⚠️  A fila '{queue_name}' foi criada por outro processo (Erro 409).\
      \ OK.\")\n                 return True\n            else:\n                response_post.raise_for_status()\
      \ # Levanta erro para outros status\n                return False # Não deve\
      \ chegar aqui\n\n        else:\n            response_get.raise_for_status()\
      \ # Levanta erro para outros status no GET\n            return False # Não deve\
      \ chegar aqui\n\n    except requests.exceptions.RequestException as e:\n   \
      \     print(f\"❌ Erro ao verificar ou criar a fila '{queue_name}': {e}\")\n\
      \        if hasattr(e, 'response') and e.response is not None:\n           \
      \  try: print(f\"   Detalhe da API: {e.response.json()}\")\n             except\
      \ json.JSONDecodeError: print(f\"   Resposta (não JSON): {e.response.text[:200]}...\"\
      )\n        return False\n\ndef message_sender_worker(session, base_url, queue_name,\
      \ worker_id):\n    \"\"\"Função executada por cada thread worker.\"\"\"\n  \
      \  global message_counter, success_count, fail_count\n    publish_url = f\"\
      {base_url}/queues/{queue_name}/messages\"\n    headers_post = {\"Content-Type\"\
      : \"application/json\"} # Content-Type para o POST\n\n    while not stop_event.is_set():\
      \ # Continua enquanto o evento de parada não for setado\n        with counter_lock:\
      \ # Adquire o lock para obter um ID único\n            current_msg_id = message_counter\n\
      \            message_counter += 1\n        # O lock é liberado aqui\n\n    \
      \    full_message = f\"{MESSAGE_BASE_CONTENT} Worker {worker_id} Msg #{current_msg_id}\
      \ ({datetime.datetime.now().isoformat()})\"\n        payload = {\"content\"\
      : full_message}\n\n        try:\n            response = session.post(\n    \
      \            publish_url,\n                headers=headers_post, # Adiciona\
      \ Content-Type (Auth já está na sessão)\n                json=payload,\n   \
      \             verify=False,\n                timeout=5 # Timeout curto para\
      \ não prender a thread por muito tempo\n            )\n            response.raise_for_status()\
      \ # Lança exceção para erros 4xx/5xx\n\n            # Se chegou aqui, foi sucesso\n\
      \            with counter_lock: # Adquire o lock para atualizar contador de\
      \ sucesso\n                success_count += 1\n            # Lock liberado\n\
      \n        except requests.exceptions.Timeout:\n            with counter_lock:\
      \ fail_count += 1\n            # print(f\"T{worker_id}:T\", end=\"\", flush=True)\
      \ # Timeout\n        except requests.exceptions.RequestException as e:\n   \
      \         with counter_lock: fail_count += 1\n            # Opcional: Logar\
      \ o erro, mas pode poluir muito\n            # status = e.response.status_code\
      \ if hasattr(e, 'response') and e.response else 'N/A'\n            # print(f\"\
      T{worker_id}:E{status}\", end=\"\", flush=True) # Erro com status\n        except\
      \ Exception as e: # Captura outras exceções inesperadas\n             with counter_lock:\
      \ fail_count += 1\n             # print(f\"T{worker_id}:X\", end=\"\", flush=True)\
      \ # Erro genérico\n        # REMOVEMOS O time.sleep() DAQUI PARA MÁXIMA VELOCIDADE\n\
      \ndef status_reporter():\n    \"\"\"Thread separada para reportar o status periodicamente.\"\
      \"\"\n    global start_time\n    last_report_time = time.time()\n    last_success_count\
      \ = 0\n\n    while not stop_event.wait(REPORT_INTERVAL): # Espera pelo intervalo\
      \ ou pelo evento de parada\n        now = time.time()\n        elapsed_interval\
      \ = now - last_report_time\n        \n        with counter_lock: # Lê contadores\
      \ com segurança\n            current_success = success_count\n            current_fail\
      \ = fail_count\n            current_total = message_counter\n\n        interval_success\
      \ = current_success - last_success_count\n        rate = interval_success /\
      \ elapsed_interval if elapsed_interval > 0 else 0\n        total_elapsed = now\
      \ - start_time\n        overall_rate = current_success / total_elapsed if total_elapsed\
      \ > 0 else 0\n\n        print(f\"\\n[Status {time.strftime('%H:%M:%S')}] Total:\
      \ {current_total} | Sucesso: {current_success} ({interval_success} no intervalo)\
      \ | Falhas: {current_fail} | Taxa Intervalo: {rate:.1f} msg/s | Taxa Média:\
      \ {overall_rate:.1f} msg/s\")\n        \n        last_report_time = now\n  \
      \      last_success_count = current_success\n\n\n# --- Execução Principal ---\n\
      if __name__ == \"__main__\":\n    print(\"--- Cliente de Teste de Stress (Máxima\
      \ Velocidade) ---\")\n    print(f\"Alvo: {BASE_URL}\")\n    print(f\"Fila: {QUEUE_NAME}\"\
      )\n    print(f\"Workers: {NUM_WORKERS}\")\n\n    # Cria uma única sessão para\
      \ ser compartilhada por todas as threads\n    # A sessão gerencia o pool de\
      \ conexões e headers comuns\n    session = requests.Session()\n    session.verify\
      \ = False # Ignora verificação SSL para toda a sessão\n\n    # 1. Obter token\
      \ de acesso e configurar na sessão\n    token = get_access_token(session, BASE_URL,\
      \ USERNAME, PASSWORD)\n    if not token:\n        print(\"\\n--- Falha no Login.\
      \ Abortando. ---\")\n        sys.exit(1)\n\n    print(\"\\n--- Preparação da\
      \ Fila ---\")\n    # 2. Verificar/Criar a fila usando a sessão\n    queue_is_ready\
      \ = check_or_create_queue(session, BASE_URL, QUEUE_NAME)\n    if not queue_is_ready:\n\
      \        print(f\"\\n--- Falha ao preparar a fila '{QUEUE_NAME}'. Abortando.\
      \ ---\")\n        session.close() # Fecha a sessão\n        sys.exit(1)\n\n\
      \    print(f\"\\n--- Iniciando {NUM_WORKERS} workers para stress na fila '{QUEUE_NAME}'\
      \ ---\")\n    print(\"Pressione Ctrl+C para parar...\")\n\n    start_time =\
      \ time.time() # Marca o tempo de início\n\n    threads = []\n    # Inicia a\
      \ thread de relatório\n    reporter_thread = threading.Thread(target=status_reporter,\
      \ daemon=True) # Daemon=True para não impedir saída\n    reporter_thread.start()\n\
      \n    # Cria e inicia as threads worker\n    for i in range(NUM_WORKERS):\n\
      \        thread = threading.Thread(target=message_sender_worker, args=(session,\
      \ BASE_URL, QUEUE_NAME, i + 1))\n        thread.start()\n        threads.append(thread)\n\
      \n    try:\n        # Mantém a thread principal viva esperando as workers terminarem\
      \ (o que só acontece com Ctrl+C)\n        # Ou poderíamos simplesmente esperar\
      \ pelo Ctrl+C aqui\n        while not stop_event.is_set():\n             time.sleep(0.5)\
      \ # Evita que a thread principal consuma 100% CPU apenas esperando\n\n    except\
      \ KeyboardInterrupt:\n        print(\"\\n\\n\U0001F6D1 Interrupção pelo usuário\
      \ recebida. Sinalizando parada para as threads...\")\n        stop_event.set()\
      \ # Sinaliza para todas as threads pararem\n\n    finally:\n        print(\"\
      Aguardando workers finalizarem...\")\n        # Espera todas as threads worker\
      \ terminarem\n        for thread in threads:\n            thread.join(timeout=10)\
      \ # Dá um tempo para as threads terminarem\n\n        # A thread reporter é\
      \ daemon, então não precisamos esperar por ela explicitamente se o programa\
      \ sair\n\n        # Fecha a sessão de requests para liberar conexões\n     \
      \   session.close()\n\n        # 5. Imprimir estatísticas finais\n        end_time\
      \ = time.time()\n        total_time = end_time - start_time\n        # Lê os\
      \ contadores finais (o lock não é estritamente necessário aqui,\n        # pois\
      \ as threads já pararam, mas é boa prática)\n        with counter_lock:\n  \
      \          final_total = message_counter\n            final_success = success_count\n\
      \            final_fail = fail_count\n\n        print(\"\\n--- Resumo Final\
      \ do Teste de Stress ---\")\n        print(f\"Tempo total de execução: {total_time:.2f}\
      \ segundos\")\n        print(f\"Total de mensagens tentadas: {final_total}\"\
      )\n        print(f\"Total de mensagens publicadas com sucesso: {final_success}\"\
      )\n        print(f\"Total de falhas na publicação: {final_fail}\")\n       \
      \ if total_time > 0:\n            average_rate = final_success / total_time\n\
      \            print(f\"Taxa média de publicação (sucesso): {average_rate:.2f}\
      \ mensagens/segundo\")\n        else:\n            print(\"Taxa média de publicação:\
      \ N/A (tempo de execução muito curto)\")\n        print(\"--------------------------------------\"\
      )\n        sys.exit(0)"
    tamanho: 0.01 MB
  geramensagem.py:
    caminho_completo: .\geramensagem.py
    classes: []
    functions:
    - docstring: Faz login na API para obter um token de acesso.
      end_lineno: 42
      lineno: 19
      name: get_access_token
    - docstring: Verifica se a fila existe. Se não, tenta criá-la.
      end_lineno: 93
      lineno: 44
      name: check_or_create_queue
    - docstring: Publica uma mensagem na fila especificada usando o token.
      end_lineno: 127
      lineno: 95
      name: publish_message_to_queue
    imports:
    - asname: null
      name: requests
    - asname: null
      name: json
    - asname: null
      name: warnings
    - asname: null
      name: sys
    - asname: null
      name: time
    numero_de_linhas: 156
    source_code: "# -*- coding: utf-8 -*-\nimport requests\nimport json\nimport warnings\n\
      import sys\nimport time # Adicionado para um pequeno delay opcional\n\n# ---\
      \ Configurações ---\nBASE_URL = \"https://localhost:8777\"\nQUEUE_NAME = \"\
      minha-fila-teste\" # A fila que queremos usar/criar\nMESSAGE_CONTENT = \"hello\
      \ world\" # Mensagem específica\nUSERNAME = \"admin\"\nPASSWORD = \"admin\"\n\
      # --- Fim das Configurações ---\n\n# Ignorar avisos sobre certificados SSL autoassinados\
      \ (APENAS PARA DESENVOLVIMENTO)\nwarnings.filterwarnings(\"ignore\", message=\"\
      Unverified HTTPS request\")\n\ndef get_access_token(base_url, username, password):\n\
      \    \"\"\"Faz login na API para obter um token de acesso.\"\"\"\n    login_url\
      \ = f\"{base_url}/login\"\n    try:\n        print(f\"\U0001F511 Tentando fazer\
      \ login como '{username}' em {login_url}...\")\n        response = requests.post(\n\
      \            login_url,\n            data={\"username\": username, \"password\"\
      : password},\n            verify=False, # Ignora verificação SSL\n         \
      \   timeout=10\n        )\n        response.raise_for_status()\n        token_data\
      \ = response.json()\n        print(\"✅ Login bem-sucedido!\")\n        return\
      \ token_data.get(\"access_token\")\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"❌ Erro de conexão ou HTTP ao tentar fazer login: {e}\"\
      )\n        if hasattr(e, 'response') and e.response is not None:\n         \
      \   try: print(f\"   Detalhe da API: {e.response.json()}\")\n            except\
      \ json.JSONDecodeError: print(f\"   Resposta (não JSON): {e.response.text[:200]}...\"\
      )\n        return None\n    except json.JSONDecodeError:\n        print(f\"\
      ❌ Erro ao decodificar a resposta JSON do login.\")\n        return None\n\n\
      def check_or_create_queue(base_url, queue_name, token):\n    \"\"\"Verifica\
      \ se a fila existe. Se não, tenta criá-la.\"\"\"\n    get_queue_url = f\"{base_url}/queues/{queue_name}\"\
      \n    create_queue_url = f\"{base_url}/queues\"\n    headers = {\"Authorization\"\
      : f\"Bearer {token}\"}\n\n    try:\n        print(f\"ℹ️  Verificando se a fila\
      \ '{queue_name}' existe...\")\n        response_get = requests.get(get_queue_url,\
      \ headers=headers, verify=False, timeout=5)\n\n        if response_get.status_code\
      \ == 200:\n            print(f\"\U0001F44D Fila '{queue_name}' já existe.\"\
      )\n            return True # Fila existe\n\n        elif response_get.status_code\
      \ == 404:\n            print(f\"⚠️  Fila '{queue_name}' não encontrada. Tentando\
      \ criar...\")\n            payload = {\"name\": queue_name}\n            headers_post\
      \ = {**headers, \"Content-Type\": \"application/json\"} # Adiciona Content-Type\n\
      \            response_post = requests.post(\n                create_queue_url,\n\
      \                headers=headers_post,\n                json=payload,\n    \
      \            verify=False,\n                timeout=10\n            )\n\n  \
      \          if response_post.status_code == 201: # 201 Created\n            \
      \    print(f\"✅ Fila '{queue_name}' criada com sucesso!\")\n               \
      \ # Pequeno delay opcional para garantir que a fila esteja pronta no DB\n  \
      \              time.sleep(0.5)\n                return True # Fila criada\n\
      \            elif response_post.status_code == 409: # 409 Conflict\n       \
      \          print(f\"⚠️  A fila '{queue_name}' foi criada por outro processo\
      \ enquanto verificávamos (Erro 409). Considerando como sucesso.\")\n       \
      \          return True # Fila já existe (provavelmente criada entre o GET e\
      \ o POST)\n            else:\n                # Outro erro durante a criação\n\
      \                response_post.raise_for_status() # Lança exceção para outros\
      \ erros HTTP\n                return False # Falha inesperada na criação (nunca\
      \ deve chegar aqui se raise_for_status funcionar)\n\n        else:\n       \
      \     # Erro inesperado ao verificar a fila\n            response_get.raise_for_status()\n\
      \            return False # Falha inesperada na verificação\n\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"❌ Erro ao verificar ou criar a fila '{queue_name}':\
      \ {e}\")\n        if hasattr(e, 'response') and e.response is not None:\n  \
      \          try: print(f\"   Detalhe da API: {e.response.json()}\")\n       \
      \     except json.JSONDecodeError: print(f\"   Resposta (não JSON): {e.response.text[:200]}...\"\
      )\n        return False # Falha\n\ndef publish_message_to_queue(base_url, queue_name,\
      \ message, token):\n    \"\"\"Publica uma mensagem na fila especificada usando\
      \ o token.\"\"\"\n    publish_url = f\"{base_url}/queues/{queue_name}/messages\"\
      \n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n       \
      \ \"Content-Type\": \"application/json\"\n    }\n    payload = {\"content\"\
      : message}\n    try:\n        print(f\"\U0001F4E4 Publicando mensagem na fila\
      \ '{queue_name}'...\")\n        print(f\"   Conteúdo: '{message}'\")\n     \
      \   response = requests.post(\n            publish_url,\n            headers=headers,\n\
      \            json=payload,\n            verify=False,\n            timeout=10\n\
      \        )\n        response.raise_for_status() # Lança exceção para erros HTTP\
      \ (4xx, 5xx)\n        response_data = response.json()\n        print(f\"✅ Mensagem\
      \ publicada com sucesso!\")\n        print(f\"   ID da Mensagem: {response_data.get('message_id')}\"\
      )\n        return response_data\n    except requests.exceptions.RequestException\
      \ as e:\n        print(f\"❌ Erro de conexão ou HTTP ao publicar mensagem:\"\
      )\n        if hasattr(e, 'response') and e.response is not None:\n         \
      \    try: print(f\"   Detalhe da API: {e.response.json()}\")\n             except\
      \ json.JSONDecodeError: print(f\"   Resposta (não JSON): {e.response.text[:200]}...\"\
      )\n        else: print(f\"   Erro: {e}\")\n        return None\n    except json.JSONDecodeError:\n\
      \        print(f\"❌ Erro ao decodificar a resposta JSON da publicação.\")\n\
      \        return None\n\n# --- Execução Principal ---\nif __name__ == \"__main__\"\
      :\n    print(\"--- Cliente da API Message Broker (v2: Cria Fila se não existir)\
      \ ---\")\n\n    # 1. Obter token de acesso\n    access_token = get_access_token(BASE_URL,\
      \ USERNAME, PASSWORD)\n\n    if not access_token:\n        print(\"\\n--- Falha\
      \ no Login. Abortando. ---\")\n        sys.exit(1)\n\n    print(\"\\n--- Preparação\
      \ da Fila ---\")\n    # 2. Verificar se a fila existe e criar se necessário\n\
      \    queue_is_ready = check_or_create_queue(BASE_URL, QUEUE_NAME, access_token)\n\
      \n    if not queue_is_ready:\n        print(f\"\\n--- Falha ao garantir a existência\
      \ da fila '{QUEUE_NAME}'. Abortando. ---\")\n        sys.exit(1)\n\n    print(\"\
      \\n--- Publicação da Mensagem ---\")\n    # 3. Publicar a mensagem\n    result\
      \ = publish_message_to_queue(BASE_URL, QUEUE_NAME, MESSAGE_CONTENT, access_token)\n\
      \n    if result:\n        print(\"\\n--- Operação Concluída com Sucesso ---\"\
      )\n    else:\n        print(\"\\n--- Falha na Publicação da Mensagem ---\")\n\
      \        sys.exit(1) # Sair com código de erro se a publicação falhar"
    tamanho: 0.01 MB
  libs.txt:
    caminho_completo: .\libs.txt
    numero_de_linhas: 3
    tamanho: 0.00 MB
  message-broker-v1.py:
    caminho_completo: .\message-broker-v1.py
    classes:
    - docstring: null
      end_lineno: 139
      lineno: 104
      name: Settings
    - docstring: null
      end_lineno: 170
      lineno: 156
      name: ColoramaFormatter
    - docstring: null
      end_lineno: 198
      lineno: 172
      name: JsonFormatter
    - docstring: null
      end_lineno: 369
      lineno: 368
      name: QueueBase
    - docstring: null
      end_lineno: 372
      lineno: 371
      name: QueueCreate
    - docstring: null
      end_lineno: 380
      lineno: 374
      name: QueueResponse
    - docstring: null
      end_lineno: 383
      lineno: 382
      name: MessageBase
    - docstring: null
      end_lineno: 386
      lineno: 385
      name: MessageCreate
    - docstring: null
      end_lineno: 395
      lineno: 388
      name: MessageResponse
    - docstring: null
      end_lineno: 400
      lineno: 397
      name: Token
    - docstring: null
      end_lineno: 417
      lineno: 402
      name: StatsResponse
    - docstring: null
      end_lineno: 420
      lineno: 419
      name: LogFileResponse
    - docstring: null
      end_lineno: 424
      lineno: 423
      name: QueueCreatePayload
    - docstring: null
      end_lineno: 427
      lineno: 426
      name: MessagePayload
    - docstring: null
      end_lineno: 431
      lineno: 429
      name: MessagePublishResponse
    - docstring: null
      end_lineno: 438
      lineno: 433
      name: MessageConsumeResponse
    - docstring: null
      end_lineno: 454
      lineno: 441
      name: Queue
    - docstring: null
      end_lineno: 476
      lineno: 456
      name: Message
    - docstring: null
      end_lineno: 1522
      lineno: 1481
      name: QueueGQL
    - docstring: null
      end_lineno: 1543
      lineno: 1525
      name: MessageGQL
    - docstring: null
      end_lineno: 1604
      lineno: 1547
      name: QueryGQL
    - docstring: null
      end_lineno: 1674
      lineno: 1609
      name: MutationGQL
    - docstring: null
      end_lineno: 451
      lineno: 449
      name: Meta
    - docstring: null
      end_lineno: 473
      lineno: 469
      name: Meta
    functions:
    - docstring: null
      end_lineno: 231
      lineno: 231
      name: log_debug
    - docstring: null
      end_lineno: 232
      lineno: 232
      name: log_info
    - docstring: null
      end_lineno: 233
      lineno: 233
      name: log_success
    - docstring: null
      end_lineno: 234
      lineno: 234
      name: log_warning
    - docstring: null
      end_lineno: 235
      lineno: 235
      name: log_error
    - docstring: null
      end_lineno: 236
      lineno: 236
      name: log_critical
    - docstring: null
      end_lineno: 237
      lineno: 237
      name: log_pipeline
    - docstring: null
      end_lineno: 285
      lineno: 254
      name: generate_self_signed_cert
    - docstring: null
      end_lineno: 170
      lineno: 160
      name: format
    - docstring: null
      end_lineno: 175
      lineno: 174
      name: formatTime
    - docstring: null
      end_lineno: 198
      lineno: 177
      name: format
    - docstring: null
      end_lineno: 454
      lineno: 453
      name: __str__
    - docstring: null
      end_lineno: 476
      lineno: 475
      name: __str__
    - docstring: null
      end_lineno: 1017
      lineno: 974
      name: read_and_parse_log_sync
    - docstring: Helper to map from Tortoise ORM model, injecting queue name.
      end_lineno: 1543
      lineno: 1534
      name: from_orm
    imports:
    - asname: null
      name: asyncio
    - asname: null
      name: json
    - asname: null
      name: logging
    - asname: null
      name: os
    - asname: null
      name: platform
    - asname: null
      name: secrets
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: traceback
    - module: contextlib
      names:
      - asynccontextmanager
    - module: datetime
      names:
      - datetime
      - timezone
      - timedelta
    - module: typing
      names:
      - Dict
      - Any
      - Optional
      - List
      - Union
      - AsyncGenerator
    - module: collections
      names:
      - deque
    - asname: null
      name: hashlib
    - asname: null
      name: os
    - asname: null
      name: sys
    - module: fastapi
      names:
      - FastAPI
      - Request
      - Response
      - Depends
      - HTTPException
      - status
      - BackgroundTasks
      - Path
      - Query
    - module: fastapi.middleware.cors
      names:
      - CORSMiddleware
    - module: fastapi.security
      names:
      - OAuth2PasswordBearer
      - OAuth2PasswordRequestForm
      - HTTPBearer
      - HTTPAuthorizationCredentials
    - module: fastapi.responses
      names:
      - JSONResponse
      - StreamingResponse
    - asname: null
      name: uvicorn
    - module: jose
      names:
      - JWTError
      - jwt
    - module: pydantic
      names:
      - BaseModel
      - ValidationError
      - Field
      - EmailStr
      - ConfigDict
    - module: tortoise
      names:
      - Tortoise
      - fields
      - models
    - module: tortoise.exceptions
      names:
      - DoesNotExist
      - IntegrityError
    - module: colorama
      names:
      - init
      - Fore
      - Style
    - module: cryptography
      names:
      - x509
    - module: cryptography.x509.oid
      names:
      - NameOID
    - module: cryptography.hazmat.primitives
      names:
      - hashes
    - module: cryptography.hazmat.backends
      names:
      - default_backend
    - module: cryptography.hazmat.primitives
      names:
      - serialization
    - module: cryptography.hazmat.primitives.asymmetric
      names:
      - rsa
    - asname: null
      name: psutil
    - module: werkzeug.utils
      names:
      - secure_filename
    - module: slowapi
      names:
      - Limiter
      - _rate_limit_exceeded_handler
    - module: slowapi.util
      names:
      - get_remote_address
    - module: slowapi.errors
      names:
      - RateLimitExceeded
    - module: slowapi.middleware
      names:
      - SlowAPIMiddleware
    - asname: null
      name: strawberry
    - module: strawberry.fastapi
      names:
      - GraphQLRouter
    - module: strawberry.types
      names:
      - Info
    - asname: aioredis
      name: redis.asyncio
    - asname: null
      name: ipaddress
    - asname: null
      name: ipaddress
    numero_de_linhas: 1910
    source_code: "# -*- coding: utf-8 -*-\n\"\"\"\nAsync Message Broker API V3.1.1\
      \ - FastAPI + Tortoise ORM + SQLite (Refactored)\n----------------------------------------------------------------------------\n\
      Features:\n- FastAPI async web framework\n- Tortoise ORM with SQLite backend\n\
      - JWT Authentication (access & refresh tokens) via python-jose\n- Message Queues\
      \ (Create, List, Delete)\n- Message Handling (Publish, Consume (async safe),\
      \ Ack, Nack)\n- Server-Sent Events (SSE) via StreamingResponse + Redis Pub/Sub\n\
      - Rate Limiting via SlowAPI + Redis\n- GraphQL endpoint via Strawberry-graphql\n\
      - System Stats collection (psutil)\n- Self-signed certificate generation\n-\
      \ Structured JSON and colored console logging\n- Automatic OpenAPI/Swagger documentation\n\
      - Improved error handling, async operations, and code structure.\n\"\"\"\n\n\
      # --- Standard Library Imports ---\nimport asyncio\nimport json\nimport logging\n\
      import os\nimport platform\nimport secrets\nimport sys\nimport time\nimport\
      \ traceback\nfrom contextlib import asynccontextmanager\nfrom datetime import\
      \ datetime, timezone, timedelta\nfrom typing import Dict, Any, Optional, List,\
      \ Union, AsyncGenerator\nfrom collections import deque # For efficient log tailing\n\
      \n# --- Hashing ---\nimport hashlib\n# from passlib.context import CryptContext\
      \ # Recommended for password hashing (see login endpoint)\n\n# --- Third-Party\
      \ Imports ---\ntry:\n    # Core FastAPI & ASGI\n    from fastapi import (FastAPI,\
      \ Request, Response, Depends, HTTPException, status,\n                     \
      \    BackgroundTasks, Path, Query as FastQuery)\n    from fastapi.middleware.cors\
      \ import CORSMiddleware\n    from fastapi.security import (OAuth2PasswordBearer,\
      \ OAuth2PasswordRequestForm,\n                                  HTTPBearer,\
      \ HTTPAuthorizationCredentials)\n    from fastapi.responses import JSONResponse,\
      \ StreamingResponse\n    import uvicorn\n\n    # JWT Handling\n    from jose\
      \ import JWTError, jwt\n    # Use Field for validation, EmailStr for user examples\n\
      \    from pydantic import BaseModel, ValidationError, Field, EmailStr, ConfigDict\n\
      \n    # Database (Tortoise ORM)\n    from tortoise import Tortoise, fields,\
      \ models\n    # from tortoise.contrib.fastapi import register_tortoise # Not\
      \ needed with lifespan\n    from tortoise.exceptions import DoesNotExist, IntegrityError\n\
      \n    # Logging & Output\n    from colorama import init, Fore, Style\n\n   \
      \ # Certificates\n    from cryptography import x509\n    from cryptography.x509.oid\
      \ import NameOID\n    from cryptography.hazmat.primitives import hashes\n  \
      \  from cryptography.hazmat.backends import default_backend\n    from cryptography.hazmat.primitives\
      \ import serialization\n    from cryptography.hazmat.primitives.asymmetric import\
      \ rsa\n\n    # System Stats\n    import psutil\n\n    # Utilities\n    from\
      \ werkzeug.utils import secure_filename\n\n    # Rate Limiting\n    from slowapi\
      \ import Limiter, _rate_limit_exceeded_handler\n    from slowapi.util import\
      \ get_remote_address\n    from slowapi.errors import RateLimitExceeded\n   \
      \ from slowapi.middleware import SlowAPIMiddleware\n\n    # GraphQL\n    import\
      \ strawberry\n    from strawberry.fastapi import GraphQLRouter\n    from strawberry.types\
      \ import Info\n\n    # Redis Client (for SSE Pub/Sub and Rate Limiting)\n  \
      \  import redis.asyncio as aioredis # Use async redis client\n\nexcept ImportError\
      \ as e:\n    missing_pkg = str(e).split(\"'\")[-2]\n    print(f\"\\nERROR: Missing\
      \ dependency '{missing_pkg}'.\")\n    print(\"Please install all required packages\
      \ by running:\")\n    print(\"\\n  pip install fastapi uvicorn[standard] tortoise-orm\
      \ aiosqlite pydantic[email] python-jose[cryptography] passlib[bcrypt] colorama\
      \ cryptography psutil Werkzeug slowapi redis strawberry-graphql[fastapi] Jinja2\\\
      n\")\n    # Jinja2 needed by slowapi internals\n    sys.exit(1)\n\n# --- Initialize\
      \ Colorama ---\ninit(autoreset=True)\n\n# --- Configuration ---\nclass Settings:\n\
      \    PROJECT_NAME: str = \"Message Broker API V3.1.1 (FastAPI/Refactored)\"\n\
      \    VERSION: str = \"0.3.1.1-fastapi-tortoise\"\n    API_PORT: int = 8777\n\
      \    # Secrets (Use environment variables for production!)\n    JWT_SECRET_KEY:\
      \ str = os.environ.get('JWT_SECRET_KEY', secrets.token_hex(32))\n    ALGORITHM:\
      \ str = \"HS256\"\n    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 1 # 1 hour\n\
      \    REFRESH_TOKEN_EXPIRE_DAYS: int = 30\n    # Database (SQLite Async with\
      \ Tortoise ORM)\n    DB_DIR: str = 'databases'\n    DB_FILENAME: str = 'message_broker_v3.db'\n\
      \    DB_PATH: str = os.path.abspath(os.path.join(DB_DIR, DB_FILENAME))\n   \
      \ DATABASE_URL: str = f\"sqlite:///{DB_PATH}\" # Use triple slash for relative\
      \ paths if needed, but absolute is safer\n    # Redis (For SSE & Rate Limiting)\n\
      \    REDIS_HOST: str = os.environ.get('REDIS_HOST', 'localhost')\n    REDIS_PORT:\
      \ int = int(os.environ.get('REDIS_PORT', 6379))\n    REDIS_URL: str = f\"redis://{REDIS_HOST}:{REDIS_PORT}\"\
      \n    REDIS_SSE_DB: int = 0 # Use DB 0 for SSE Pub/Sub\n    REDIS_RATE_LIMIT_DB:\
      \ int = 1 # Use DB 1 for Rate Limiting\n    # Directories\n    LOG_DIR: str\
      \ = 'logs_v3'\n    CERT_DIR: str = 'certs_v3'\n    # Files\n    CERT_FILE: str\
      \ = os.path.join(CERT_DIR, 'cert.pem')\n    KEY_FILE: str = os.path.join(CERT_DIR,\
      \ 'key_nopass.pem')\n    # CORS\n    ALLOWED_ORIGINS: List[str] = [\"*\"] #\
      \ Be more specific in production\n    # Rate Limiting\n    DEFAULT_RATE_LIMIT:\
      \ str = \"100/minute\"\n    # Logging\n    LOG_LEVEL_STR: str = os.environ.get(\"\
      LOG_LEVEL\", \"INFO\").upper()\n    LOG_LEVEL: int = getattr(logging, LOG_LEVEL_STR,\
      \ logging.INFO)\n    LOG_FORMAT_CONSOLE: str = '%(asctime)s - %(levelname)s\
      \ - %(message)s'\n    # Env type for reload/debug toggle\n    APP_ENV: str =\
      \ os.environ.get(\"APP_ENV\", \"production\").lower()\n\nsettings = Settings()\n\
      \n# --- Create Directories ---\nos.makedirs(settings.LOG_DIR, exist_ok=True)\n\
      os.makedirs(settings.CERT_DIR, exist_ok=True)\nos.makedirs(settings.DB_DIR,\
      \ exist_ok=True)\n\n# --- Logging Setup ---\ntimestamp = datetime.now().strftime(\"\
      %Y%m%d_%H%M%S\")\nunique_hash = hashlib.sha1(str(os.getpid()).encode()).hexdigest()[:8]\n\
      LOG_FILENAME = os.path.join(settings.LOG_DIR, f\"broker_log_{timestamp}_{unique_hash}.json\"\
      )\n\n# --- Logging Setup ---\n# ... (timestamp, unique_hash, LOG_FILENAME remain\
      \ the same) ...\n\nclass ColoramaFormatter(logging.Formatter):\n    LEVEL_COLORS\
      \ = { logging.DEBUG: Fore.CYAN, logging.INFO: Fore.GREEN, logging.WARNING: Fore.YELLOW,\
      \ logging.ERROR: Fore.RED, logging.CRITICAL: Fore.MAGENTA }\n    LEVEL_ICONS\
      \ = { logging.DEBUG: \"⚙️ \", logging.INFO: \"ℹ️ \", logging.WARNING: \"⚠️ \"\
      , logging.ERROR: \"❌ \", logging.CRITICAL: \"\U0001F525 \", 'SUCCESS': \"✅ \"\
      , 'PIPELINE': \"➡️ \", 'DB': \"\U0001F4BE \", 'AUTH': \"\U0001F511 \", 'QUEUE':\
      \ \"\U0001F4E5 \", 'MSG': \"✉️ \", 'HTTP': \"\U0001F310 \", 'STATS': \"\U0001F4CA\
      \ \", 'LOGS': \"\U0001F4C4 \", 'SEC': \"\U0001F6E1️ \", 'ASYNC': \"⚡ \", 'GRAPHQL':\
      \ \"\U0001F353 \", 'SSE': \"\U0001F4E1 \", 'RATELIMIT': \"⏱️ \", 'STARTUP':\
      \ '\U0001F680', 'SHUTDOWN': '\U0001F6D1'}\n    # --- CORRECTED FORMAT METHOD\
      \ ---\n    def format(self, record):\n        level_color = self.LEVEL_COLORS.get(record.levelno,\
      \ Fore.WHITE)\n        icon_type = getattr(record, 'icon_type', record.levelname)\
      \ # Use icon_type if provided\n        icon = self.LEVEL_ICONS.get(icon_type,\
      \ \"\")\n\n        # record.asctime is already formatted by the time format()\
      \ is called,\n        # based on the datefmt provided during Formatter initialization\
      \ or to the Handler.\n        # We will set the datefmt when creating the formatter\
      \ instance below.\n        log_message_content = f\"[{record.levelname}] {icon}{record.getMessage()}\"\
      \n        log_line = f\"{record.asctime} - {record.name} - {level_color}{Style.BRIGHT}{log_message_content}{Style.RESET_ALL}\"\
      \n        return log_line\n\nclass JsonFormatter(logging.Formatter):\n    #\
      \ Keep your existing JsonFormatter as it correctly uses datetime.isoformat()\n\
      \    def formatTime(self, record, datefmt=None):\n        return datetime.fromtimestamp(record.created,\
      \ tz=timezone.utc).isoformat(timespec='milliseconds').replace('+00:00', 'Z')\n\
      \n    def format(self, record):\n        log_record = {\n            'timestamp':\
      \ self.formatTime(record),\n            'level': record.levelname,\n       \
      \     'name': record.name,\n            'pid': record.process,\n           \
      \ 'thread': record.threadName,\n            'message': record.getMessage(),\n\
      \            'pathname': record.pathname,\n            'lineno': record.lineno,\n\
      \        }\n        if hasattr(record, 'icon_type'):\n            log_record['icon_type']\
      \ = record.icon_type\n        if hasattr(record, 'extra_data') and isinstance(record.extra_data,\
      \ dict):\n            log_record.update(record.extra_data)\n        if record.exc_info:\n\
      \            log_record['exception'] = {\n                'type': record.exc_info[0].__name__,\n\
      \                'value': str(record.exc_info[1]),\n                'traceback':\
      \ \"\".join(traceback.format_exception(*record.exc_info)) if settings.APP_ENV\
      \ == 'development' else 'Traceback hidden in production'\n            }\n  \
      \      return json.dumps(log_record, ensure_ascii=False, default=str)\n\n# Configure\
      \ root logger slightly differently if needed, but focusing on app logger\nlogger\
      \ = logging.getLogger(\"MessageBroker\")\nlogger.setLevel(settings.LOG_LEVEL)\n\
      logger.propagate = False # Prevent duplicate logs if root logger also has handlers\n\
      \n# Define the date format string ONCE\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S' #\
      \ REMOVED %f\n\n# Avoid adding handlers multiple times if script is reloaded\n\
      if not logger.handlers:\n    # --- CONSOLE HANDLER ---\n    console_handler\
      \ = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(settings.LOG_LEVEL)\n\
      \    # Instantiate the formatter and pass the date format\n    console_formatter\
      \ = ColoramaFormatter(datefmt=DATE_FORMAT)\n    console_handler.setFormatter(console_formatter)\n\
      \n    # --- FILE HANDLER ---\n    file_handler = logging.FileHandler(LOG_FILENAME,\
      \ mode='a', encoding='utf-8')\n    file_handler.setLevel(settings.LOG_LEVEL)\n\
      \    # JsonFormatter handles its own timestamp formatting internally, no datefmt\
      \ needed here\n    file_formatter = JsonFormatter()\n    file_handler.setFormatter(file_formatter)\n\
      \n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n\
      \n# --- Logging Helper Functions ---\n# ... (keep your log_debug, log_info,\
      \ etc. functions as they are) ...\n\n# --- Logging Helper Functions ---\ndef\
      \ log_debug(message: str, icon_type: str = 'DEBUG', extra: Optional[Dict[str,\
      \ Any]] = None): logger.debug(message, extra={'icon_type': icon_type, 'extra_data':\
      \ extra or {}})\ndef log_info(message: str, icon_type: str = 'INFO', extra:\
      \ Optional[Dict[str, Any]] = None): logger.info(message, extra={'icon_type':\
      \ icon_type, 'extra_data': extra or {}})\ndef log_success(message: str, icon_type:\
      \ str = 'SUCCESS', extra: Optional[Dict[str, Any]] = None): logger.info(message,\
      \ extra={'icon_type': icon_type, 'extra_data': extra or {}})\ndef log_warning(message:\
      \ str, icon_type: str = 'WARNING', extra: Optional[Dict[str, Any]] = None):\
      \ logger.warning(message, extra={'icon_type': icon_type, 'extra_data': extra\
      \ or {}})\ndef log_error(message: str, exc_info: bool = False, icon_type: str\
      \ = 'ERROR', extra: Optional[Dict[str, Any]] = None): logger.error(message,\
      \ exc_info=exc_info, extra={'icon_type': icon_type, 'extra_data': extra or {}})\n\
      def log_critical(message: str, exc_info: bool = False, icon_type: str = 'CRITICAL',\
      \ extra: Optional[Dict[str, Any]] = None): logger.critical(message, exc_info=exc_info,\
      \ extra={'icon_type': icon_type, 'extra_data': extra or {}})\ndef log_pipeline(message:\
      \ str, icon_type: str = 'PIPELINE', extra: Optional[Dict[str, Any]] = None):\
      \ logger.info(message, extra={'icon_type': icon_type, 'extra_data': extra or\
      \ {}})\n\n\n# --- Password Hashing Context ---\n# !!! IMPORTANT SECURITY WARNING\
      \ !!!\n# The hardcoded 'admin':'admin' password in the /login route is INSECURE.\n\
      # For production, uncomment and use passlib for proper password hashing.\n#\
      \ Example:\n# pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"\
      auto\")\n# def verify_password(plain_password, hashed_password):\n#     return\
      \ pwd_context.verify(plain_password, hashed_password)\n# def get_password_hash(password):\n\
      #     return pwd_context.hash(password)\n# You would then store the HASHED password,\
      \ not the plain one.\n\n\n# --- Certificate Generation ---\ndef generate_self_signed_cert(cert_path:\
      \ str, key_path: str, key_password: Optional[bytes] = None, common_name: str\
      \ = \"localhost\"):\n    log_info(f\"\U0001F6E1️ Generating new RSA private\
      \ key and self-signed certificate for CN='{common_name}'...\", icon_type='SEC')\n\
      \    try:\n        private_key = rsa.generate_private_key(public_exponent=65537,\
      \ key_size=2048, backend=default_backend())\n        subject = issuer = x509.Name([\n\
      \            x509.NameAttribute(NameOID.COUNTRY_NAME, u\"XX\"), x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME,\
      \ u\"Default\"),\n            x509.NameAttribute(NameOID.LOCALITY_NAME, u\"\
      Default\"), x509.NameAttribute(NameOID.ORGANIZATION_NAME, u\"Message Broker\
      \ V3\"),\n            x509.NameAttribute(NameOID.COMMON_NAME, common_name),\n\
      \        ])\n        # Add Subject Alternative Name (SAN) for browser compatibility\
      \ (localhost + 127.0.0.1)\n        san_extension = x509.SubjectAlternativeName([x509.DNSName(common_name),\
      \ x509.IPAddress(ipaddress.ip_address(\"127.0.0.1\"))])\n        import ipaddress\
      \ # Need to import this\n\n        cert_builder = x509.CertificateBuilder().subject_name(subject).issuer_name(issuer).public_key(private_key.public_key())\
      \ \\\n            .serial_number(x509.random_serial_number()).not_valid_before(datetime.now(timezone.utc))\
      \ \\\n            .not_valid_after(datetime.now(timezone.utc) + timedelta(days=365))\
      \ \\\n            .add_extension(san_extension, critical=False) # Add SAN\n\
      \        certificate = cert_builder.sign(private_key, hashes.SHA256(), default_backend())\n\
      \n        key_pem_encryption = serialization.NoEncryption()\n        if key_password:\
      \ key_pem_encryption = serialization.BestAvailableEncryption(key_password)\n\
      \        with open(key_path, \"wb\") as f: f.write(private_key.private_bytes(encoding=serialization.Encoding.PEM,\
      \ format=serialization.PrivateFormat.PKCS8, encryption_algorithm=key_pem_encryption))\n\
      \        log_success(f\"\U0001F511 Private key saved: {key_path}\", icon_type='SEC')\n\
      \        with open(cert_path, \"wb\") as f: f.write(certificate.public_bytes(serialization.Encoding.PEM))\n\
      \        log_success(f\"\U0001F4DC Self-signed certificate saved: {cert_path}\"\
      , icon_type='SEC')\n        return True\n    except ImportError:\n        log_critical(\"\
      The 'ipaddress' module is needed for certificate generation with IP SAN. Please\
      \ install it.\", icon_type='CRITICAL')\n        return False\n    except Exception\
      \ as e:\n        log_critical(f\"Failed to generate certificates/key: {e}\"\
      , exc_info=True, icon_type='CRITICAL')\n        return False\n\n# --- Application\
      \ Statistics ---\napp_stats: Dict[str, Any] = {\n    \"start_time\": datetime.now(timezone.utc),\n\
      \    \"requests_total\": 0, \"requests_by_route\": {}, \"requests_by_status\"\
      : {},\n    \"queues_total\": 0, \"messages_total\": 0, \"messages_pending\"\
      : 0,\n    \"messages_processing\": 0, \"messages_processed\": 0, \"messages_failed\"\
      : 0,\n    \"last_error\": None,\n    \"system\": {\n        \"python_version\"\
      : platform.python_version(), \"platform\": platform.system(),\n        \"platform_release\"\
      : platform.release(), \"architecture\": platform.machine(),\n    },\n    \"\
      broker_specific\": {\n        \"framework\": \"FastAPI\",\n        \"version\"\
      : settings.VERSION,\n        \"db_engine\": \"sqlite (tortoise-orm)\",\n   \
      \     \"auth_method\": \"jwt (access+refresh, python-jose)\",\n        \"notification\"\
      : \"sse (redis pub/sub)\",\n        \"rate_limit\": \"redis (slowapi)\",\n \
      \       \"graphql\": \"strawberry-graphql\"\n    }\n}\nstats_lock = asyncio.Lock()\
      \ # Use asyncio Lock for safe async updates\n\nasync def update_request_stats(route_template:\
      \ str, method: str, status_code: int):\n    \"\"\"Updates request counters asynchronously\
      \ and safely.\"\"\"\n    async with stats_lock:\n        app_stats[\"requests_total\"\
      ] += 1\n        route_stats = app_stats[\"requests_by_route\"].setdefault(route_template,\
      \ {})\n        route_stats[method] = route_stats.get(method, 0) + 1\n      \
      \  app_stats[\"requests_by_status\"][str(status_code)] = app_stats[\"requests_by_status\"\
      ].get(str(status_code), 0) + 1\n\nasync def update_broker_stats():\n    \"\"\
      \"Updates broker stats from the database asynchronously and concurrently.\"\"\
      \"\n    log_pipeline(\"\U0001F4CA Fetching broker stats from DB...\", icon_type='STATS')\n\
      \    try:\n        q_count_task = Queue.all().count()\n        m_pending_task\
      \ = Message.filter(status='pending').count()\n        m_processing_task = Message.filter(status='processing').count()\n\
      \        m_processed_task = Message.filter(status='processed').count()\n   \
      \     m_failed_task = Message.filter(status='failed').count()\n\n        q_count,\
      \ pending, processing, processed, failed = await asyncio.gather(\n         \
      \   q_count_task, m_pending_task, m_processing_task, m_processed_task, m_failed_task\n\
      \        )\n        total = pending + processing + processed + failed\n\n  \
      \      async with stats_lock:\n            app_stats[\"queues_total\"] = q_count\n\
      \            app_stats[\"messages_pending\"] = pending\n            app_stats[\"\
      messages_processing\"] = processing\n            app_stats[\"messages_processed\"\
      ] = processed\n            app_stats[\"messages_failed\"] = failed\n       \
      \     app_stats[\"messages_total\"] = total\n            # Clear last error\
      \ on successful update if it was set\n            if app_stats[\"last_error\"\
      ] and \"Broker Stats Update Failed\" in app_stats[\"last_error\"]:\n       \
      \          app_stats[\"last_error\"] = None\n\n        log_success(\"\U0001F4CA\
      \ Broker stats updated.\", icon_type='STATS', extra={'counts': {'queues': q_count,\
      \ 'pending': pending, 'processing': processing, 'processed': processed, 'failed':\
      \ failed}})\n\n    except Exception as e:\n        log_error(f\"Error updating\
      \ broker stats: {e}\", icon_type='STATS', exc_info=True)\n        async with\
      \ stats_lock:\n            app_stats[\"last_error\"] = f\"Broker Stats Update\
      \ Failed: {datetime.now(timezone.utc).isoformat()}\"\n\n# --- Database Setup\
      \ (Tortoise ORM with SQLite) ---\nasync def init_tortoise():\n    \"\"\"Initialize\
      \ Tortoise ORM and create schemas if they don't exist.\"\"\"\n    log_info(f\"\
      \U0001F4BE Configuring Tortoise ORM for SQLite: {settings.DATABASE_URL}\", icon_type='DB')\n\
      \    try:\n        await Tortoise.init(\n            db_url=settings.DATABASE_URL,\n\
      \            modules={'models': ['__main__']} # Models are in this script\n\
      \        )\n        await Tortoise.generate_schemas(safe=True) # safe=True avoids\
      \ errors if tables exist\n        log_success(\"\U0001F4BE ORM tables verified/created\
      \ successfully.\", icon_type='DB')\n        await update_broker_stats() # Populate\
      \ initial stats\n    except Exception as e:\n        log_critical(f\"Fatal:\
      \ Failed to initialize Tortoise ORM: {e}\", icon_type='CRITICAL', exc_info=True)\n\
      \        sys.exit(1)\n\n# --- Pydantic Models (Input/Output Schemas) ---\nclass\
      \ QueueBase(BaseModel):\n    name: str = Field(..., min_length=1, max_length=255,\
      \ description=\"Unique name for the queue\")\n\nclass QueueCreate(QueueBase):\n\
      \    pass\n\nclass QueueResponse(QueueBase):\n    id: int\n    created_at: datetime\n\
      \    updated_at: datetime\n    message_count: int = Field(default=0, description=\"\
      Current number of messages in the queue\")\n\n    model_config = ConfigDict(from_attributes=True)\
      \ # Pydantic v2 ORM mode\n\nclass MessageBase(BaseModel):\n    content: str\
      \ = Field(..., min_length=1, description=\"The content/payload of the message\"\
      )\n\nclass MessageCreate(MessageBase):\n    pass\n\nclass MessageResponse(MessageBase):\n\
      \    id: int\n    queue_id: int\n    status: str = Field(description=\"Current\
      \ status: pending, processing, processed, failed\")\n    created_at: datetime\n\
      \    updated_at: datetime\n\n    model_config = ConfigDict(from_attributes=True)\n\
      \nclass Token(BaseModel):\n    access_token: str\n    refresh_token: str\n \
      \   token_type: str = \"bearer\"\n\nclass StatsResponse(BaseModel):\n    start_time:\
      \ str = Field(description=\"ISO 8601 timestamp when the server started (UTC)\"\
      )\n    uptime_seconds: float = Field(description=\"Server uptime in seconds\"\
      )\n    uptime_human: str = Field(description=\"Human-readable server uptime\
      \ (e.g., 1d 2h 30m 15s)\")\n    requests_total: int\n    requests_by_route:\
      \ Dict[str, Dict[str, int]]\n    requests_by_status: Dict[str, int] # Keys are\
      \ string representations of status codes\n    queues_total: int\n    messages_total:\
      \ int\n    messages_pending: int\n    messages_processing: int\n    messages_processed:\
      \ int\n    messages_failed: int\n    last_error: Optional[str] = Field(None,\
      \ description=\"Timestamp and type of the last unhandled error, if any\")\n\
      \    system: Dict[str, Any] = Field(description=\"System metrics (CPU, Memory,\
      \ Disk, etc.)\")\n    broker_specific: Dict[str, str] = Field(description=\"\
      Broker implementation details\")\n\nclass LogFileResponse(BaseModel):\n    log_files:\
      \ List[str]\n\n# Payload Models for specific endpoints\nclass QueueCreatePayload(BaseModel):\n\
      \    name: str = Field(..., min_length=1, max_length=255)\n\nclass MessagePayload(BaseModel):\n\
      \    content: str = Field(..., min_length=1)\n\nclass MessagePublishResponse(BaseModel):\n\
      \    message: str = Field(default=\"Message published successfully\")\n    message_id:\
      \ int\n\nclass MessageConsumeResponse(BaseModel):\n    message_id: int\n   \
      \ queue: str\n    content: str\n    status: str = Field(default='processing')\
      \ # Status after consumption\n    retrieved_at: datetime\n\n# --- Tortoise ORM\
      \ Models ---\nclass Queue(models.Model):\n    id = fields.IntField(pk=True)\n\
      \    name = fields.CharField(max_length=255, unique=True, index=True, description=\"\
      Unique queue name\")\n    created_at = fields.DatetimeField(auto_now_add=True)\n\
      \    updated_at = fields.DatetimeField(auto_now=True)\n    # Relationship accessor\
      \ from Message -> Queue\n    messages: fields.ReverseRelation[\"Message\"]\n\
      \n    class Meta:\n        table = \"queues\"\n        ordering = [\"name\"\
      ]\n\n    def __str__(self):\n        return self.name\n\nclass Message(models.Model):\n\
      \    id = fields.IntField(pk=True)\n    queue: fields.ForeignKeyRelation[Queue]\
      \ = fields.ForeignKeyField(\n        'models.Queue', related_name='messages',\
      \ on_delete=fields.CASCADE, description=\"The queue this message belongs to\"\
      \n    )\n    content = fields.TextField(description=\"Message payload\")\n \
      \   status = fields.CharField(\n        max_length=20, default='pending', index=True,\n\
      \        description=\"Status: pending, processing, processed, failed\"\n  \
      \  )\n    created_at = fields.DatetimeField(auto_now_add=True, index=True)\n\
      \    updated_at = fields.DatetimeField(auto_now=True)\n\n    class Meta:\n \
      \       table = \"messages\"\n        # Composite index to speed up finding\
      \ the oldest pending message in a queue\n        indexes = [(\"queue_id\", \"\
      status\", \"created_at\")]\n        ordering = [\"created_at\"] # Default ordering\n\
      \n    def __str__(self):\n        return f\"Message {self.id} ({self.status})\"\
      \n\n\n# --- Redis Connection Pool ---\nredis_sse_pool: Optional[aioredis.ConnectionPool]\
      \ = None\nredis_rate_limit_pool: Optional[aioredis.ConnectionPool] = None\n\
      redis_sse: Optional[aioredis.Redis] = None\nredis_limiter_client: Optional[aioredis.Redis]\
      \ = None\n\nasync def setup_redis():\n    \"\"\"Initializes Redis connection\
      \ pools and clients.\"\"\"\n    global redis_sse_pool, redis_rate_limit_pool,\
      \ redis_sse, redis_limiter_client\n    try:\n        log_pipeline(\"\U0001F4E1\
      \ Configuring Redis connections...\", icon_type='SSE')\n        redis_sse_pool\
      \ = aioredis.ConnectionPool.from_url(f\"{settings.REDIS_URL}/{settings.REDIS_SSE_DB}\"\
      , decode_responses=True, max_connections=20, health_check_interval=30)\n   \
      \     redis_rate_limit_pool = aioredis.ConnectionPool.from_url(f\"{settings.REDIS_URL}/{settings.REDIS_RATE_LIMIT_DB}\"\
      , decode_responses=True, max_connections=20, health_check_interval=30)\n\n \
      \       redis_sse = aioredis.Redis(connection_pool=redis_sse_pool)\n       \
      \ redis_limiter_client = aioredis.Redis(connection_pool=redis_rate_limit_pool)\n\
      \n        # Verify connections\n        await redis_sse.ping()\n        await\
      \ redis_limiter_client.ping()\n        log_success(\"\U0001F4E1 Redis connections\
      \ established and verified.\", icon_type='SSE')\n        return True\n\n   \
      \ except aioredis.RedisError as e:\n        log_critical(f\"Fatal: Failed to\
      \ connect to Redis at {settings.REDIS_URL}: {e}. SSE/Rate Limiting unavailable.\"\
      , icon_type='CRITICAL', exc_info=True)\n        # Allow startup without Redis?\
      \ Depends on requirements. Set clients to None.\n        redis_sse = None\n\
      \        redis_limiter_client = None\n        return False\n    except Exception\
      \ as e:\n        log_critical(f\"Fatal: Unexpected error during Redis setup:\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        redis_sse = None\n  \
      \      redis_limiter_client = None\n        return False # Treat unexpected\
      \ error as critical failure for Redis\n\nasync def shutdown_redis():\n    \"\
      \"\"Closes Redis connections and pools gracefully.\"\"\"\n    log_pipeline(\"\
      \U0001F50C Closing Redis connections...\", icon_type='SHUTDOWN')\n    # Close\
      \ clients first\n    if redis_sse:\n        try: await redis_sse.close()\n \
      \       except Exception as e: log_warning(f\"Error closing Redis SSE client:\
      \ {e}\", icon_type='SSE')\n    if redis_limiter_client:\n        try: await\
      \ redis_limiter_client.close()\n        except Exception as e: log_warning(f\"\
      Error closing Redis Limiter client: {e}\", icon_type='RATELIMIT')\n    # Then\
      \ disconnect pools\n    if redis_sse_pool:\n        try: await redis_sse_pool.disconnect(inuse_connections=True)\n\
      \        except Exception as e: log_warning(f\"Error disconnecting Redis SSE\
      \ pool: {e}\", icon_type='SSE')\n    if redis_rate_limit_pool:\n        try:\
      \ await redis_rate_limit_pool.disconnect(inuse_connections=True)\n        except\
      \ Exception as e: log_warning(f\"Error disconnecting Redis Limiter pool: {e}\"\
      , icon_type='RATELIMIT')\n    log_success(\"\U0001F50C Redis connections/pools\
      \ closed.\", icon_type='SHUTDOWN')\n\n# --- Lifespan Context Manager (Startup/Shutdown\
      \ Events) ---\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n   \
      \ # Startup Sequence\n    log_info(\"\U0001F680 Application Startup Initiated...\"\
      , icon_type='STARTUP')\n    # 1. Initialize Tortoise ORM\n    await init_tortoise()\n\
      \    # 2. Initialize Redis\n    redis_ok = await setup_redis()\n    limiter_client\
      \ = None # Initialize to None\n    if redis_ok:\n        # Store clients in\
      \ app state for potential direct access (optional)\n        app.state.redis_sse\
      \ = redis_sse\n        app.state.redis_limiter = redis_limiter_client\n    \
      \    limiter_client = redis_limiter_client # Get the client for SlowAPI\n  \
      \  else:\n        log_warning(\"Redis setup failed, proceeding without Redis\
      \ features (SSE, Rate Limiting).\", icon_type='WARNING')\n        app.state.redis_sse\
      \ = None\n        app.state.redis_limiter = None\n\n    # --- MOVED FROM @app.on_event\
      \ ---\n    # 3. Configure and add SlowAPI middleware AFTER Redis client is potentially\
      \ initialized\n    if limiter_client:\n        app.add_middleware(SlowAPIMiddleware,\
      \ redis_client=limiter_client)\n        log_info(f\"⏱️ Rate Limiter configured\
      \ with Redis backend (DB {settings.REDIS_RATE_LIMIT_DB}).\", icon_type='RATELIMIT')\n\
      \    else:\n        # Fallback to in-memory store if Redis client setup failed\n\
      \        app.add_middleware(SlowAPIMiddleware)\n        log_warning(\"⏱️ Rate\
      \ Limiter configured with In-Memory backend (Redis unavailable).\", icon_type='RATELIMIT')\n\
      \    # --- END MOVED SECTION ---\n\n    log_success(\"\U0001F680 Application\
      \ Startup Complete.\", icon_type='STARTUP')\n    yield\n    # Shutdown Sequence\n\
      \    log_info(\"\U0001F6D1 Application Shutdown Initiated...\", icon_type='SHUTDOWN')\n\
      \    # 1. Close Redis Connections\n    await shutdown_redis()\n    # 2. Close\
      \ Database Connections\n    try:\n        await Tortoise.close_connections()\n\
      \        log_success(\"\U0001F4BE Database connections closed.\", icon_type='DB')\n\
      \    except Exception as e:\n         log_warning(f\"Error closing Tortoise\
      \ connections: {e}\", icon_type='DB')\n    log_success(\"\U0001F6D1 Application\
      \ Shutdown Complete.\", icon_type='SHUTDOWN')\n\n# --- FastAPI Application Setup\
      \ ---\nlog_info(f\"\U0001F680 Initializing FastAPI Application ({settings.PROJECT_NAME}\
      \ v{settings.VERSION})...\", icon_type='STARTUP')\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n\
      \    version=settings.VERSION,\n    description=__doc__.split('---')[0].strip(),\
      \ # Use module docstring\n    lifespan=lifespan,\n    docs_url=\"/docs\",\n\
      \    redoc_url=\"/redoc\",\n    openapi_tags=[ # Define tags for better organization\
      \ in Swagger UI\n        {\"name\": \"General\", \"description\": \"Basic health\
      \ and info endpoints\"},\n        {\"name\": \"Authentication\", \"description\"\
      : \"User login and token management\"},\n        {\"name\": \"Monitoring\",\
      \ \"description\": \"System stats and log viewing\"},\n        {\"name\": \"\
      Queues\", \"description\": \"Operations for managing message queues\"},\n  \
      \      {\"name\": \"Messages\", \"description\": \"Publishing, consuming, and\
      \ managing messages\"},\n        {\"name\": \"Realtime (SSE)\", \"description\"\
      : \"Server-Sent Event streams for queue updates\"},\n        {\"name\": \"GraphQL\"\
      , \"description\": \"GraphQL API endpoint\"},\n    ]\n)\n\n# --- Rate Limiter\
      \ Setup (SlowAPI with Async Redis or Memory Fallback) ---\nlimiter = Limiter(key_func=get_remote_address,\
      \ default_limits=[settings.DEFAULT_RATE_LIMIT])\napp.state.limiter = limiter\
      \ # Make limiter available globally if needed\napp.add_exception_handler(RateLimitExceeded,\
      \ _rate_limit_exceeded_handler)\n\n@app.on_event(\"startup\")\nasync def startup_configure_slowapi_middleware():\n\
      \    \"\"\"Adds SlowAPI middleware after Redis client is potentially initialized\
      \ by lifespan.\"\"\"\n    # Access the client stored in app.state by the lifespan\
      \ manager\n    limiter_client = getattr(app.state, \"redis_limiter\", None)\n\
      \    if limiter_client:\n        app.add_middleware(SlowAPIMiddleware, redis_client=limiter_client)\n\
      \        log_info(f\"⏱️ Rate Limiter configured with Redis backend (DB {settings.REDIS_RATE_LIMIT_DB}).\"\
      , icon_type='RATELIMIT')\n    else:\n        # Fallback to in-memory store if\
      \ Redis client setup failed\n        app.add_middleware(SlowAPIMiddleware)\n\
      \        log_warning(\"⏱️ Rate Limiter configured with In-Memory backend (Redis\
      \ unavailable).\", icon_type='RATELIMIT')\n\n# --- CORS Middleware ---\napp.add_middleware(\n\
      \    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n\
      \    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\nlog_info(f\"\U0001F6E1\
      ️ CORS configured for origins: {settings.ALLOWED_ORIGINS}\", icon_type='SEC')\n\
      \n# --- Authentication Dependencies ---\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"\
      login\", auto_error=False) # auto_error=False to handle optional auth later\
      \ if needed\nbearer_scheme = HTTPBearer(auto_error=False)\n\nasync def create_jwt_token(data:\
      \ dict, expires_delta: timedelta) -> str:\n    to_encode = data.copy()\n   \
      \ expire = datetime.now(timezone.utc) + expires_delta\n    to_encode.update({\"\
      exp\": expire, \"iat\": datetime.now(timezone.utc)})\n    encoded_jwt = jwt.encode(to_encode,\
      \ settings.JWT_SECRET_KEY, algorithm=settings.ALGORITHM)\n    return encoded_jwt\n\
      \nasync def create_access_token(username: str) -> str:\n    return await create_jwt_token(\n\
      \        {\"sub\": username, \"type\": \"access\"}, timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n\
      \    )\n\nasync def create_refresh_token(username: str) -> str:\n    return\
      \ await create_jwt_token(\n        {\"sub\": username, \"type\": \"refresh\"\
      }, timedelta(days=settings.REFRESH_TOKEN_EXPIRE_DAYS)\n    )\n\n# Unified Token\
      \ Decode Logic\nasync def _decode_token(token: str, expected_type: str) -> str:\n\
      \    \"\"\"Decodes JWT, validates type and sub, returns username or raises HTTPException.\"\
      \"\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n\
      \        detail=f\"Could not validate {expected_type} token\",\n        headers={\"\
      WWW-Authenticate\": f\"Bearer error=\\\"invalid_token\\\", error_description=\\\
      \"Invalid {expected_type} token\\\"\"},\n    )\n    if not token:\n        raise\
      \ credentials_exception\n\n    try:\n        payload = jwt.decode(\n       \
      \      token,\n             settings.JWT_SECRET_KEY,\n             algorithms=[settings.ALGORITHM],\n\
      \             options={\"verify_aud\": False} # No audience verification for\
      \ this example\n        )\n        username: Optional[str] = payload.get(\"\
      sub\")\n        token_type: Optional[str] = payload.get(\"type\")\n\n      \
      \  if username is None or token_type != expected_type:\n            log_warning(f\"\
      {expected_type.capitalize()} token validation failed: 'sub' missing or type\
      \ mismatch ('{token_type}' != '{expected_type}').\", icon_type='AUTH', extra={\"\
      payload\": payload})\n            raise credentials_exception\n\n        # log_debug(f\"\
      User '{username}' authenticated via {expected_type} token.\", icon_type='AUTH')\n\
      \        return username\n    except JWTError as e:\n        log_warning(f\"\
      {expected_type.capitalize()} token validation JWTError: {e}\", icon_type='AUTH',\
      \ extra={\"token\": token[:10] + \"...\"})\n        raise credentials_exception\n\
      \    except Exception as e:\n         log_error(f\"Unexpected error during {expected_type}\
      \ token decode: {e}\", icon_type='AUTH', exc_info=True)\n         raise credentials_exception\
      \ # Re-raise as the original unauthorized exception\n\n# Dependency for Access\
      \ Token\nasync def get_current_user(token: Optional[str] = Depends(oauth2_scheme))\
      \ -> str:\n    \"\"\"Dependency to validate JWT access token and return the\
      \ username.\"\"\"\n    # If token is None (because auto_error=False), raise\
      \ manually\n    if token is None:\n         raise HTTPException(\n         \
      \   status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\"\
      ,\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return\
      \ await _decode_token(token, \"access\")\n\n# Dependency for Refresh Token (expects\
      \ Bearer header)\nasync def validate_refresh_token(credentials: Optional[HTTPAuthorizationCredentials]\
      \ = Depends(bearer_scheme)) -> str:\n    \"\"\"Dependency to validate JWT refresh\
      \ token from Bearer header and return username.\"\"\"\n    if credentials is\
      \ None:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n\
      \            detail=\"Refresh token missing or invalid header\",\n         \
      \   headers={\"WWW-Authenticate\": \"Bearer error=\\\"invalid_request\\\"\"\
      },\n        )\n    return await _decode_token(credentials.credentials, \"refresh\"\
      )\n\n\n# --- Stats Update Middleware ---\n@app.middleware(\"http\")\nasync def\
      \ update_stats_middleware(request: Request, call_next):\n    start_time_mw =\
      \ time.perf_counter()\n    response = await call_next(request)\n    process_time_mw\
      \ = time.perf_counter() - start_time_mw\n    response.headers[\"X-Process-Time\"\
      ] = f\"{process_time_mw:.4f}s\" # Add units\n\n    # Update request stats after\
      \ response is generated\n    route = request.scope.get(\"route\")\n    if route\
      \ and hasattr(route, 'path'):\n        route_template = route.path\n       \
      \ # More robust ignore list, also handles potential None route path\n      \
      \  ignored_prefixes = ('/docs', '/redoc', '/openapi.json', '/stream', '/logs',\
      \ '/graphql', '/favicon.ico', '/stats')\n        if route_template and not route_template.startswith(ignored_prefixes):\n\
      \            await update_request_stats(route_template, request.method, response.status_code)\n\
      \n    return response\n\n# --- Helper Function for DB Lookups ---\nasync def\
      \ _get_queue_or_404(queue_name: str, conn=None) -> Queue:\n    \"\"\"Fetches\
      \ a queue by name or raises HTTPException 404. Can use existing transaction\
      \ connection.\"\"\"\n    try:\n        query = Queue.all()\n        if conn:\
      \ query = query.using_connection(conn)\n        queue = await query.get(name=queue_name)\n\
      \        return queue\n    except DoesNotExist:\n        log_warning(f\"Queue\
      \ '{queue_name}' not found in database.\", icon_type='DB')\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,\
      \ detail=f\"Queue '{queue_name}' not found\")\n    except Exception as e:\n\
      \        log_error(f\"Database error fetching queue '{queue_name}': {e}\", icon_type='DB',\
      \ exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Database error retrieving queue\")\n\n# --- SSE Notification Function\
      \ ---\nasync def notify_sse(queue_name: str, message_id: int, event_type: str):\n\
      \    \"\"\"Publishes a notification via Redis Pub/Sub for SSE listeners.\"\"\
      \"\n    if not redis_sse:\n        log_warning(f\"Cannot send SSE notification\
      \ for queue '{queue_name}': Redis SSE client unavailable.\", icon_type='SSE')\n\
      \        return\n    try:\n        sse_data = json.dumps({\"queue\": queue_name,\
      \ \"message_id\": message_id, \"event\": event_type, \"timestamp\": datetime.now(timezone.utc).isoformat()})\n\
      \        redis_channel = f\"sse:{queue_name}\"\n        # log_pipeline(f\"\U0001F4E1\
      \ Publishing SSE to Redis '{redis_channel}': {sse_data}\", icon_type='SSE')\
      \ # Can be verbose\n        published_count = await redis_sse.publish(redis_channel,\
      \ sse_data)\n        if published_count > 0:\n             log_debug(f\"\U0001F4E1\
      \ SSE published to '{redis_channel}' ({published_count} listeners). Data: {sse_data}\"\
      , icon_type='SSE')\n        else:\n             log_debug(f\"\U0001F4E1 SSE\
      \ published to '{redis_channel}' (0 listeners). Data: {sse_data}\", icon_type='SSE')\n\
      \n    except aioredis.RedisError as e:\n        log_error(f\"Redis error publishing\
      \ SSE for queue '{queue_name}': {e}\", icon_type='SSE', exc_info=True)\n   \
      \ except Exception as e:\n        log_error(f\"Error preparing/publishing SSE\
      \ notification for queue '{queue_name}': {e}\", icon_type='SSE', exc_info=True)\n\
      \n\n# --- =================== ---\n# --- API ROUTE DEFINITIONS ---\n# --- ===================\
      \ ---\n\n# --- General Routes ---\n@app.get(\"/\", tags=[\"General\"], summary=\"\
      Health Check\")\n@limiter.limit(\"5/second\")\nasync def index(request: Request):\n\
      \    \"\"\"Provides a basic health check and server information.\"\"\"\n   \
      \ log_info(\"\U0001F310 GET / request\", icon_type='HTTP', extra={\"client\"\
      : request.client.host})\n    return {\n        \"message\": settings.PROJECT_NAME,\n\
      \        \"status\": \"ok\",\n        \"version\": settings.VERSION,\n     \
      \   \"timestamp\": datetime.now(timezone.utc).isoformat()\n    }\n\n# --- Authentication\
      \ Routes ---\n@app.post(\"/login\", response_model=Token, tags=[\"Authentication\"\
      ], summary=\"User Login\")\n@limiter.limit(\"10/minute\") # Stricter limit for\
      \ login attempts\nasync def login_for_access_token(\n    request: Request,\n\
      \    form_data: OAuth2PasswordRequestForm = Depends(),\n):\n    \"\"\"\n   \
      \ Handles user login using OAuth2 compatible form data (username/password).\n\
      \n    **Note:** Uses insecure hardcoded credentials ('admin'/'admin') for demo\
      \ purposes.\n    Replace with secure password verification (e.g., using passlib)\
      \ in production.\n    \"\"\"\n    log_info(f\"\U0001F511 POST /login attempt\
      \ for user: '{form_data.username}'\", icon_type='AUTH', extra={\"client\": request.client.host})\n\
      \n    # --- !!! REPLACE WITH SECURE PASSWORD VERIFICATION !!! ---\n    # Example\
      \ using passlib (if pwd_context is configured):\n    # user = await User.get_or_none(username=form_data.username)\
      \ # Fetch user from DB\n    # if not user or not verify_password(form_data.password,\
      \ user.hashed_password):\n    #     log_warning(...)\n    #     raise HTTPException(...)\n\
      \    if form_data.username == 'admin' and form_data.password == 'admin': # INSECURE\
      \ DEMO\n        log_pipeline(f\"Credentials valid for '{form_data.username}'.\
      \ Generating tokens...\")\n        access_token = await create_access_token(username=form_data.username)\n\
      \        refresh_token = await create_refresh_token(username=form_data.username)\n\
      \        log_success(f\"Tokens generated for '{form_data.username}'.\", icon_type='AUTH')\n\
      \        return Token(access_token=access_token, refresh_token=refresh_token)\n\
      \    else:\n        log_warning(f\"Login failed for '{form_data.username}':\
      \ Invalid credentials.\", icon_type='AUTH')\n        raise HTTPException(\n\
      \            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"\
      Incorrect username or password\",\n            headers={\"WWW-Authenticate\"\
      : \"Bearer error=\\\"invalid_grant\\\"\"},\n        )\n\n@app.post(\"/refresh\"\
      , response_model=Token, tags=[\"Authentication\"], summary=\"Refresh Access\
      \ Token\")\n@limiter.limit(\"20/minute\")\nasync def refresh_access_token(\n\
      \    request: Request,\n    username: str = Depends(validate_refresh_token)\
      \ # Validates refresh token in header\n):\n    \"\"\"Issues a new access and\
      \ refresh token using a valid refresh token (passed via Bearer auth header).\"\
      \"\"\n    log_info(f\"\U0001F511 POST /refresh request by user '{username}'\"\
      , icon_type='AUTH', extra={\"client\": request.client.host})\n    new_access_token\
      \ = await create_access_token(username=username)\n    # Rotate refresh token\
      \ for better security\n    new_refresh_token = await create_refresh_token(username=username)\n\
      \    log_success(f\"New access/refresh tokens generated for '{username}'.\"\
      , icon_type='AUTH')\n    return Token(access_token=new_access_token, refresh_token=new_refresh_token)\n\
      \n# --- Monitoring Routes ---\n@app.get(\"/stats\", response_model=StatsResponse,\
      \ tags=[\"Monitoring\"], summary=\"Get System Statistics\")\n@limiter.limit(\"\
      30/minute\")\nasync def get_stats(\n    request: Request,\n    current_user:\
      \ str = Depends(get_current_user)\n) -> StatsResponse:\n    \"\"\"Returns current\
      \ application, system, and broker statistics (requires authentication).\"\"\"\
      \n    log_info(f\"\U0001F4CA GET /stats request by user '{current_user}'\",\
      \ icon_type='STATS', extra={\"client\": request.client.host})\n\n    # Update\
      \ broker stats from DB (uses Tortoise)\n    await update_broker_stats()\n\n\
      \    # Collect system stats using psutil (in thread pool for potentially blocking\
      \ calls)\n    system_metrics = {}\n    try:\n        process = psutil.Process(os.getpid())\n\
      \n        async def _get_psutil_data():\n            mem_info = process.memory_info()\n\
      \            proc_cpu = process.cpu_percent(interval=0.1) # Blocking call\n\
      \            sys_cpu = psutil.cpu_percent(interval=0.1) # Blocking call\n  \
      \          virt_mem = psutil.virtual_memory() # Blocking call\n            try:\n\
      \                disk_parts = psutil.disk_partitions(all=False) # Blocking call\n\
      \            except Exception as disk_e:\n                log_warning(f\"Could\
      \ not get disk partitions: {disk_e}\", icon_type='STATS')\n                disk_parts\
      \ = []\n\n            disk_usage_data = {}\n            for part in disk_parts:\n\
      \                try:\n                    # Basic filtering of irrelevant partitions\n\
      \                    if not os.path.exists(part.mountpoint) or not os.path.ismount(part.mountpoint):\
      \ continue\n                    if 'loop' in part.device or 'snap' in part.device\
      \ or part.fstype in ['squashfs', 'tmpfs', 'devtmpfs', 'fuse.gvfsd-fuse', 'overlay']:\
      \ continue\n                    usage = psutil.disk_usage(part.mountpoint) #\
      \ Blocking call\n                    disk_usage_data[part.mountpoint] = {\n\
      \                        \"total_gb\": round(usage.total / (1024**3), 2),\n\
      \                        \"used_gb\": round(usage.used / (1024**3), 2),\n  \
      \                      \"free_gb\": round(usage.free / (1024**3), 2),\n    \
      \                    \"percent\": usage.percent\n                    }\n   \
      \             except Exception as part_e:\n                    log_warning(f\"\
      Could not get disk usage for {getattr(part, 'mountpoint', 'N/A')}: {part_e}\"\
      , icon_type='STATS')\n\n            return {\n                \"cpu_percent\"\
      : sys_cpu,\n                \"memory_total_gb\": round(virt_mem.total / (1024**3),\
      \ 2),\n                \"memory_available_gb\": round(virt_mem.available / (1024**3),\
      \ 2),\n                \"memory_used_gb\": round(virt_mem.used / (1024**3),\
      \ 2),\n                \"memory_percent\": virt_mem.percent,\n             \
      \   \"disk_usage\": disk_usage_data or {\"info\": \"No valid partitions found\
      \ or error reading usage.\"},\n                \"process_memory_mb\": round(mem_info.rss\
      \ / (1024**2), 2),\n                \"process_cpu_percent\": proc_cpu,\n   \
      \             \"load_average\": os.getloadavg() if hasattr(os, 'getloadavg')\
      \ else \"N/A\",\n                \"cpu_count_logical\": psutil.cpu_count(logical=True),\n\
      \                \"cpu_count_physical\": psutil.cpu_count(logical=False),\n\
      \            }\n\n        system_metrics = await asyncio.to_thread(_get_psutil_data)\n\
      \n    except ImportError:\n        log_warning(\"psutil not installed, system\
      \ stats unavailable.\", icon_type='STATS')\n        system_metrics[\"error\"\
      ] = \"psutil package not installed\"\n    except Exception as e:\n        log_warning(f\"\
      Error collecting system stats: {e}\", icon_type='STATS', exc_info=True)\n  \
      \      system_metrics[\"error\"] = f\"psutil data collection failed: {type(e).__name__}\"\
      \n\n    # Construct response data safely using the lock\n    response_data =\
      \ {}\n    async with stats_lock:\n        current_stats_copy = app_stats.copy()\
      \ # Work on a copy\n        current_stats_copy[\"system\"].update(system_metrics)\
      \ # Merge system metrics\n\n        # Calculate uptime\n        start_time_dt\
      \ = current_stats_copy[\"start_time\"]\n        uptime_delta = datetime.now(timezone.utc)\
      \ - start_time_dt\n        uptime_seconds = uptime_delta.total_seconds()\n \
      \       current_stats_copy[\"uptime_seconds\"] = round(uptime_seconds, 2)\n\n\
      \        days, rem = divmod(int(uptime_seconds), 86400)\n        hours, rem\
      \ = divmod(rem, 3600)\n        minutes, seconds = divmod(rem, 60)\n        parts\
      \ = [f\"{days}d\" if days else \"\", f\"{hours}h\" if hours else \"\", f\"{minutes}m\"\
      \ if minutes else \"\", f\"{seconds}s\"]\n        current_stats_copy[\"uptime_human\"\
      ] = \" \".join(p for p in parts if p) or \"0s\"\n\n        # Ensure dates are\
      \ strings, status codes are strings\n        current_stats_copy[\"start_time\"\
      ] = start_time_dt.isoformat()\n        # requests_by_status keys are already\
      \ strings from update_request_stats\n        response_data = current_stats_copy\n\
      \n    log_success(f\"Stats returned for user '{current_user}'.\", icon_type='STATS')\n\
      \    try:\n        # Validate final data against the Pydantic model\n      \
      \  return StatsResponse.model_validate(response_data)\n    except ValidationError\
      \ as e:\n        log_critical(f\"Stats data validation failed: {e.errors()}\"\
      , icon_type='CRITICAL', extra={\"invalid_stats\": response_data})\n        raise\
      \ HTTPException(status_code=500, detail=\"Internal Server Error: Failed to generate\
      \ valid stats data.\")\n\n\n@app.get(\"/logs\", response_model=LogFileResponse,\
      \ tags=[\"Monitoring\"], summary=\"List Log Files\")\n@limiter.limit(\"10/minute\"\
      )\nasync def list_log_files(\n    request: Request,\n    current_user: str =\
      \ Depends(get_current_user)\n):\n    \"\"\"Lists available JSON log files in\
      \ the configured log directory (requires authentication).\"\"\"\n    log_info(f\"\
      \U0001F4C4 GET /logs request by user '{current_user}'\", icon_type='LOGS', extra={\"\
      client\": request.client.host})\n    try:\n        # Use asyncio.to_thread for\
      \ os.listdir\n        log_files_all = await asyncio.to_thread(os.listdir, settings.LOG_DIR)\n\
      \        log_files_json = sorted(\n            [f for f in log_files_all if\
      \ f.endswith('.json') and os.path.isfile(os.path.join(settings.LOG_DIR, f))],\n\
      \            reverse=True # Show newest first\n        )\n        log_success(f\"\
      Found {len(log_files_json)} JSON log files.\", icon_type='LOGS')\n        return\
      \ LogFileResponse(log_files=log_files_json)\n    except FileNotFoundError:\n\
      \         log_error(f\"Log directory '{settings.LOG_DIR}' not found.\", icon_type='LOGS')\n\
      \         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"\
      Log directory configured but not found\")\n    except OSError as e:\n      \
      \  log_error(f\"Error listing log files in '{settings.LOG_DIR}': {e}\", exc_info=True,\
      \ icon_type='LOGS')\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error accessing log directory\")\n\n@app.get(\"/logs/{filename:path}\"\
      , response_model=List[Dict[str, Any]], tags=[\"Monitoring\"], summary=\"Get\
      \ Log File Content\")\n@limiter.limit(\"60/minute\") # Limit log reads\nasync\
      \ def get_log_file(\n    request: Request,\n    filename: str = Path(..., title=\"\
      Log filename\", description=\"Name of the JSON log file to retrieve\"),\n  \
      \  start: Optional[int] = FastQuery(None, ge=1, description=\"Start line number\
      \ (1-based index)\"),\n    end: Optional[int] = FastQuery(None, ge=1, description=\"\
      End line number (inclusive)\"),\n    tail: Optional[int] = FastQuery(None, ge=1,\
      \ le=10000, description=\"Return last N lines (max 10000)\"), # Limit tail size\n\
      \    current_user: str = Depends(get_current_user)\n) -> List[Dict]:\n    \"\
      \"\"\n    Retrieves the content of a specific JSON log file, allowing slicing\
      \ or tailing (requires authentication).\n    Each line is parsed as JSON. Invalid\
      \ lines are returned with an error indicator.\n    \"\"\"\n    safe_filename\
      \ = secure_filename(filename)\n    if not safe_filename or safe_filename !=\
      \ filename or not safe_filename.endswith('.json'):\n        log_warning(f\"\
      Invalid log file access attempt: '{filename}' by user '{current_user}'\", icon_type='SEC')\n\
      \        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"\
      Invalid or non-JSON log filename provided\")\n\n    log_path = os.path.join(settings.LOG_DIR,\
      \ safe_filename)\n    log_info(f\"\U0001F4C4 GET /logs/{safe_filename} request\
      \ by '{current_user}' (start={start}, end={end}, tail={tail})\", icon_type='LOGS')\n\
      \n    # Define synchronous file reading logic\n    def read_and_parse_log_sync():\n\
      \        if not os.path.isfile(log_path):\n            return None # Indicate\
      \ file not found\n\n        lines_to_parse = []\n        try:\n            if\
      \ tail is not None and tail > 0:\n                with open(log_path, 'r', encoding='utf-8')\
      \ as f:\n                    # Use deque for efficient tailing\n           \
      \         lines_to_parse = deque(f, maxlen=tail)\n            else:\n      \
      \          with open(log_path, 'r', encoding='utf-8') as f:\n              \
      \      all_lines_iter = enumerate(f, 1) # 1-based line numbers\n           \
      \         line_count_in_range = 0\n                    for line_num, line in\
      \ all_lines_iter:\n                        if start is not None and line_num\
      \ < start: continue\n                        if end is not None and line_num\
      \ > end: break\n                        lines_to_parse.append(line.strip())\n\
      \                        line_count_in_range += 1\n                        #\
      \ Prevent excessive memory usage for open-ended ranges\n                   \
      \     if start is not None and end is None and line_count_in_range >= 10000:\n\
      \                            log_warning(f\"Log file read for '{safe_filename}'\
      \ truncated at 10000 lines (start={start}, no end).\", icon_type='LOGS')\n \
      \                           lines_to_parse.append(json.dumps({\"_warning\":\
      \ \"Result set truncated at 10000 lines\", \"_limit\": 10000}))\n          \
      \                  break\n        except Exception as read_exc:\n          \
      \  log_error(f\"Error reading log file '{safe_filename}': {read_exc}\", exc_info=True,\
      \ icon_type='LOGS')\n            # Return an error entry instead of raising\
      \ here to indicate read failure\n            return [{\"_error\": f\"Failed\
      \ to read file: {read_exc}\"}]\n\n        # Parse JSON lines\n        parsed_lines\
      \ = []\n        for i, line in enumerate(lines_to_parse):\n            line_num_info\
      \ = f\"tail_{i+1}\" if tail else (start or 1) + i\n            try:\n      \
      \          if line: # Avoid parsing empty lines\n                    parsed_lines.append(json.loads(line))\n\
      \                # Optionally add indicator for empty lines if not tailing:\n\
      \                # elif tail is None: parsed_lines.append({\"_info\": \"Empty\
      \ line\", \"_line\": line_num_info})\n            except json.JSONDecodeError:\n\
      \                parsed_lines.append({\"_error\": \"Invalid JSON\", \"_line\"\
      : line_num_info, \"_raw\": line[:250]}) # Include raw snippet\n            except\
      \ Exception as parse_exc:\n                 parsed_lines.append({\"_error\"\
      : f\"Parsing error: {parse_exc}\", \"_line\": line_num_info, \"_raw\": line[:250]})\n\
      \n        return parsed_lines\n\n    # Run the synchronous function in a thread\
      \ pool\n    try:\n        result_lines = await asyncio.to_thread(read_and_parse_log_sync)\n\
      \n        if result_lines is None:\n            log_warning(f\"Log file not\
      \ found: {log_path}\", icon_type='LOGS')\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,\
      \ detail=f\"Log file '{safe_filename}' not found\")\n\n        log_success(f\"\
      {len(result_lines)} log entries returned from '{safe_filename}'.\", icon_type='LOGS')\n\
      \        return result_lines\n\n    except HTTPException:\n        raise # Re-raise\
      \ existing HTTP exceptions\n    except Exception as e:\n        log_error(f\"\
      Unexpected error processing log file '{safe_filename}': {e}\", exc_info=True,\
      \ icon_type='LOGS')\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Unexpected error processing log file\")\n\n# --- Queue Routes ---\n\
      @app.get(\"/queues\", response_model=List[QueueResponse], tags=[\"Queues\"],\
      \ summary=\"List All Queues\")\n@limiter.limit(\"60/minute\")\nasync def list_queues(\n\
      \    request: Request,\n    current_user: str = Depends(get_current_user),\n\
      ) -> List[QueueResponse]:\n    \"\"\"Lists all available message queues, including\
      \ the count of messages in each (requires authentication).\"\"\"\n    log_info(f\"\
      \U0001F4CB GET /queues request by user '{current_user}'\", icon_type='QUEUE')\n\
      \    try:\n        queues = await Queue.all().order_by('name')\n        if not\
      \ queues:\n            return [] # Return empty list if no queues exist\n\n\
      \        # Fetch message counts concurrently\n        count_tasks = {q.id: Message.filter(queue_id=q.id).count()\
      \ for q in queues}\n        message_counts = await asyncio.gather(*count_tasks.values())\n\
      \        # Map counts back to queues using the keys from count_tasks\n     \
      \   counts_dict = dict(zip(count_tasks.keys(), message_counts))\n\n        response_list\
      \ = [\n            QueueResponse(\n                id=q.id,\n              \
      \  name=q.name,\n                created_at=q.created_at,\n                updated_at=q.updated_at,\n\
      \                message_count=counts_dict.get(q.id, 0) # Get count from gathered\
      \ results\n            ) for q in queues\n        ]\n        log_success(f\"\
      Returned {len(response_list)} queues.\", icon_type='QUEUE')\n        return\
      \ response_list\n    except Exception as e:\n        log_error(f\"Error listing\
      \ queues: {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error retrieving queue list\")\n\n@app.post(\"/queues\", response_model=QueueResponse,\
      \ status_code=status.HTTP_201_CREATED, tags=[\"Queues\"], summary=\"Create New\
      \ Queue\")\n@limiter.limit(\"30/minute\")\nasync def create_queue(\n    request:\
      \ Request,\n    payload: QueueCreatePayload, # Use dedicated payload model\n\
      \    current_user: str = Depends(get_current_user),\n) -> QueueResponse:\n \
      \   \"\"\"Creates a new message queue. Returns 409 Conflict if the queue name\
      \ already exists (requires authentication).\"\"\"\n    queue_name = payload.name\n\
      \    log_info(f\"➕ POST /queues request by '{current_user}' to create '{queue_name}'\"\
      , icon_type='QUEUE')\n    try:\n        # Use get_or_create for atomic check-and-create\n\
      \        new_queue, created = await Queue.get_or_create(name=queue_name)\n \
      \       if not created:\n            log_warning(f\"Queue '{queue_name}' already\
      \ exists. Creation request denied (409).\", icon_type='QUEUE')\n           \
      \ raise HTTPException(\n                status_code=status.HTTP_409_CONFLICT,\n\
      \                detail=f\"Queue with name '{queue_name}' already exists.\"\n\
      \            )\n        log_success(f\"Queue '{queue_name}' created successfully\
      \ (ID: {new_queue.id}).\", icon_type='QUEUE')\n        # Convert ORM model to\
      \ Pydantic response model (message_count defaults to 0)\n        return QueueResponse.model_validate(new_queue)\n\
      \    except IntegrityError: # Should be caught by get_or_create, but as a fallback\n\
      \         log_warning(f\"IntegrityError during queue creation for '{queue_name}'.\
      \ Likely already exists.\", icon_type='DB')\n         raise HTTPException(\n\
      \             status_code=status.HTTP_409_CONFLICT,\n             detail=f\"\
      Queue with name '{queue_name}' already exists (database constraint).\"\n   \
      \      )\n    except Exception as e:\n        log_error(f\"Error creating queue\
      \ '{queue_name}': {e}\", icon_type='CRITICAL', exc_info=True)\n        raise\
      \ HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=\"\
      Error creating queue\")\n\n@app.get(\"/queues/{queue_name}\", response_model=QueueResponse,\
      \ tags=[\"Queues\"], summary=\"Get Queue Details\")\n@limiter.limit(\"60/minute\"\
      )\nasync def get_queue(\n    request: Request,\n    queue_name: str = Path(...,\
      \ description=\"Name of the queue\"),\n    current_user: str = Depends(get_current_user),\n\
      ) -> QueueResponse:\n    \"\"\"Gets details for a specific queue by name, including\
      \ its message count (requires authentication).\"\"\"\n    log_info(f\"\U0001F4E5\
      \ GET /queues/{queue_name} request by user '{current_user}'\", icon_type='QUEUE')\n\
      \    try:\n        queue = await _get_queue_or_404(queue_name) # Handles 404\n\
      \        message_count = await Message.filter(queue_id=queue.id).count()\n \
      \       log_success(f\"Details for queue '{queue_name}' returned.\", icon_type='QUEUE')\n\
      \        response = QueueResponse.model_validate(queue)\n        response.message_count\
      \ = message_count\n        return response\n    except HTTPException:\n    \
      \    raise # Let 404 from _get_queue_or_404 pass through\n    except Exception\
      \ as e:\n        log_error(f\"Error getting queue details for '{queue_name}':\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error retrieving queue details\")\n\n@app.delete(\"/queues/{queue_name}\"\
      , status_code=status.HTTP_204_NO_CONTENT, tags=[\"Queues\"], summary=\"Delete\
      \ Queue\")\n@limiter.limit(\"10/minute\") # Lower limit for destructive actions\n\
      async def delete_queue(\n    request: Request,\n    queue_name: str = Path(...,\
      \ description=\"Name of the queue to delete\"),\n    current_user: str = Depends(get_current_user),\n\
      ) -> Response:\n    \"\"\"\n    Deletes a queue and all its associated messages\
      \ (due to cascade delete).\n    Returns 204 No Content on success (requires\
      \ authentication).\n    \"\"\"\n    log_info(f\"\U0001F5D1️ DELETE /queues/{queue_name}\
      \ request by user '{current_user}'\", icon_type='QUEUE')\n    try:\n       \
      \ queue = await _get_queue_or_404(queue_name) # Handles 404\n        queue_id\
      \ = queue.id # Get ID for logging before deletion\n        log_pipeline(f\"\
      Queue '{queue_name}' (ID: {queue_id}) found. Proceeding with deletion...\")\n\
      \n        # Tortoise ORM handles cascade deletion based on the ForeignKey(on_delete=CASCADE)\n\
      \        await queue.delete()\n\n        log_success(f\"Queue '{queue_name}'\
      \ (ID: {queue_id}) and associated messages deleted successfully.\", icon_type='QUEUE')\n\
      \        # Return an empty response with 204 status code\n        return Response(status_code=status.HTTP_204_NO_CONTENT)\n\
      \    except HTTPException:\n        raise # Let 404 pass through\n    except\
      \ Exception as e:\n        log_error(f\"Error deleting queue '{queue_name}':\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error deleting queue\")\n\n\n# --- Message Routes ---\n@app.post(\"\
      /queues/{queue_name}/messages\", response_model=MessagePublishResponse, status_code=status.HTTP_201_CREATED,\
      \ tags=[\"Messages\"], summary=\"Publish Message\")\n@limiter.limit(\"100/second\"\
      ) # Allow higher rate for publishing\nasync def publish_message(\n    # Non-default\
      \ arguments first\n    request: Request,\n    payload: MessagePayload,     \
      \      # Request body\n    background_tasks: BackgroundTasks, # FastAPI dependency\
      \ injection\n    # Default arguments (Path, Depends) last\n    queue_name: str\
      \ = Path(..., description=\"Name of the target queue\"),\n    current_user:\
      \ str = Depends(get_current_user), # Authentication\n) -> MessagePublishResponse:\n\
      \    \"\"\"Publishes a new message with the given content to the specified queue\
      \ (requires authentication).\"\"\"\n    log_info(f\"\U0001F4E4 POST /queues/{queue_name}/messages\
      \ by '{current_user}'\", icon_type='MSG', extra={\"content_preview\": payload.content[:50]\
      \ + \"...\"})\n    try:\n        queue = await _get_queue_or_404(queue_name)\
      \ # Handles 404\n        log_pipeline(f\"Queue '{queue_name}' found. Creating\
      \ message...\")\n        new_message = await Message.create(\n            queue=queue,\
      \ # Pass the ORM object\n            content=payload.content,\n            status='pending'\
      \ # Initial status\n        )\n        log_success(f\"Message ID {new_message.id}\
      \ published to queue '{queue_name}'.\", icon_type='MSG')\n\n        # Notify\
      \ SSE listeners in the background\n        background_tasks.add_task(notify_sse,\
      \ queue_name, new_message.id, \"new_message\")\n\n        return MessagePublishResponse(message_id=new_message.id)\n\
      \    except HTTPException:\n        raise # Let 404 pass through\n    except\
      \ Exception as e:\n        log_error(f\"Error publishing message to queue '{queue_name}':\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error publishing message\")\n\n@app.get(\"/queues/{queue_name}/messages/consume\"\
      , response_model=Optional[MessageConsumeResponse], tags=[\"Messages\"], summary=\"\
      Consume Message\")\n@limiter.limit(\"60/second\") # Rate limit consumption attempts\n\
      async def consume_message(\n    request: Request,\n    queue_name: str = Path(...,\
      \ description=\"Name of the queue to consume from\"),\n    current_user: str\
      \ = Depends(get_current_user),\n) -> Optional[MessageConsumeResponse]:\n   \
      \ \"\"\"\n    Atomically consumes the oldest 'pending' message from the specified\
      \ queue.\n    Marks the message status as 'processing' and returns it.\n   \
      \ Returns null (HTTP 200) if the queue is empty or has no 'pending' messages.\n\
      \    Requires authentication.\n    \"\"\"\n    log_info(f\"\U0001F4E9 GET /queues/{queue_name}/messages/consume\
      \ request by '{current_user}'\", icon_type='MSG', extra={\"client\": request.client.host})\n\
      \    try:\n        queue = await _get_queue_or_404(queue_name) # Handles 404\n\
      \n        # Use a transaction for atomic read-then-update\n        conn = Tortoise.get_connection(\"\
      default\")\n        async with conn.in_transaction() as tx:\n            # Find\
      \ the oldest pending message and lock the row (best effort in SQLite)\n    \
      \        message = await Message.filter(\n                queue_id=queue.id,\n\
      \                status='pending'\n            ).using_connection(tx).order_by('created_at').select_for_update().first()\n\
      \n            if not message:\n                log_info(f\"No pending messages\
      \ found in queue '{queue_name}' for consumption.\", icon_type='MSG')\n     \
      \           # Return None, FastAPI converts Optional[Model] to null in JSON\
      \ response with HTTP 200\n                return None\n\n            # Mark\
      \ as processing and update timestamp\n            message.status = 'processing'\n\
      \            message.updated_at = datetime.now(timezone.utc)\n            #\
      \ Save only the changed fields\n            await message.save(using_connection=tx,\
      \ update_fields=['status', 'updated_at'])\n\n            log_success(f\"Message\
      \ ID {message.id} consumed from queue '{queue_name}' by '{current_user}' (status\
      \ -> processing).\", icon_type='MSG')\n            # Return the consumed message\
      \ details\n            return MessageConsumeResponse(\n                message_id=message.id,\n\
      \                queue=queue_name,\n                content=message.content,\n\
      \                status=message.status, # Should be 'processing'\n         \
      \       retrieved_at=message.updated_at\n            )\n\n    except HTTPException:\n\
      \        raise # Let 404 pass through\n    except IntegrityError as e: # e.g.,\
      \ if locking causes issues or unexpected constraint violation\n        log_warning(f\"\
      DB integrity error during consumption from '{queue_name}': {e}\", icon_type='DB')\n\
      \        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"\
      Conflict during message consumption attempt, try again.\")\n    except Exception\
      \ as e:\n        log_error(f\"Error consuming message from queue '{queue_name}':\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error consuming message\")\n\n@app.post(\"/messages/{message_id}/ack\"\
      , status_code=status.HTTP_200_OK, response_model=Dict[str, str], tags=[\"Messages\"\
      ], summary=\"Acknowledge Message\")\n@limiter.limit(\"100/second\")\nasync def\
      \ acknowledge_message(\n    # Non-default args first\n    request: Request,\n\
      \    background_tasks: BackgroundTasks,\n    # Default args last\n    message_id:\
      \ int = Path(..., ge=1, description=\"ID of the message to acknowledge\"),\n\
      \    current_user: str = Depends(get_current_user)\n) -> Dict[str, str]:\n \
      \   \"\"\"\n    Marks a message currently in the 'processing' state as 'processed'.\n\
      \    Requires authentication. Returns 404 if message not found, or 409 if message\
      \ is not 'processing'.\n    \"\"\"\n    log_info(f\"✅ POST /messages/{message_id}/ack\
      \ request by '{current_user}'\", icon_type='MSG')\n    try:\n        conn =\
      \ Tortoise.get_connection(\"default\")\n        async with conn.in_transaction()\
      \ as tx:\n            # Find the message, ensuring it's 'processing' and lock\
      \ it\n            message = await Message.filter(\n                id=message_id,\n\
      \                status='processing' # MUST be 'processing' to be ACK'd\n  \
      \          ).using_connection(tx).select_for_update().get_or_none()\n\n    \
      \        if not message:\n                # Check if message exists but has\
      \ wrong status\n                existing_msg = await Message.filter(id=message_id).using_connection(tx).first()\n\
      \                if existing_msg:\n                    log_warning(f\"ACK failed\
      \ for message {message_id}: Found but status is '{existing_msg.status}', not\
      \ 'processing'.\", icon_type='MSG')\n                    raise HTTPException(\n\
      \                        status_code=status.HTTP_409_CONFLICT,\n           \
      \             detail=f\"Message {message_id} is in '{existing_msg.status}' state,\
      \ cannot ACK.\"\n                    )\n                else:\n            \
      \        log_warning(f\"ACK failed for message {message_id}: Not found.\", icon_type='MSG')\n\
      \                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n\
      \                        detail=f\"Message {message_id} not found.\"\n     \
      \               )\n\n            # Mark as processed\n            original_queue\
      \ = await message.queue.first().using_connection(tx) # Get related queue within\
      \ transaction\n            if not original_queue:\n                 # This shouldn't\
      \ happen due to FK constraints, but defensively check\n                 log_error(f\"\
      Critical: Message {message_id} has no associated queue during ACK.\", icon_type='DB')\n\
      \                 raise HTTPException(status_code=500, detail=\"Internal error:\
      \ Message queue association lost.\")\n            original_queue_name = original_queue.name\n\
      \n            message.status = 'processed'\n            message.updated_at =\
      \ datetime.now(timezone.utc)\n            await message.save(using_connection=tx,\
      \ update_fields=['status', 'updated_at'])\n\n            # Optional: Delete\
      \ the message after successful processing?\n            # await message.delete(using_connection=tx)\n\
      \            # log_pipeline(f\"Message ID {message_id} deleted after ACK.\"\
      , icon_type='MSG')\n\n            log_success(f\"Message ID {message_id} acknowledged\
      \ by '{current_user}' (status -> processed).\", icon_type='MSG')\n         \
      \   # Notify SSE listeners about the successful processing\n            background_tasks.add_task(notify_sse,\
      \ original_queue_name, message.id, \"message_processed\")\n            return\
      \ {\"detail\": f\"Message {message_id} acknowledged successfully.\"}\n\n   \
      \ except HTTPException:\n        raise # Let 404/409 pass through\n    except\
      \ Exception as e:\n        log_error(f\"Error acknowledging message {message_id}:\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error acknowledging message\")\n\n@app.post(\"/messages/{message_id}/nack\"\
      , status_code=status.HTTP_200_OK, response_model=Dict[str, str], tags=[\"Messages\"\
      ], summary=\"Negative Acknowledge Message\")\n@limiter.limit(\"100/second\"\
      )\nasync def negative_acknowledge_message(\n    # Non-default args first\n \
      \   request: Request,\n    background_tasks: BackgroundTasks,\n    # Default\
      \ args last\n    message_id: int = Path(..., ge=1, description=\"ID of the message\
      \ to NACK\"),\n    requeue: bool = FastQuery(False, description=\"If true, reset\
      \ status to 'pending'. If false, set status to 'failed'.\"),\n    current_user:\
      \ str = Depends(get_current_user)\n) -> Dict[str, str]:\n    \"\"\"\n    Marks\
      \ a 'processing' message as 'failed' or requeues it ('pending').\n    Requires\
      \ authentication. Returns 404 if message not found, or 409 if message is not\
      \ 'processing'.\n    \"\"\"\n    action = \"requeued\" if requeue else \"marked\
      \ as failed\"\n    log_info(f\"❌ POST /messages/{message_id}/nack request by\
      \ '{current_user}' (requeue={requeue})\", icon_type='MSG')\n    try:\n     \
      \   conn = Tortoise.get_connection(\"default\")\n        async with conn.in_transaction()\
      \ as tx:\n            message = await Message.filter(\n                id=message_id,\n\
      \                status='processing' # MUST be 'processing' to be NACK'd\n \
      \           ).using_connection(tx).select_for_update().get_or_none()\n\n   \
      \         if not message:\n                existing_msg = await Message.filter(id=message_id).using_connection(tx).first()\n\
      \                if existing_msg:\n                    log_warning(f\"NACK failed\
      \ for message {message_id}: Found but status is '{existing_msg.status}', not\
      \ 'processing'.\", icon_type='MSG')\n                    raise HTTPException(status_code=status.HTTP_409_CONFLICT,\
      \ detail=f\"Message {message_id} is in '{existing_msg.status}' state, cannot\
      \ NACK.\")\n                else:\n                    log_warning(f\"NACK failed\
      \ for message {message_id}: Not found.\", icon_type='MSG')\n               \
      \     raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f\"\
      Message {message_id} not found.\")\n\n            original_queue = await message.queue.first().using_connection(tx)\n\
      \            if not original_queue:\n                 log_error(f\"Critical:\
      \ Message {message_id} has no associated queue during NACK.\", icon_type='DB')\n\
      \                 raise HTTPException(status_code=500, detail=\"Internal error:\
      \ Message queue association lost.\")\n            original_queue_name = original_queue.name\n\
      \n            new_status = 'pending' if requeue else 'failed'\n            message.status\
      \ = new_status\n            message.updated_at = datetime.now(timezone.utc)\n\
      \            await message.save(using_connection=tx, update_fields=['status',\
      \ 'updated_at'])\n\n            log_success(f\"Message ID {message_id} NACK'd\
      \ by '{current_user}' (status -> {new_status}).\", icon_type='MSG')\n      \
      \      event_type = \"message_requeued\" if requeue else \"message_failed\"\n\
      \            background_tasks.add_task(notify_sse, original_queue_name, message.id,\
      \ event_type)\n\n            return {\"detail\": f\"Message {message_id} successfully\
      \ {action}.\"}\n\n    except HTTPException:\n        raise # Let 404/409 pass\
      \ through\n    except Exception as e:\n        log_error(f\"Error NACK'ing message\
      \ {message_id}: {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=f\"Error negatively acknowledging message (action: {action})\")\n\n\
      \n# --- Server-Sent Events (SSE) Stream ---\n@app.get(\"/stream/{queue_name}\"\
      , tags=[\"Realtime (SSE)\"], summary=\"Subscribe to Queue Events\")\nasync def\
      \ sse_stream_queue(\n    request: Request,\n    queue_name: str = Path(...,\
      \ description=\"Queue name to subscribe to for events\"),\n    current_user:\
      \ str = Depends(get_current_user) # Protect stream\n):\n    \"\"\"Provides a\
      \ Server-Sent Event stream for real-time updates on a specific queue (requires\
      \ authentication).\"\"\"\n    if not redis_sse:\n         log_error(\"SSE stream\
      \ unavailable: Redis client not configured or connection failed.\", icon_type='CRITICAL')\n\
      \         raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\
      \ detail=\"SSE service is currently unavailable due to Redis connection issues.\"\
      )\n\n    log_info(f\"\U0001F4E1 SSE stream requested for queue '{queue_name}'\
      \ by user '{current_user}'\", icon_type='SSE', extra={\"client\": request.client.host})\n\
      \n    # Validate queue exists before starting the potentially long-running generator\n\
      \    try:\n        await _get_queue_or_404(queue_name)\n    except HTTPException\
      \ as e:\n        # Log the attempt and re-raise the exception (e.g., 404)\n\
      \        log_warning(f\"Attempt to stream non-existent queue '{queue_name}'\
      \ by '{current_user}'. Denying.\", icon_type='SSE')\n        raise e # Re-raise\
      \ the HTTP 404 exception\n\n    async def event_generator() -> AsyncGenerator[str,\
      \ None]:\n        pubsub = None\n        redis_channel = f\"sse:{queue_name}\"\
      \n        is_subscribed = False\n        keep_alive_interval = 15 # Send keep-alive\
      \ every 15 seconds\n        disconnect_check_interval = 2 # Check for client\
      \ disconnect every 2 seconds\n\n        try:\n            pubsub = redis_sse.pubsub(ignore_subscribe_messages=True)\n\
      \            await pubsub.subscribe(redis_channel)\n            is_subscribed\
      \ = True\n            log_success(f\"\U0001F4E1 Subscribed to Redis channel\
      \ '{redis_channel}' for SSE stream (user: {current_user}).\", icon_type='SSE')\n\
      \n            # Send initial connection confirmation message\n            connect_data\
      \ = json.dumps({'event': 'connected', 'queue': queue_name, 'channel': redis_channel,\
      \ 'timestamp': datetime.now(timezone.utc).isoformat()})\n            yield f\"\
      event: system\\ndata: {connect_data}\\n\\n\"\n\n            last_keep_alive\
      \ = time.monotonic()\n\n            while True:\n                # Check for\
      \ client disconnect periodically\n                if await request.is_disconnected():\n\
      \                     log_info(f\"\U0001F4E1 SSE client for '{queue_name}' disconnected\
      \ (user: {current_user}). Closing stream.\", icon_type='SSE')\n            \
      \         break\n\n                try:\n                    # Wait for a message\
      \ with a timeout slightly longer than disconnect check\n                   \
      \ message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=disconnect_check_interval)\n\
      \                    if message and message.get(\"type\") == \"message\":\n\
      \                        data = message['data']\n                        # log_debug(f\"\
      \U0001F4E1 SSE Data Received from Redis '{redis_channel}': {data[:150]}...\"\
      , icon_type='SSE')\n                        # Assume data from notify_sse is\
      \ already valid JSON\n                        yield f\"event: message\\ndata:\
      \ {data}\\n\\n\"\n                        last_keep_alive = time.monotonic()\
      \ # Reset keep-alive timer on message\n\n                    # Send keep-alive\
      \ periodically if no messages received\n                    elif time.monotonic()\
      \ - last_keep_alive > keep_alive_interval:\n                        yield \"\
      : keep-alive\\n\\n\"\n                        last_keep_alive = time.monotonic()\n\
      \n                except asyncio.TimeoutError:\n                    # Timeout\
      \ just means no message received, continue loop\n                    # Send\
      \ keep-alive if needed based on timer check above\n                    if time.monotonic()\
      \ - last_keep_alive > keep_alive_interval:\n                        yield \"\
      : keep-alive\\n\\n\"\n                        last_keep_alive = time.monotonic()\n\
      \                    continue # Go back to checking disconnect/waiting for message\n\
      \                except aioredis.RedisError as redis_err:\n                \
      \    log_error(f\"Redis error reading pub/sub for '{redis_channel}': {redis_err}\"\
      , icon_type='SSE', exc_info=True)\n                    error_data = json.dumps({'error':\
      \ 'Redis connection error', 'channel': redis_channel, 'timestamp': datetime.now(timezone.utc).isoformat()})\n\
      \                    yield f\"event: error\\ndata: {error_data}\\n\\n\"\n  \
      \                  await asyncio.sleep(5) # Backoff before breaking\n      \
      \              break # Exit on persistent Redis errors\n                except\
      \ Exception as e:\n                    # Catch unexpected errors within the\
      \ loop\n                    log_error(f\"Unexpected error in SSE generator for\
      \ '{redis_channel}': {e}\", icon_type='CRITICAL', exc_info=True)\n         \
      \           try:\n                         error_data = json.dumps({'error':\
      \ 'Internal server error in SSE stream', 'channel': redis_channel, 'timestamp':\
      \ datetime.now(timezone.utc).isoformat()})\n                         yield f\"\
      event: error\\ndata: {error_data}\\n\\n\"\n                    except Exception:\
      \ pass # Ignore if can't send error message\n                    break # Exit\
      \ loop on unexpected error\n        except asyncio.CancelledError:\n       \
      \      log_info(f\"\U0001F4E1 SSE stream for '{queue_name}' task cancelled (user:\
      \ {current_user}).\", icon_type='SSE')\n        except Exception as e:\n   \
      \          # Catch errors during initial setup (subscribe etc.)\n          \
      \   log_error(f\"Error setting up SSE stream generator for '{queue_name}': {e}\"\
      , icon_type='CRITICAL', exc_info=True)\n             # Can't yield here if setup\
      \ failed, error handled by FastAPI's exception handlers\n        finally:\n\
      \            # Cleanup Redis PubSub resources\n            if pubsub:\n    \
      \            try:\n                    if is_subscribed:\n                 \
      \       log_pipeline(f\"\U0001F4E1 Unsubscribing from Redis channel '{redis_channel}'...\"\
      , icon_type='SSE')\n                        await pubsub.unsubscribe(redis_channel)\n\
      \                    await pubsub.close()\n                    log_pipeline(f\"\
      \U0001F4E1 Closed Redis PubSub client for '{redis_channel}'.\", icon_type='SSE')\n\
      \                except Exception as close_e:\n                    log_warning(f\"\
      Error closing Redis PubSub resources for '{redis_channel}': {close_e}\", icon_type='SSE')\n\
      \n    # Return the streaming response\n    headers = {'Cache-Control': 'no-cache',\
      \ 'Connection': 'keep-alive', 'X-Accel-Buffering': 'no'} # Nginx buffering off\n\
      \    return StreamingResponse(event_generator(), media_type=\"text/event-stream\"\
      , headers=headers)\n\n\n# --- GraphQL Setup (Strawberry) ---\nlog_info(\"\U0001F353\
      \ Configuring GraphQL endpoint with Strawberry...\", icon_type='GRAPHQL')\n\n\
      # --- Strawberry Type Definitions ---\n@strawberry.type(description=\"Represents\
      \ a message queue\")\nclass QueueGQL:\n    # ... (id, name, created_at, updated_at,\
      \ message_count field definitions) ...\n\n    @strawberry.field(description=\"\
      Retrieves messages belonging to this queue, filterable by status\")\n    async\
      \ def messages(\n        self, info: Info,\n        # --- CORRECTED ARGUMENTS\
      \ ---\n        status: Optional[str] = strawberry.field(default=None, description=\"\
      Filter messages by status (pending, processing, processed, failed)\"),\n   \
      \     limit: int = strawberry.field(default=10, description=\"Maximum number\
      \ of messages to return (1-100)\"),\n        offset: int = strawberry.field(default=0,\
      \ description=\"Number of messages to skip (for pagination)\")\n        # ---\
      \ END CORRECTIONS ---\n    ) -> List[\"MessageGQL\"]:\n        \"\"\"Resolver\
      \ to fetch messages related to this queue with filtering and pagination.\"\"\
      \"\n        log_debug(f\"GQL: Fetching messages for Queue ID {self.id} (status={status},\
      \ limit={limit}, offset={offset})\")\n        valid_statuses = ['pending', 'processing',\
      \ 'processed', 'failed']\n        if status and status not in valid_statuses:\n\
      \             # Use Strawberry's error handling\n             raise ValueError(f\"\
      Invalid status filter: '{status}'. Must be one of {valid_statuses}.\")\n\n \
      \       # Sanitize limit and offset\n        limit = max(1, min(limit, 100))\
      \ # Enforce reasonable limit\n        offset = max(0, offset)\n\n        try:\n\
      \             # Convert strawberry.ID back to int for DB query\n           \
      \  queue_id_int = int(self.id)\n             query = Message.filter(queue_id=queue_id_int)\n\
      \             if status:\n                 query = query.filter(status=status)\n\
      \n             # Apply ordering, limit, and offset\n             messages_db\
      \ = await query.order_by('-created_at').offset(offset).limit(limit) # Get latest\
      \ first\n\n             # Map ORM models to GQL types (queue name is available\
      \ from self.name)\n             return [MessageGQL.from_orm(m, queue_name_str=self.name)\
      \ for m in messages_db]\n        except ValueError as ve: # Catch ID parsing\
      \ error or status validation error\n            log_warning(f\"GQL messages\
      \ resolver validation error for queue {self.id}: {ve}\")\n            raise\
      \ ve # Let Strawberry handle the GraphQL error response\n        except Exception\
      \ as e:\n            log_error(f\"GQL messages resolver error for queue {self.id}:\
      \ {e}\", exc_info=True)\n            # Returning empty list on error is safer\
      \ than raising a generic 500 in GQL\n            return []\n\n@strawberry.type(description=\"\
      Represents a message within a queue\")\nclass MessageGQL:\n    id: strawberry.ID\
      \ = strawberry.field(description=\"Unique identifier for the message\")\n  \
      \  queue_name: str = strawberry.field(description=\"Name of the queue this message\
      \ belongs to\")\n    content: str = strawberry.field(description=\"Payload content\
      \ of the message\")\n    status: str = strawberry.field(description=\"Current\
      \ status (pending, processing, processed, failed)\")\n    created_at: datetime\
      \ = strawberry.field(description=\"Timestamp when the message was created (UTC)\"\
      )\n    updated_at: datetime = strawberry.field(description=\"Timestamp when\
      \ the message was last updated (UTC)\")\n\n    @classmethod\n    def from_orm(cls,\
      \ model: Message, queue_name_str: str) -> \"MessageGQL\":\n         \"\"\"Helper\
      \ to map from Tortoise ORM model, injecting queue name.\"\"\"\n         return\
      \ cls(\n             id=strawberry.ID(str(model.id)),\n             queue_name=queue_name_str,\
      \ # Inject name passed from QueueGQL resolver\n             content=model.content,\n\
      \             status=model.status,\n             created_at=model.created_at,\n\
      \             updated_at=model.updated_at,\n         )\n\n# --- Strawberry Query\
      \ Root ---\n@strawberry.type\nclass QueryGQL:\n    @strawberry.field(description=\"\
      Retrieves a list of all available message queues\")\n    async def all_queues(self,\
      \ info: Info) -> List[QueueGQL]:\n        log_info(\"\U0001F353 GraphQL Query:\
      \ all_queues\", icon_type='GRAPHQL')\n        try:\n             queues_db =\
      \ await Queue.all().order_by('name')\n             # Map ORM models to GQL types\n\
      \             # message_count and messages are resolved by QueueGQL field resolvers\n\
      \             return [\n                 QueueGQL(\n                     id=strawberry.ID(str(q.id)),\n\
      \                     name=q.name,\n                     created_at=q.created_at,\n\
      \                     updated_at=q.updated_at\n                 ) for q in queues_db\n\
      \             ]\n        except Exception as e:\n             log_error(f\"\
      GraphQL all_queues error: {e}\", icon_type='GRAPHQL', exc_info=True)\n     \
      \        # Return empty list on error\n             return []\n\n    @strawberry.field(description=\"\
      Retrieves a specific message queue by its unique name\")\n    async def queue_by_name(self,\
      \ info: Info, name: str = strawberry.argument(description=\"The name of the\
      \ queue to retrieve\")) -> Optional[QueueGQL]:\n        log_info(f\"\U0001F353\
      \ GraphQL Query: queue_by_name (name='{name}')\", icon_type='GRAPHQL')\n   \
      \     try:\n            queue_db = await Queue.get_or_none(name=name)\n    \
      \        if queue_db:\n                 # Map ORM model to GQL type\n      \
      \           return QueueGQL(\n                     id=strawberry.ID(str(queue_db.id)),\n\
      \                     name=queue_db.name,\n                     created_at=queue_db.created_at,\n\
      \                     updated_at=queue_db.updated_at\n                 )\n \
      \           else:\n                 log_warning(f\"GraphQL: Queue '{name}' not\
      \ found.\", icon_type='GRAPHQL')\n                 return None # Return null\
      \ if not found\n        except Exception as e:\n             log_error(f\"GraphQL\
      \ queue_by_name error for '{name}': {e}\", icon_type='GRAPHQL', exc_info=True)\n\
      \             return None # Return null on error\n\n    @strawberry.field(description=\"\
      Retrieves a specific message by its unique ID\")\n    async def message_by_id(self,\
      \ info: Info, id: strawberry.ID = strawberry.argument(description=\"The unique\
      \ ID of the message\")) -> Optional[MessageGQL]:\n        log_info(f\"\U0001F353\
      \ GraphQL Query: message_by_id (id={id})\", icon_type='GRAPHQL')\n        try:\n\
      \            message_db = await Message.get_or_none(id=int(id)).select_related('queue')\
      \ # Fetch related queue for name\n            if message_db and message_db.queue:\n\
      \                 # Map ORM model to GQL type\n                 return MessageGQL.from_orm(message_db,\
      \ queue_name_str=message_db.queue.name)\n            else:\n               \
      \  log_warning(f\"GraphQL: Message ID {id} not found or has no queue.\", icon_type='GRAPHQL')\n\
      \                 return None\n        except (ValueError, DoesNotExist):\n\
      \             log_warning(f\"GraphQL: Message ID {id} not found or invalid.\"\
      , icon_type='GRAPHQL')\n             return None\n        except Exception as\
      \ e:\n             log_error(f\"GraphQL message_by_id error for ID {id}: {e}\"\
      , icon_type='GRAPHQL', exc_info=True)\n             return None\n\n\n# --- Strawberry\
      \ Mutation Root ---\n@strawberry.type\nclass MutationGQL:\n    @strawberry.mutation(description=\"\
      Creates a new message queue\")\n    async def create_queue(self, info: Info,\
      \ name: str = strawberry.argument(description=\"Unique name for the new queue\"\
      )) -> QueueGQL:\n         # Access context if needed for auth: user = info.context.get(\"\
      current_user\")\n         log_info(f\"\U0001F353 GraphQL Mutation: create_queue\
      \ (name='{name}')\", icon_type='GRAPHQL')\n         try:\n             new_queue,\
      \ created = await Queue.get_or_create(name=name)\n             if not created:\n\
      \                  raise Exception(f\"Queue '{name}' already exists.\") # Raise\
      \ GQL-handled exception\n\n             log_success(f\"GQL: Queue '{name}' created\
      \ (ID: {new_queue.id}).\", icon_type='QUEUE')\n             # Map ORM to GQL\
      \ type\n             return QueueGQL(\n                 id=strawberry.ID(str(new_queue.id)),\n\
      \                 name=new_queue.name,\n                 created_at=new_queue.created_at,\n\
      \                 updated_at=new_queue.updated_at\n             )\n        \
      \ except Exception as e: # Catch DB errors or the explicit raise above\n   \
      \           log_error(f\"GraphQL create_queue error: {e}\", icon_type='GRAPHQL',\
      \ exc_info=isinstance(e, IntegrityError)) # Less verbose trace for known conflict\n\
      \              raise Exception(f\"Failed to create queue '{name}': {e}\") #\
      \ Propagate as GraphQL error\n\n    @strawberry.mutation(description=\"Deletes\
      \ a queue and all its messages\")\n    async def delete_queue(self, info: Info,\
      \ name: str = strawberry.argument(description=\"Name of the queue to delete\"\
      )) -> bool:\n        log_info(f\"\U0001F353 GraphQL Mutation: delete_queue (name='{name}')\"\
      , icon_type='GRAPHQL')\n        try:\n            queue = await Queue.get_or_none(name=name)\n\
      \            if not queue:\n                 raise Exception(f\"Queue '{name}'\
      \ not found.\")\n\n            await queue.delete() # Cascade delete handled\
      \ by ORM\n            log_success(f\"GQL: Queue '{name}' deleted successfully.\"\
      , icon_type='QUEUE')\n            return True\n        except Exception as e:\n\
      \             log_error(f\"GraphQL delete_queue error for '{name}': {e}\", icon_type='GRAPHQL',\
      \ exc_info=True)\n             raise Exception(f\"Failed to delete queue '{name}':\
      \ {e}\")\n\n\n    @strawberry.mutation(description=\"Publishes a message to\
      \ a specified queue\")\n    async def publish_message(\n        self, info:\
      \ Info,\n        queue_name: str = strawberry.argument(description=\"Name of\
      \ the target queue\"),\n        content: str = strawberry.argument(description=\"\
      Message content/payload\")\n    ) -> MessageGQL:\n        # background_tasks\
      \ = info.context.get(\"background_tasks\") # Get from context if needed\n  \
      \      log_info(f\"\U0001F353 GraphQL Mutation: publish_message (queue='{queue_name}')\"\
      , icon_type='GRAPHQL')\n        try:\n            queue = await Queue.get_or_none(name=queue_name)\n\
      \            if not queue:\n                 raise Exception(f\"Queue '{queue_name}'\
      \ not found.\")\n\n            new_message = await Message.create(queue=queue,\
      \ content=content, status='pending')\n            log_success(f\"GQL: Message\
      \ ID {new_message.id} published to queue '{queue_name}'.\", icon_type='MSG')\n\
      \n            # Background task for SSE notification (cannot directly access\
      \ FastAPI's background_tasks here easily)\n            # Option 1: Don't notify\
      \ SSE from GQL mutations\n            # Option 2: Use a separate async task\
      \ runner (e.g., Celery, ARQ) triggered here\n            # Option 3: Pass BackgroundTasks\
      \ through context (more complex setup)\n            # For simplicity, omitting\
      \ SSE notification from GQL mutation here.\n            # await notify_sse(queue_name,\
      \ new_message.id, \"new_message\") # Would need global redis_sse access\n\n\
      \            return MessageGQL.from_orm(new_message, queue_name_str=queue_name)\n\
      \n        except Exception as e:\n            log_error(f\"GraphQL publish_message\
      \ error to queue '{queue_name}': {e}\", icon_type='GRAPHQL', exc_info=True)\n\
      \            raise Exception(f\"Failed to publish message: {e}\")\n\n\n# ---\
      \ GraphQL Context Getter ---\nasync def get_graphql_context(\n    request: Request,\
      \ # Access request object\n    response: Response, # Access response object\n\
      \    background_tasks: BackgroundTasks, # Inject background tasks\n    # Use\
      \ HTTPBearer for token, allows optional authentication if needed later\n   \
      \ auth: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme)\n) ->\
      \ Dict:\n    \"\"\"Provides context for GraphQL resolvers, including authenticated\
      \ user and background tasks.\"\"\"\n    context = {\n        \"request\": request,\n\
      \        \"response\": response,\n        \"background_tasks\": background_tasks,\
      \ # Make BT available if needed by mutations\n        \"current_user\": None\n\
      \    }\n    if auth: # If Authorization header was present\n        try:\n \
      \           # Use the internal decode logic, expecting an access token for GQL\
      \ access\n            username = await _decode_token(auth.credentials, \"access\"\
      )\n            context[\"current_user\"] = username\n        except HTTPException\
      \ as auth_exc:\n            # GQL should ideally raise its own errors, but we\
      \ can log here\n            log_warning(f\"GraphQL authentication failed: {auth_exc.detail}\"\
      , icon_type='AUTH')\n            # Do not raise HTTPException here; let Strawberry\
      \ handle lack of user if resolvers require it\n            # Resolvers should\
      \ check `info.context.get(\"current_user\")` if auth is mandatory\n\n    return\
      \ context\n\n\n# --- Initialize GraphQL Schema and Router ---\ngql_schema =\
      \ strawberry.Schema(query=QueryGQL, mutation=MutationGQL)\ngraphql_app = GraphQLRouter(\n\
      \    gql_schema,\n    context_getter=get_graphql_context, # Use custom context\
      \ getter\n    graphiql=None, # Deprecated: Use graphql_ide instead\n    graphql_ide=\"\
      apollo-sandbox\" # Preferred: \"apollo-sandbox\", \"graphiql\"\n)\n\napp.include_router(\n\
      \    graphql_app,\n    prefix=\"/graphql\",\n    tags=[\"GraphQL\"],\n    #\
      \ Authentication is handled within the context_getter/resolvers now\n)\nlog_success(\"\
      \U0001F353 GraphQL endpoint /graphql configured.\", icon_type='GRAPHQL')\n\n\
      \n# --- Global Exception Handlers ---\n# Order handlers from most specific to\
      \ least specific\n\n@app.exception_handler(DoesNotExist)\nasync def tortoise_does_not_exist_handler(request:\
      \ Request, exc: DoesNotExist):\n    \"\"\"Handles Tortoise DoesNotExist errors,\
      \ returning 404.\"\"\"\n    model_name = str(exc).split(\":\")[0] # Attempt\
      \ to get model name\n    detail = f\"Resource not found ({model_name}).\"\n\
      \    log_warning(f\"Resource Not Found (DB): {exc} ({request.method} {request.url.path})\"\
      , icon_type='DB', extra={\"client\": request.client.host})\n    return JSONResponse(\n\
      \        status_code=status.HTTP_404_NOT_FOUND,\n        content={\"detail\"\
      : detail},\n    )\n\n@app.exception_handler(IntegrityError)\nasync def tortoise_integrity_error_handler(request:\
      \ Request, exc: IntegrityError):\n    \"\"\"Handles Tortoise IntegrityError\
      \ (e.g., unique constraint), returning 409.\"\"\"\n    detail = \"Database conflict\
      \ occurred.\"\n    # Try to extract specific constraint violation message if\
      \ available (driver-dependent)\n    if hasattr(exc, 'args') and exc.args:\n\
      \         detail += f\" Details: {exc.args[0]}\"\n    log_warning(f\"Database\
      \ Integrity Conflict: {exc} ({request.method} {request.url.path})\", icon_type='DB',\
      \ extra={\"client\": request.client.host})\n    return JSONResponse(\n     \
      \   status_code=status.HTTP_409_CONFLICT,\n        content={\"detail\": detail},\n\
      \    )\n\n@app.exception_handler(ValidationError)\nasync def pydantic_validation_exception_handler(request:\
      \ Request, exc: ValidationError):\n    \"\"\"Handles Pydantic validation errors,\
      \ returning 422.\"\"\"\n    log_warning(f\"Request Validation Error: {exc.errors()}\
      \ ({request.method} {request.url.path})\", icon_type='HTTP', extra={\"client\"\
      : request.client.host})\n    # Provide structured error details\n    return\
      \ JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n\
      \        content={\"detail\": \"Request validation failed\", \"errors\": exc.errors()},\n\
      \    )\n\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request:\
      \ Request, exc: HTTPException):\n    \"\"\"Handles FastAPI's standard HTTPExceptions.\"\
      \"\"\n    log_level = log_warning if 400 <= exc.status_code < 500 else log_error\n\
      \    icon = 'HTTP' if 400 <= exc.status_code < 500 else 'ERROR'\n    log_level(\n\
      \        f\"HTTP Error Handled: Status={exc.status_code}, Detail='{exc.detail}'\
      \ ({request.method} {request.url.path})\",\n        icon_type=icon,\n      \
      \  extra={\"client\": request.client.host if request.client else \"N/A\"}\n\
      \    )\n    return JSONResponse(\n        status_code=exc.status_code,\n   \
      \     content={\"detail\": exc.detail},\n        headers=getattr(exc, \"headers\"\
      , None), # Include headers like WWW-Authenticate\n    )\n\n@app.exception_handler(Exception)\n\
      async def generic_exception_handler(request: Request, exc: Exception):\n   \
      \ \"\"\"Generic fallback handler for any unhandled exceptions, returning 500.\"\
      \"\"\n    tb_str = \"\".join(traceback.format_exception(etype=type(exc), value=exc,\
      \ tb=exc.__traceback__))\n    log_critical(\n        f\"Unhandled Internal Server\
      \ Error: {type(exc).__name__}: {exc} ({request.method} {request.url.path})\"\
      ,\n        icon_type='CRITICAL',\n        exc_info=False, # Traceback logged\
      \ separately\n        extra={\n            \"client\": request.client.host if\
      \ request.client else \"N/A\",\n            \"traceback_summary\": tb_str.splitlines()[-3:],\
      \ # Log last few lines of traceback\n            \"full_traceback\": tb_str\
      \ # Include full traceback in JSON log if needed\n        }\n    )\n    # Update\
      \ global error state\n    async with stats_lock:\n        app_stats[\"last_error\"\
      ] = f\"Unhandled {type(exc).__name__} @ {datetime.now(timezone.utc).isoformat()}\"\
      \n\n    return JSONResponse(\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n\
      \        content={\"detail\": \"An unexpected internal server error occurred.\
      \ Please check server logs.\"},\n    )\n\n# --- Main Execution Block ---\nimport\
      \ os\nimport sys\n# Assume the following are defined/imported elsewhere:\n#\
      \ - settings (with CERT_DIR, CERT_FILE, KEY_FILE attributes)\n# - log_info,\
      \ log_warning, log_critical, log_success functions\n# - generate_self_signed_cert\
      \ function\n\nif __name__ == '__main__':\n    log_info(\"\U0001F3C1 Main execution\
      \ block started...\", icon_type='STARTUP')\n\n    # --- 1. Check/Generate Self-Signed\
      \ Certificates for HTTPS ---\n    log_info(\"Checking for SSL certificate and\
      \ key...\", icon_type='SEC')\n    try:\n        # Ensure the certificate directory\
      \ exists\n        os.makedirs(settings.CERT_DIR, exist_ok=True)\n\n        cert_exists\
      \ = os.path.exists(settings.CERT_FILE)\n        key_exists = os.path.exists(settings.KEY_FILE)\n\
      \n        # Check if both certificate and key file exist\n        if cert_exists\
      \ and key_exists:\n            log_success(f\"\U0001F6E1️ SSL Certificate '{os.path.basename(settings.CERT_FILE)}'\
      \ and Key '{os.path.basename(settings.KEY_FILE)}' found in '{settings.CERT_DIR}'.\"\
      , icon_type='SEC')\n        else:\n            # If one or both are missing,\
      \ attempt generation\n            missing_files = []\n            if not cert_exists:\
      \ missing_files.append(os.path.basename(settings.CERT_FILE))\n            if\
      \ not key_exists: missing_files.append(os.path.basename(settings.KEY_FILE))\n\
      \            log_warning(f\"SSL {' / '.join(missing_files)} not found. Generating\
      \ new self-signed ones for localhost...\", icon_type='SEC')\n\n            try:\n\
      \                # Import 'ipaddress' only when needed for generation\n    \
      \            # This avoids an unnecessary import if certs already exist.\n \
      \               import ipaddress # noqa: F401 -- Keep import here as it's conditional\n\
      \                log_info(\"Attempting to generate self-signed certificates...\"\
      , icon_type='GEN')\n\n                if not generate_self_signed_cert(settings.CERT_FILE,\
      \ settings.KEY_FILE, common_name=\"localhost\"):\n                    # The\
      \ function itself should ideally log details, but we catch the failure indication\n\
      \                    log_critical(\"Critical failure generating SSL certificates\
      \ (function returned false). Aborting.\", icon_type='CRITICAL')\n          \
      \          sys.exit(1)\n                else:\n                    log_success(\"\
      ✅ Successfully generated self-signed SSL certificate and key.\", icon_type='SEC')\n\
      \n            except ImportError:\n                log_critical(\"The 'ipaddress'\
      \ module is required to generate certificates with IP SAN. Install it (`pip\
      \ install ipaddress`) or provide existing certs.\", icon_type='CRITICAL')\n\
      \                sys.exit(1)\n            except Exception as cert_gen_e:\n\
      \                # Catch any other unexpected error during the generation process\n\
      \                log_critical(f\"Unexpected error during certificate generation:\
      \ {cert_gen_e}\", icon_type='CRITICAL')\n                sys.exit(1)\n\n   \
      \ except Exception as setup_e:\n         # Catch potential errors during directory\
      \ creation or initial path checks\n         log_critical(f\"Unexpected error\
      \ during initial certificate setup check: {setup_e}\", icon_type='CRITICAL')\n\
      \         sys.exit(1)\n\n    # --- Continue with the rest of the main execution\
      \ ---\n    log_info(\"Initial setup checks complete. Proceeding...\", icon_type='SETUP')\n\
      \    # ... (rest of your main script logic would go here) ...\n    # For example:\n\
      \    # server = setup_server()\n    # server.run()\n\n    # DB Init and Redis\
      \ Init are handled by the lifespan manager\n\n    # Log Configuration Summary\n\
      \    log_info(f\"=== Configuration Summary ===\", icon_type='INFO')\n    log_info(f\"\
      \  Project: {settings.PROJECT_NAME} v{settings.VERSION}\", icon_type='INFO')\n\
      \    log_info(f\"  Environment: {settings.APP_ENV}\", icon_type='INFO')\n  \
      \  log_info(f\"  Log Level: {settings.LOG_LEVEL_STR}\", icon_type='LOGS')\n\
      \    log_info(f\"  JWT Secret: {'Set (Hidden)' if settings.JWT_SECRET_KEY !=\
      \ secrets.token_hex(32) else 'Using Generated Default'}\", icon_type='AUTH')\n\
      \    log_info(f\"  DB Path: {settings.DB_PATH}\", icon_type='DB')\n    log_info(f\"\
      \  Redis URL: {settings.REDIS_URL} (SSE DB: {settings.REDIS_SSE_DB}, Limiter\
      \ DB: {settings.REDIS_RATE_LIMIT_DB})\", icon_type='SSE')\n    log_info(f\"\
      \  Rate Limit: {settings.DEFAULT_RATE_LIMIT}\", icon_type='RATELIMIT')\n   \
      \ log_info(f\"  CORS Origins: {settings.ALLOWED_ORIGINS}\", icon_type='HTTP')\n\
      \    log_info(f\"  Log Dir: {settings.LOG_DIR}\", icon_type='LOGS')\n    log_info(f\"\
      \  Cert Dir: {settings.CERT_DIR}\", icon_type='SEC')\n    log_info(f\"============================\"\
      , icon_type='INFO')\n\n\n    # Determine Uvicorn settings based on environment\n\
      \    reload_enabled = settings.APP_ENV == \"development\"\n    log_level_uvicorn\
      \ = \"debug\" if reload_enabled else \"info\"\n\n    log_info(f\"\U0001F310\U0001F680\
      \ Starting Uvicorn server on https://0.0.0.0:{settings.API_PORT}\", icon_type='STARTUP',\
      \ extra={\"reload\": reload_enabled, \"log_level\": log_level_uvicorn})\n  \
      \  log_info(f\"   Access API root at: https://localhost:{settings.API_PORT}/\"\
      , icon_type='HTTP')\n    log_info(f\"   Swagger UI docs:  https://localhost:{settings.API_PORT}/docs\"\
      , icon_type='HTTP')\n    log_info(f\"   ReDoc docs:       https://localhost:{settings.API_PORT}/redoc\"\
      , icon_type='HTTP')\n    log_info(f\"   GraphQL endpoint: https://localhost:{settings.API_PORT}/graphql\"\
      , icon_type='GRAPHQL')\n    log_info(\"   Press Ctrl+C to stop the server.\"\
      , icon_type='INFO')\n\n    try:\n        uvicorn.run(\n            \"__main__:app\"\
      , # Point to the FastAPI app instance in this file\n            host=\"0.0.0.0\"\
      ,\n            port=settings.API_PORT,\n            log_level=log_level_uvicorn,\n\
      \            ssl_keyfile=settings.KEY_FILE,\n            ssl_certfile=settings.CERT_FILE,\n\
      \            reload=reload_enabled,\n            # lifespan=\"on\" # Uvicorn\
      \ detects lifespan automatically\n            use_colors=True # Enable Uvicorn's\
      \ own color logs if desired\n        )\n    except KeyboardInterrupt:\n    \
      \    log_info(\"\\n\U0001F6A6 Server shutdown requested via Ctrl+C.\", icon_type='SHUTDOWN')\n\
      \    except Exception as e:\n        # Catch potential startup errors (e.g.,\
      \ port binding)\n        log_critical(f\"Fatal: Failed to start or run Uvicorn\
      \ server: {e}\", exc_info=True)\n        sys.exit(1)\n    finally:\n       \
      \ log_info(\"\U0001F3C1 Uvicorn server process finished.\", icon_type='SHUTDOWN')"
    tamanho: 0.10 MB
  message-broker-v2-clean.py:
    caminho_completo: .\message-broker-v2-clean.py
    classes:
    - docstring: null
      end_lineno: 95
      lineno: 72
      name: Settings
    - docstring: null
      end_lineno: 131
      lineno: 120
      name: ColoramaFormatter
    - docstring: null
      end_lineno: 152
      lineno: 133
      name: JsonFormatter
    - docstring: null
      end_lineno: 340
      lineno: 338
      name: QueueBase
    - docstring: null
      end_lineno: 342
      lineno: 342
      name: QueueCreatePayload
    - docstring: null
      end_lineno: 349
      lineno: 344
      name: QueueResponse
    - docstring: null
      end_lineno: 352
      lineno: 351
      name: MessageBase
    - docstring: null
      end_lineno: 354
      lineno: 354
      name: MessagePayload
    - docstring: null
      end_lineno: 362
      lineno: 356
      name: MessageResponse
    - docstring: null
      end_lineno: 367
      lineno: 364
      name: MessagePublishResponse
    - docstring: null
      end_lineno: 375
      lineno: 369
      name: MessageConsumeResponse
    - docstring: null
      end_lineno: 381
      lineno: 377
      name: Token
    - docstring: null
      end_lineno: 400
      lineno: 383
      name: StatsResponse
    - docstring: null
      end_lineno: 404
      lineno: 402
      name: LogFileResponse
    - docstring: null
      end_lineno: 422
      lineno: 407
      name: Queue
    - docstring: null
      end_lineno: 445
      lineno: 424
      name: Message
    - docstring: null
      end_lineno: 1399
      lineno: 1381
      name: MessageGQL
    - docstring: null
      end_lineno: 1457
      lineno: 1402
      name: QueueGQL
    - docstring: null
      end_lineno: 1514
      lineno: 1461
      name: QueryGQL
    - docstring: null
      end_lineno: 1572
      lineno: 1518
      name: MutationGQL
    - docstring: null
      end_lineno: 419
      lineno: 417
      name: Meta
    - docstring: null
      end_lineno: 442
      lineno: 438
      name: Meta
    functions:
    - docstring: null
      end_lineno: 180
      lineno: 180
      name: log_debug
    - docstring: null
      end_lineno: 181
      lineno: 181
      name: log_info
    - docstring: null
      end_lineno: 182
      lineno: 182
      name: log_success
    - docstring: null
      end_lineno: 183
      lineno: 183
      name: log_warning
    - docstring: null
      end_lineno: 184
      lineno: 184
      name: log_error
    - docstring: null
      end_lineno: 185
      lineno: 185
      name: log_critical
    - docstring: null
      end_lineno: 186
      lineno: 186
      name: log_pipeline
    - docstring: null
      end_lineno: 244
      lineno: 189
      name: generate_self_signed_cert
    - docstring: null
      end_lineno: 131
      lineno: 124
      name: format
    - docstring: null
      end_lineno: 135
      lineno: 134
      name: formatTime
    - docstring: null
      end_lineno: 152
      lineno: 137
      name: format
    - docstring: null
      end_lineno: 422
      lineno: 421
      name: __str__
    - docstring: null
      end_lineno: 445
      lineno: 444
      name: __str__
    - docstring: null
      end_lineno: 983
      lineno: 924
      name: read_and_parse_log_sync
    - docstring: Helper to create MessageGQL from Tortoise Message model.
      end_lineno: 1399
      lineno: 1390
      name: from_orm
    - docstring: null
      end_lineno: 819
      lineno: 765
      name: _get_psutil_data_sync
    - docstring: null
      end_lineno: 881
      lineno: 880
      name: list_dir_sync
    imports:
    - asname: null
      name: asyncio
    - asname: null
      name: json
    - asname: null
      name: logging
    - asname: null
      name: os
    - asname: null
      name: platform
    - asname: null
      name: secrets
    - asname: null
      name: sys
    - asname: null
      name: time
    - asname: null
      name: traceback
    - module: contextlib
      names:
      - asynccontextmanager
    - module: datetime
      names:
      - datetime
      - timezone
      - timedelta
    - module: typing
      names:
      - Dict
      - Any
      - Optional
      - List
      - AsyncGenerator
      - Union
    - module: collections
      names:
      - deque
    - asname: null
      name: hashlib
    - asname: null
      name: ipaddress
    - module: tortoise
      names:
      - Tortoise
      - fields
      - models
    - module: tortoise.exceptions
      names:
      - DoesNotExist
      - IntegrityError
    - module: tortoise.transactions
      names:
      - in_transaction
    - module: fastapi
      names:
      - FastAPI
      - Request
      - Response
      - Depends
      - HTTPException
      - status
      - BackgroundTasks
      - Path
      - Query
    - module: fastapi.middleware.cors
      names:
      - CORSMiddleware
    - module: fastapi.security
      names:
      - OAuth2PasswordBearer
      - OAuth2PasswordRequestForm
      - HTTPBearer
      - HTTPAuthorizationCredentials
    - module: fastapi.responses
      names:
      - JSONResponse
    - asname: null
      name: uvicorn
    - module: jose
      names:
      - JWTError
      - jwt
    - module: pydantic
      names:
      - BaseModel
      - ValidationError
      - Field
      - EmailStr
      - ConfigDict
    - module: colorama
      names:
      - init
      - Fore
      - Style
    - module: cryptography
      names:
      - x509
    - module: cryptography.x509.oid
      names:
      - NameOID
    - module: cryptography.hazmat.primitives
      names:
      - hashes
    - module: cryptography.hazmat.backends
      names:
      - default_backend
    - module: cryptography.hazmat.primitives
      names:
      - serialization
    - module: cryptography.hazmat.primitives.asymmetric
      names:
      - rsa
    - asname: null
      name: psutil
    - module: werkzeug.utils
      names:
      - secure_filename
    - module: slowapi
      names:
      - Limiter
      - _rate_limit_exceeded_handler
    - module: slowapi.util
      names:
      - get_remote_address
    - module: slowapi.errors
      names:
      - RateLimitExceeded
    - module: slowapi.middleware
      names:
      - SlowAPIMiddleware
    - asname: null
      name: strawberry
    - module: strawberry.fastapi
      names:
      - GraphQLRouter
    - module: strawberry.types
      names:
      - Info
    numero_de_linhas: 1811
    source_code: "# -*- coding: utf-8 -*-\nimport asyncio\nimport json\nimport logging\n\
      import os\nimport platform\nimport secrets\nimport sys\nimport time\nimport\
      \ traceback\nfrom contextlib import asynccontextmanager\nfrom datetime import\
      \ datetime, timezone, timedelta\nfrom typing import Dict, Any, Optional, List,\
      \ AsyncGenerator, Union # Added Union\nfrom collections import deque\nimport\
      \ hashlib\nimport ipaddress\n\n# --- Tortoise ORM Imports ---\n# Ensure these\
      \ are present\nfrom tortoise import Tortoise, fields, models\nfrom tortoise.exceptions\
      \ import DoesNotExist, IntegrityError\nfrom tortoise.transactions import in_transaction\
      \ # <<< ESSENTIAL IMPORT FOR FIXES\n\n# --- Dependency Imports (Try/Except for\
      \ User Feedback) ---\ntry:\n    from fastapi import (FastAPI, Request, Response,\
      \ Depends, HTTPException, status,\n                         BackgroundTasks,\
      \ Path, Query as FastQuery)\n    from fastapi.middleware.cors import CORSMiddleware\n\
      \    from fastapi.security import (OAuth2PasswordBearer, OAuth2PasswordRequestForm,\n\
      \                                  HTTPBearer, HTTPAuthorizationCredentials)\n\
      \    from fastapi.responses import JSONResponse\n    import uvicorn\n\n    from\
      \ jose import JWTError, jwt\n    from pydantic import BaseModel, ValidationError,\
      \ Field, EmailStr, ConfigDict\n\n    # Tortoise already imported above\n\n \
      \   from colorama import init, Fore, Style\n\n    from cryptography import x509\n\
      \    from cryptography.x509.oid import NameOID\n    from cryptography.hazmat.primitives\
      \ import hashes\n    from cryptography.hazmat.backends import default_backend\n\
      \    from cryptography.hazmat.primitives import serialization\n    from cryptography.hazmat.primitives.asymmetric\
      \ import rsa\n\n    import psutil\n    from werkzeug.utils import secure_filename\n\
      \n    from slowapi import Limiter, _rate_limit_exceeded_handler\n    from slowapi.util\
      \ import get_remote_address\n    from slowapi.errors import RateLimitExceeded\n\
      \    from slowapi.middleware import SlowAPIMiddleware\n\n    import strawberry\n\
      \    from strawberry.fastapi import GraphQLRouter\n    from strawberry.types\
      \ import Info\n\nexcept ImportError as e:\n    missing_pkg = str(e).split(\"\
      '\")[-2]\n    print(f\"\\nERROR: Missing dependency '{missing_pkg}'.\")\n  \
      \  print(\"Please install all required packages by running:\")\n    # Updated\
      \ install command list\n    print(\"\\n  pip install fastapi uvicorn[standard]\
      \ tortoise-orm aiosqlite pydantic[email] python-jose[cryptography] colorama\
      \ cryptography psutil Werkzeug slowapi strawberry-graphql[fastapi] Jinja2 ipaddress\
      \ passlib # Add passlib if using hashed passwords\\n\")\n    sys.exit(1)\n\n\
      # --- Basic Setup ---\ninit(autoreset=True) # Initialize Colorama\n\n# --- Settings\
      \ ---\nclass Settings:\n    PROJECT_NAME: str = \"Message Broker API V3.1.4\
      \ (FastAPI/Tortoise/ConsumeFix)\" # Updated Name\n    VERSION: str = \"0.3.1.4-fastapi-tortoise-fixed-consume\"\
      \ # Updated Version\n    API_PORT: int = 8777\n    # IMPORTANT: Generate a strong\
      \ secret key and set it via environment variable in production!\n    # Example\
      \ generation: python -c 'import secrets; print(secrets.token_hex(32))'\n   \
      \ JWT_SECRET_KEY: str = os.environ.get('JWT_SECRET_KEY', '!!_CHANGE_ME_IN_PRODUCTION_'\
      \ + secrets.token_hex(16) + '_!!')\n    ALGORITHM: str = \"HS256\"\n    ACCESS_TOKEN_EXPIRE_MINUTES:\
      \ int = 60 * 1 # 1 hour\n    REFRESH_TOKEN_EXPIRE_DAYS: int = 30\n    DB_DIR:\
      \ str = 'databases'\n    DB_FILENAME: str = 'message_broker_v3.db'\n    DB_PATH:\
      \ str = os.path.abspath(os.path.join(DB_DIR, DB_FILENAME))\n    DATABASE_URL:\
      \ str = f\"sqlite:///{DB_PATH}\"\n    LOG_DIR: str = 'logs_v3'\n    CERT_DIR:\
      \ str = 'certs_v3'\n    CERT_FILE: str = os.path.join(CERT_DIR, 'cert.pem')\n\
      \    KEY_FILE: str = os.path.join(CERT_DIR, 'key_nopass.pem')\n    ALLOWED_ORIGINS:\
      \ List[str] = [\"*\"] # WARNING: \"*\" is insecure for production. List specific\
      \ origins.\n    DEFAULT_RATE_LIMIT: str = \"200/minute\" # Default limit for\
      \ most endpoints\n    HIGH_TRAFFIC_RATE_LIMIT: str = \"200/second\" # Higher\
      \ limit for publish/consume/ack/nack\n    LOG_LEVEL_STR: str = os.environ.get(\"\
      LOG_LEVEL\", \"INFO\").upper()\n    LOG_LEVEL: int = getattr(logging, LOG_LEVEL_STR,\
      \ logging.INFO)\n    APP_ENV: str = os.environ.get(\"APP_ENV\", \"production\"\
      ).lower() # 'development' or 'production'\n\nsettings = Settings()\n\n# ---\
      \ Security Warnings ---\nif \"CHANGE_ME_IN_PRODUCTION\" in settings.JWT_SECRET_KEY\
      \ and settings.APP_ENV == \"production\":\n    print(f\"{Fore.RED}{Style.BRIGHT}\U0001F6A8\
      \ CRITICAL SECURITY WARNING: Running in PRODUCTION environment with a DEFAULT\
      \ JWT_SECRET_KEY! Generate a strong secret and set the JWT_SECRET_KEY environment\
      \ variable.{Style.RESET_ALL}\")\n    # Consider exiting if in production with\
      \ default secret: sys.exit(1)\nelif \"CHANGE_ME_IN_PRODUCTION\" in settings.JWT_SECRET_KEY:\n\
      \     print(f\"{Fore.YELLOW}⚠️ SECURITY WARNING: Using a generated default JWT_SECRET_KEY.\
      \ Set the JWT_SECRET_KEY environment variable for persistent sessions between\
      \ restarts.{Style.RESET_ALL}\")\n\nif settings.ALLOWED_ORIGINS == [\"*\"] and\
      \ settings.APP_ENV == \"production\":\n    print(f\"{Fore.YELLOW}⚠️ SECURITY\
      \ WARNING: CORS ALLOWED_ORIGINS is set to '*' in production. This is insecure.\
      \ Specify allowed origins explicitly.{Style.RESET_ALL}\")\n\n\n# --- Directory\
      \ Setup ---\nos.makedirs(settings.LOG_DIR, exist_ok=True)\nos.makedirs(settings.CERT_DIR,\
      \ exist_ok=True)\nos.makedirs(settings.DB_DIR, exist_ok=True)\n\n# --- Logging\
      \ Setup ---\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nunique_hash\
      \ = hashlib.sha1(str(os.getpid()).encode()).hexdigest()[:8]\nLOG_FILENAME =\
      \ os.path.join(settings.LOG_DIR, f\"broker_log_{timestamp}_{unique_hash}.json\"\
      )\n\nclass ColoramaFormatter(logging.Formatter):\n    LEVEL_COLORS = { logging.DEBUG:\
      \ Fore.CYAN, logging.INFO: Fore.GREEN, logging.WARNING: Fore.YELLOW, logging.ERROR:\
      \ Fore.RED, logging.CRITICAL: Fore.MAGENTA }\n    LEVEL_ICONS = { logging.DEBUG:\
      \ \"⚙️ \", logging.INFO: \"ℹ️ \", logging.WARNING: \"⚠️ \", logging.ERROR: \"\
      ❌ \", logging.CRITICAL: \"\U0001F525 \", 'SUCCESS': \"✅ \", 'PIPELINE': \"➡️\
      \ \", 'DB': \"\U0001F4BE \", 'AUTH': \"\U0001F511 \", 'QUEUE': \"\U0001F4E5\
      \ \", 'MSG': \"✉️ \", 'HTTP': \"\U0001F310 \", 'STATS': \"\U0001F4CA \", 'LOGS':\
      \ \"\U0001F4C4 \", 'SEC': \"\U0001F6E1️ \", 'ASYNC': \"⚡ \", 'GRAPHQL': \"\U0001F353\
      \ \", 'RATELIMIT': \"⏱️ \", 'STARTUP': '\U0001F680', 'SHUTDOWN': '\U0001F6D1\
      ', 'GEN': '✨', 'SETUP': '\U0001F527'}\n\n    def format(self, record):\n   \
      \     level_color = self.LEVEL_COLORS.get(record.levelno, Fore.WHITE)\n    \
      \    icon_type = getattr(record, 'icon_type', record.levelname)\n        icon\
      \ = self.LEVEL_ICONS.get(icon_type, \"\")\n        formatted_time = self.formatTime(record,\
      \ self.datefmt)\n        log_message_content = f\"[{record.levelname}] {icon}{record.getMessage()}\"\
      \n        log_line = f\"{formatted_time} - {record.name} - {level_color}{Style.BRIGHT}{log_message_content}{Style.RESET_ALL}\"\
      \n        return log_line\n\nclass JsonFormatter(logging.Formatter):\n    def\
      \ formatTime(self, record, datefmt=None):\n        return datetime.fromtimestamp(record.created,\
      \ tz=timezone.utc).isoformat(timespec='milliseconds').replace('+00:00', 'Z')\n\
      \n    def format(self, record):\n        log_record = {\n            'timestamp':\
      \ self.formatTime(record), 'level': record.levelname, 'name': record.name,\n\
      \            'pid': record.process, 'thread': record.threadName, 'message':\
      \ record.getMessage(),\n            'pathname': record.pathname, 'lineno': record.lineno,\n\
      \        }\n        if hasattr(record, 'icon_type'): log_record['icon_type']\
      \ = record.icon_type\n        if hasattr(record, 'extra_data') and isinstance(record.extra_data,\
      \ dict): log_record.update(record.extra_data)\n        if record.exc_info:\n\
      \            # Only include full traceback in development for security\n   \
      \         traceback_str = \"\".join(traceback.format_exception(*record.exc_info))\
      \ if settings.APP_ENV == 'development' else 'Traceback hidden in production'\n\
      \            log_record['exception'] = {\n                'type': record.exc_info[0].__name__,\
      \ 'value': str(record.exc_info[1]),\n                'traceback': traceback_str\n\
      \            }\n        return json.dumps(log_record, ensure_ascii=False, default=str)\n\
      \nlogger = logging.getLogger(\"MessageBroker\")\nlogger.setLevel(settings.LOG_LEVEL)\n\
      logger.propagate = False\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\nif not logger.handlers:\n\
      \    # Console Handler (Colored Text)\n    console_handler = logging.StreamHandler(sys.stdout)\n\
      \    console_handler.setLevel(settings.LOG_LEVEL)\n    console_formatter = ColoramaFormatter(datefmt=DATE_FORMAT)\n\
      \    console_handler.setFormatter(console_formatter)\n    logger.addHandler(console_handler)\n\
      \n    # File Handler (JSON)\n    try:\n        file_handler = logging.FileHandler(LOG_FILENAME,\
      \ mode='a', encoding='utf-8')\n        file_handler.setLevel(settings.LOG_LEVEL)\n\
      \        file_formatter = JsonFormatter()\n        file_handler.setFormatter(file_formatter)\n\
      \        logger.addHandler(file_handler)\n    except Exception as e:\n     \
      \    # Log error about file handler to console if it fails\n         logger.error(f\"\
      ❌ CRITICAL: Failed to initialize file logging ({LOG_FILENAME}): {e}\", exc_info=True)\n\
      \n\n# --- Logging Helper Functions ---\ndef log_debug(message: str, icon_type:\
      \ str = 'DEBUG', extra: Optional[Dict[str, Any]] = None): logger.debug(message,\
      \ extra={'icon_type': icon_type, 'extra_data': extra or {}})\ndef log_info(message:\
      \ str, icon_type: str = 'INFO', extra: Optional[Dict[str, Any]] = None): logger.info(message,\
      \ extra={'icon_type': icon_type, 'extra_data': extra or {}})\ndef log_success(message:\
      \ str, icon_type: str = 'SUCCESS', extra: Optional[Dict[str, Any]] = None):\
      \ logger.info(message, extra={'icon_type': icon_type, 'extra_data': extra or\
      \ {}})\ndef log_warning(message: str, icon_type: str = 'WARNING', extra: Optional[Dict[str,\
      \ Any]] = None): logger.warning(message, extra={'icon_type': icon_type, 'extra_data':\
      \ extra or {}})\ndef log_error(message: str, exc_info: bool = False, icon_type:\
      \ str = 'ERROR', extra: Optional[Dict[str, Any]] = None): logger.error(message,\
      \ exc_info=exc_info, extra={'icon_type': icon_type, 'extra_data': extra or {}})\n\
      def log_critical(message: str, exc_info: bool = False, icon_type: str = 'CRITICAL',\
      \ extra: Optional[Dict[str, Any]] = None): logger.critical(message, exc_info=exc_info,\
      \ extra={'icon_type': icon_type, 'extra_data': extra or {}})\ndef log_pipeline(message:\
      \ str, icon_type: str = 'PIPELINE', extra: Optional[Dict[str, Any]] = None):\
      \ logger.info(message, extra={'icon_type': icon_type, 'extra_data': extra or\
      \ {}})\n\n# --- SSL Certificate Generation ---\ndef generate_self_signed_cert(cert_path:\
      \ str, key_path: str, key_password: Optional[bytes] = None, common_name: str\
      \ = \"localhost\"):\n    log_info(f\"\U0001F6E1️ Generating new RSA private\
      \ key and self-signed certificate for CN='{common_name}'...\", icon_type='GEN')\n\
      \    try:\n        private_key = rsa.generate_private_key(public_exponent=65537,\
      \ key_size=2048, backend=default_backend())\n        subject = issuer = x509.Name([\n\
      \            x509.NameAttribute(NameOID.COUNTRY_NAME, u\"XX\"),\n          \
      \  x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, u\"DefaultState\"),\n\
      \            x509.NameAttribute(NameOID.LOCALITY_NAME, u\"DefaultCity\"),\n\
      \            x509.NameAttribute(NameOID.ORGANIZATION_NAME, u\"Message Broker\
      \ SelfSigned\"),\n            x509.NameAttribute(NameOID.COMMON_NAME, common_name),\n\
      \        ])\n        # Add Subject Alternative Name (SAN) for localhost IP and\
      \ DNS name\n        san_extension = x509.SubjectAlternativeName([\n        \
      \    x509.DNSName(common_name),\n            x509.IPAddress(ipaddress.ip_address(\"\
      127.0.0.1\")),\n            # Add other IPs/DNS names if needed, e.g., x509.DNSName(\"\
      my-service.local\")\n        ])\n        cert_builder = x509.CertificateBuilder().subject_name(subject).issuer_name(issuer).public_key(private_key.public_key())\
      \ \\\n            .serial_number(x509.random_serial_number()).not_valid_before(datetime.now(timezone.utc)\
      \ - timedelta(days=1)) \\\n            .not_valid_after(datetime.now(timezone.utc)\
      \ + timedelta(days=365*2)) \\\n            .add_extension(san_extension, critical=False)\
      \ # SAN is not critical\n\n        # Add Basic Constraints extension (recommended\
      \ for self-signed certs)\n        cert_builder = cert_builder.add_extension(\n\
      \            x509.BasicConstraints(ca=False, path_length=None), critical=True,\n\
      \        )\n\n        certificate = cert_builder.sign(private_key, hashes.SHA256(),\
      \ default_backend())\n\n        # Choose key encryption (NoEncryption is fine\
      \ for local dev, use password in staging/prod)\n        key_pem_encryption =\
      \ serialization.NoEncryption()\n        if key_password:\n             log_info(\"\
      \U0001F511 Encrypting private key with provided password.\", icon_type='SEC')\n\
      \             key_pem_encryption = serialization.BestAvailableEncryption(key_password)\n\
      \n        # Write private key to file\n        with open(key_path, \"wb\") as\
      \ f:\n             f.write(private_key.private_bytes(\n                 encoding=serialization.Encoding.PEM,\n\
      \                 format=serialization.PrivateFormat.PKCS8, # Use PKCS8, more\
      \ modern than TraditionalOpenSSL\n                 encryption_algorithm=key_pem_encryption\n\
      \             ))\n        log_success(f\"\U0001F511 Private key saved: {key_path}\"\
      , icon_type='SEC')\n\n        # Write certificate to file\n        with open(cert_path,\
      \ \"wb\") as f:\n             f.write(certificate.public_bytes(serialization.Encoding.PEM))\n\
      \        log_success(f\"\U0001F4DC Self-signed certificate saved: {cert_path}\"\
      , icon_type='SEC')\n        return True\n    except ImportError:\n        #\
      \ This should not happen if dependencies are installed, but good practice\n\
      \        log_critical(\"The 'ipaddress' module is required for certificate generation.\
      \ Please install it (`pip install ipaddress`).\", icon_type='CRITICAL')\n  \
      \      return False\n    except Exception as e:\n        log_critical(f\"Failed\
      \ to generate certificates/key: {e}\", exc_info=True, icon_type='CRITICAL')\n\
      \        return False\n\n# --- Application State & Stats ---\napp_stats: Dict[str,\
      \ Any] = {\n    \"start_time\": datetime.now(timezone.utc),\n    \"requests_total\"\
      : 0, \"requests_by_route\": {}, \"requests_by_status\": {},\n    \"queues_total\"\
      : 0, \"messages_total\": 0, \"messages_pending\": 0,\n    \"messages_processing\"\
      : 0, \"messages_processed\": 0, \"messages_failed\": 0,\n    \"last_error\"\
      : None, \"last_error_timestamp\": None,\n    \"system\": {\n        \"python_version\"\
      : platform.python_version(), \"platform\": platform.system(),\n        \"platform_release\"\
      : platform.release(), \"architecture\": platform.machine(),\n    },\n    \"\
      broker_specific\": {\n        \"framework\": \"FastAPI\", \"version\": settings.VERSION,\
      \ \"db_engine\": \"sqlite (tortoise-orm)\",\n        \"auth_method\": \"jwt\
      \ (access+refresh, python-jose)\",\n        \"notification\": \"None\", \"rate_limit\"\
      : \"In-Memory (slowapi)\",\n        \"graphql\": \"strawberry-graphql\"\n  \
      \  }\n}\nstats_lock = asyncio.Lock() # Lock for thread-safe updates to app_stats\n\
      \n# --- Stats Update Functions ---\nasync def update_request_stats(route_template:\
      \ str, method: str, status_code: int):\n    \"\"\"Increment request counters\
      \ safely.\"\"\"\n    async with stats_lock:\n        app_stats[\"requests_total\"\
      ] += 1\n        route_stats = app_stats[\"requests_by_route\"].setdefault(route_template,\
      \ {})\n        route_stats[method] = route_stats.get(method, 0) + 1\n      \
      \  app_stats[\"requests_by_status\"][str(status_code)] = app_stats[\"requests_by_status\"\
      ].get(str(status_code), 0) + 1\n\nasync def update_broker_stats():\n    \"\"\
      \"Fetch queue and message counts from DB and update stats.\"\"\"\n    log_pipeline(\"\
      \U0001F4CA Fetching broker stats from DB...\", icon_type='STATS')\n    try:\n\
      \        # Use asyncio.gather for concurrent DB queries\n        # Ensure Tortoise\
      \ is initialized before calling this\n        if not Tortoise._connections:\n\
      \             log_warning(\"DB not initialized, cannot update broker stats.\"\
      , icon_type='DB')\n             return\n\n        q_count_task = Queue.all().count()\n\
      \        m_pending_task = Message.filter(status='pending').count()\n       \
      \ m_processing_task = Message.filter(status='processing').count()\n        m_processed_task\
      \ = Message.filter(status='processed').count()\n        m_failed_task = Message.filter(status='failed').count()\n\
      \n        q_count, pending, processing, processed, failed = await asyncio.gather(\n\
      \            q_count_task, m_pending_task, m_processing_task, m_processed_task,\
      \ m_failed_task\n        )\n        total = pending + processing + processed\
      \ + failed\n\n        async with stats_lock:\n            app_stats[\"queues_total\"\
      ] = q_count\n            app_stats[\"messages_pending\"] = pending\n       \
      \     app_stats[\"messages_processing\"] = processing\n            app_stats[\"\
      messages_processed\"] = processed\n            app_stats[\"messages_failed\"\
      ] = failed\n            app_stats[\"messages_total\"] = total\n            #\
      \ Clear last error if update was successful and it was a stats update error\n\
      \            if app_stats[\"last_error\"] and \"Broker Stats Update Failed\"\
      \ in app_stats[\"last_error\"]:\n                 app_stats[\"last_error\"]\
      \ = None\n                 app_stats[\"last_error_timestamp\"] = None\n    \
      \    log_success(\"\U0001F4CA Broker stats updated.\", icon_type='STATS', extra={'counts':\
      \ {'queues': q_count, 'pending': pending, 'processing': processing, 'processed':\
      \ processed, 'failed': failed}})\n    except Exception as e:\n        error_time\
      \ = datetime.now(timezone.utc).isoformat()\n        log_error(f\"Error updating\
      \ broker stats: {e}\", icon_type='STATS', exc_info=True)\n        async with\
      \ stats_lock:\n            app_stats[\"last_error\"] = f\"Broker Stats Update\
      \ Failed: {error_time}\"\n            app_stats[\"last_error_timestamp\"] =\
      \ error_time\n\n# --- Tortoise ORM Setup ---\nasync def init_tortoise():\n \
      \   \"\"\"Initialize Tortoise ORM connection and generate schemas.\"\"\"\n \
      \   log_info(f\"\U0001F4BE Configuring Tortoise ORM for SQLite: {settings.DATABASE_URL}\"\
      , icon_type='DB')\n    try:\n        await Tortoise.init(\n            db_url=settings.DATABASE_URL,\n\
      \            modules={'models': ['__main__']} # Looks for models in the current\
      \ script (__name__ == '__main__')\n        )\n        log_info(\"\U0001F4BE\
      \ Generating DB schemas if necessary...\", icon_type='DB')\n        await Tortoise.generate_schemas(safe=True)\
      \ # Creates tables if they don't exist, doesn't alter existing ones dangerously\n\
      \        log_success(\"\U0001F4BE ORM tables verified/created successfully.\"\
      , icon_type='DB')\n        await update_broker_stats() # Initial stats population\
      \ after DB is ready\n    except Exception as e:\n        log_critical(f\"Fatal:\
      \ Failed to initialize Tortoise ORM: {e}\", icon_type='CRITICAL', exc_info=True)\n\
      \        sys.exit(1) # Cannot run without DB\n\n# --- Pydantic Models (Data\
      \ Validation & Serialization) ---\n# Use ConfigDict for Pydantic V2+\n# Enable\
      \ population_by_field_name to allow using field names or aliases\n# Enable from_attributes=True\
      \ to allow creating models from ORM objects\nPYDANTIC_CONFIG = ConfigDict(populate_by_name=True,\
      \ from_attributes=True)\n\nclass QueueBase(BaseModel):\n    name: str = Field(...,\
      \ min_length=1, max_length=255, pattern=r\"^[a-zA-Z0-9_-]+$\",\n           \
      \           description=\"Unique queue name (alphanumeric, underscore, hyphen).\"\
      )\n\nclass QueueCreatePayload(QueueBase): pass # Alias for input\n\nclass QueueResponse(QueueBase):\n\
      \    id: int\n    created_at: datetime\n    updated_at: datetime\n    message_count:\
      \ int = Field(default=0, description=\"Current number of messages in the queue\"\
      )\n    model_config = PYDANTIC_CONFIG\n\nclass MessageBase(BaseModel):\n   \
      \ content: str = Field(..., min_length=1, description=\"The content/payload\
      \ of the message (arbitrary string)\")\n\nclass MessagePayload(MessageBase):\
      \ pass # Alias for input\n\nclass MessageResponse(MessageBase):\n    id: int\n\
      \    queue_id: int\n    status: str = Field(description=\"Current status: pending,\
      \ processing, processed, failed\")\n    created_at: datetime\n    updated_at:\
      \ datetime\n    model_config = PYDANTIC_CONFIG\n\nclass MessagePublishResponse(BaseModel):\n\
      \    message: str = Field(default=\"Message published successfully\")\n    message_id:\
      \ int\n    model_config = PYDANTIC_CONFIG\n\nclass MessageConsumeResponse(BaseModel):\n\
      \    message_id: int\n    queue: str # Queue name included for context\n   \
      \ content: str\n    status: str = Field(default='processing', description=\"\
      Should always be 'processing' on successful consume\")\n    retrieved_at: datetime\
      \ # Timestamp when the message status changed to 'processing'\n    model_config\
      \ = PYDANTIC_CONFIG\n\nclass Token(BaseModel):\n    access_token: str\n    refresh_token:\
      \ str\n    token_type: str = \"bearer\"\n    model_config = PYDANTIC_CONFIG\n\
      \nclass StatsResponse(BaseModel):\n    start_time: str\n    uptime_seconds:\
      \ float\n    uptime_human: str\n    requests_total: int\n    requests_by_route:\
      \ Dict[str, Dict[str, int]]\n    requests_by_status: Dict[str, int]\n    queues_total:\
      \ int\n    messages_total: int\n    messages_pending: int\n    messages_processing:\
      \ int\n    messages_processed: int\n    messages_failed: int\n    last_error:\
      \ Optional[str]\n    last_error_timestamp: Optional[datetime]\n    system: Dict[str,\
      \ Any]\n    broker_specific: Dict[str, str]\n    model_config = PYDANTIC_CONFIG\n\
      \nclass LogFileResponse(BaseModel):\n    log_files: List[str]\n    model_config\
      \ = PYDANTIC_CONFIG\n\n# --- Tortoise ORM Models ---\nclass Queue(models.Model):\n\
      \    id = fields.IntField(pk=True)\n    # Ensure name constraints match Pydantic\
      \ validation\n    name = fields.CharField(max_length=255, unique=True, index=True,\
      \ description=\"Unique queue name\")\n    created_at = fields.DatetimeField(auto_now_add=True)\n\
      \    updated_at = fields.DatetimeField(auto_now=True)\n\n    # Reverse relation:\
      \ allows accessing messages from a queue instance (queue.messages)\n    messages:\
      \ fields.ReverseRelation[\"Message\"]\n\n    class Meta:\n        table = \"\
      queues\"\n        ordering = [\"name\"] # Default ordering when querying multiple\
      \ queues\n\n    def __str__(self):\n        return self.name\n\nclass Message(models.Model):\n\
      \    id = fields.IntField(pk=True)\n    # Foreign key relation: defines the\
      \ many-to-one relationship (many messages to one queue)\n    queue: fields.ForeignKeyRelation[Queue]\
      \ = fields.ForeignKeyField(\n        'models.Queue', related_name='messages',\
      \ on_delete=fields.CASCADE, # Cascade delete: deleting a queue deletes its messages\n\
      \        description=\"The queue this message belongs to\"\n    )\n    content\
      \ = fields.TextField(description=\"Message payload\")\n    # Define allowed\
      \ status values explicitly if possible, or use validation elsewhere\n    status\
      \ = fields.CharField(max_length=20, default='pending', index=True,\n       \
      \                       description=\"Status: pending, processing, processed,\
      \ failed\")\n    created_at = fields.DatetimeField(auto_now_add=True, index=True)\
      \ # Index for ordering/filtering\n    updated_at = fields.DatetimeField(auto_now=True)\n\
      \n    class Meta:\n        table = \"messages\"\n        # Compound index useful\
      \ for finding the oldest pending message in a queue efficiently\n        indexes\
      \ = [(\"queue_id\", \"status\", \"created_at\")]\n        ordering = [\"created_at\"\
      ] # Default ordering for messages\n\n    def __str__(self):\n        return\
      \ f\"Message {self.id} (Queue: {self.queue_id}, Status: {self.status})\"\n\n\
      # --- Lifespan Context Manager (Startup/Shutdown Events) ---\n@asynccontextmanager\n\
      async def lifespan(app_ref: FastAPI): # Renamed arg to avoid shadowing 'app'\n\
      \    # Startup Sequence\n    log_info(\"\U0001F680 Application Startup Initiated...\"\
      , icon_type='STARTUP')\n    await init_tortoise()\n    # Note: Middleware (CORS,\
      \ Rate Limiter, Stats) is added *after* FastAPI() instance creation\n    # but\
      \ *before* routes are defined or the server starts running requests.\n    log_success(\"\
      \U0001F680 Application Startup Complete. Ready to accept connections.\", icon_type='STARTUP')\n\
      \    yield # Application runs here\n    # Shutdown Sequence\n    log_info(\"\
      \U0001F6D1 Application Shutdown Initiated...\", icon_type='SHUTDOWN')\n    try:\n\
      \        log_info(\"\U0001F4BE Closing database connections...\", icon_type='DB')\n\
      \        await Tortoise.close_connections()\n        log_success(\"\U0001F4BE\
      \ Database connections closed gracefully.\", icon_type='DB')\n    except Exception\
      \ as e:\n         log_warning(f\"⚠️ Error closing Tortoise connections during\
      \ shutdown: {e}\", icon_type='DB')\n    log_success(\"\U0001F6D1 Application\
      \ Shutdown Complete.\", icon_type='SHUTDOWN')\n\n\n# --- FastAPI Application\
      \ Setup ---\nlog_info(f\"\U0001F680 Initializing FastAPI Application ({settings.PROJECT_NAME}\
      \ v{settings.VERSION})...\", icon_type='SETUP')\napp = FastAPI(\n    title=settings.PROJECT_NAME,\
      \ version=settings.VERSION,\n    description=\"Asynchronous Message Broker API\
      \ using FastAPI, Tortoise ORM, and SQLite.\",\n    lifespan=lifespan, # Assign\
      \ the lifespan manager for startup/shutdown events\n    docs_url=\"/docs\",\
      \ # URL for Swagger UI\n    redoc_url=\"/redoc\", # URL for ReDoc documentation\n\
      \    openapi_tags=[ # Define tags for organizing endpoints in docs\n       \
      \ {\"name\": \"General\", \"description\": \"Basic health and info endpoints\"\
      },\n        {\"name\": \"Authentication\", \"description\": \"User login and\
      \ token management (JWT)\"},\n        {\"name\": \"Monitoring\", \"description\"\
      : \"System statistics and log viewing\"},\n        {\"name\": \"Queues\", \"\
      description\": \"Operations for managing message queues\"},\n        {\"name\"\
      : \"Messages\", \"description\": \"Publishing, consuming, and managing messages\"\
      },\n        {\"name\": \"GraphQL\", \"description\": \"GraphQL API endpoint\
      \ (alternative to REST)\"},\n    ]\n)\n\n# --- Rate Limiter Setup (Must be added\
      \ BEFORE routes are hit) ---\nlimiter = Limiter(key_func=get_remote_address,\
      \ default_limits=[settings.DEFAULT_RATE_LIMIT])\napp.state.limiter = limiter\
      \ # Make limiter accessible via app state if needed elsewhere\napp.add_middleware(SlowAPIMiddleware)\
      \ # Add the middleware to process requests\napp.add_exception_handler(RateLimitExceeded,\
      \ _rate_limit_exceeded_handler) # Handle rate limit exceeded errors globally\n\
      log_info(f\"⏱️ Rate Limiter configured (Default: {settings.DEFAULT_RATE_LIMIT},\
      \ High Traffic: {settings.HIGH_TRAFFIC_RATE_LIMIT}).\", icon_type='RATELIMIT')\n\
      \n# --- CORS Middleware ---\n# IMPORTANT: Configure origins explicitly for production\
      \ security.\napp.add_middleware(\n    CORSMiddleware, allow_origins=settings.ALLOWED_ORIGINS,\
      \ allow_credentials=True,\n    allow_methods=[\"*\"], # Allow all standard methods\n\
      \    allow_headers=[\"*\"], # Allow all standard headers\n)\nlog_info(f\"\U0001F6E1\
      ️ CORS configured for origins: {settings.ALLOWED_ORIGINS}\", icon_type='SEC')\n\
      \n# --- Authentication Setup & Dependencies ---\n# Token URL points to the /login\
      \ endpoint\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"login\", auto_error=False)\
      \ # auto_error=False allows checking token existence manually in dependencies\n\
      bearer_scheme = HTTPBearer(auto_error=False) # For validating refresh tokens\
      \ in Authorization header\n\nasync def create_jwt_token(data: dict, expires_delta:\
      \ timedelta) -> str:\n    \"\"\"Creates a JWT token with expiry.\"\"\"\n   \
      \ to_encode = data.copy()\n    issue_time = datetime.now(timezone.utc)\n   \
      \ expire = issue_time + expires_delta\n    to_encode.update({\"exp\": expire,\
      \ \"iat\": issue_time})\n    # Add 'iss' (issuer) and 'aud' (audience) claims\
      \ for better practice if needed\n    # to_encode.update({\"iss\": \"MyBrokerApp\"\
      , \"aud\": \"BrokerClients\"})\n    encoded_jwt = jwt.encode(to_encode, settings.JWT_SECRET_KEY,\
      \ algorithm=settings.ALGORITHM)\n    return encoded_jwt\n\nasync def create_access_token(username:\
      \ str) -> str:\n    \"\"\"Creates an access token (short-lived).\"\"\"\n   \
      \ return await create_jwt_token(\n        {\"sub\": username, \"type\": \"access\"\
      },\n        timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n    )\n\
      \nasync def create_refresh_token(username: str) -> str:\n    \"\"\"Creates a\
      \ refresh token (long-lived).\"\"\"\n    return await create_jwt_token(\n  \
      \      {\"sub\": username, \"type\": \"refresh\"},\n        timedelta(days=settings.REFRESH_TOKEN_EXPIRE_DAYS)\n\
      \    )\n\nasync def _decode_token(token: str, expected_type: str) -> str:\n\
      \    \"\"\"\n    Decodes and validates a JWT token.\n    Returns the username\
      \ (subject) if valid and matches expected type.\n    Raises HTTPException 401\
      \ otherwise.\n    \"\"\"\n    credentials_exception = HTTPException(\n     \
      \   status_code=status.HTTP_401_UNAUTHORIZED, detail=f\"Could not validate {expected_type}\
      \ token\",\n        headers={\"WWW-Authenticate\": f\"Bearer error=\\\"invalid_token\\\
      \", error_description=\\\"Invalid {expected_type} token\\\"\"},\n    )\n   \
      \ if not token:\n        log_warning(f\"Token decode attempt failed: No token\
      \ provided (expected type: {expected_type}).\", icon_type='AUTH')\n        raise\
      \ credentials_exception\n\n    try:\n        payload = jwt.decode(\n       \
      \     token,\n            settings.JWT_SECRET_KEY,\n            algorithms=[settings.ALGORITHM],\n\
      \            # Specify audience if you set it during encoding: options={\"verify_aud\"\
      : True}, audience=\"BrokerClients\"\n            options={\"verify_aud\": False}\
      \ # Disable audience verification if not used\n        )\n        username:\
      \ Optional[str] = payload.get(\"sub\")\n        token_type: Optional[str] =\
      \ payload.get(\"type\")\n\n        if username is None or token_type != expected_type:\n\
      \            log_warning(f\"{expected_type.capitalize()} token validation failed:\
      \ 'sub' missing or type mismatch (Got '{token_type}', Expected '{expected_type}').\"\
      , icon_type='AUTH', extra={\"payload_keys\": list(payload.keys())})\n      \
      \      raise credentials_exception\n\n        # --- Optional: Add further checks\
      \ ---\n        # E.g., check if the user exists in a user database and is active\n\
      \        # user = await get_user_from_db(username) # Hypothetical function\n\
      \        # if not user or not user.is_active:\n        #     log_warning(f\"\
      Authentication failed for '{username}': User not found or inactive.\", icon_type='AUTH')\n\
      \        #     raise credentials_exception\n        # --- End Optional Checks\
      \ ---\n\n        # If all checks pass, return the username\n        return username\n\
      \n    except JWTError as e:\n        # Handles expired tokens, invalid signatures,\
      \ etc.\n        log_warning(f\"{expected_type.capitalize()} token validation\
      \ JWTError: {e}\", icon_type='AUTH', extra={\"token_preview\": token[:10] +\
      \ \"...\"})\n        # Provide more specific error detail if possible\n    \
      \    detail = f\"Invalid {expected_type} token: {e}\"\n        if \"expired\"\
      \ in str(e).lower():\n             detail = f\"{expected_type.capitalize()}\
      \ token has expired\"\n             credentials_exception.headers[\"WWW-Authenticate\"\
      ] = f\"Bearer error=\\\"invalid_token\\\", error_description=\\\"Token expired\\\
      \"\"\n        credentials_exception.detail = detail\n        raise credentials_exception\n\
      \    except Exception as e:\n         # Catch unexpected errors during decoding\n\
      \         log_error(f\"Unexpected error during {expected_type} token decode:\
      \ {e}\", icon_type='AUTH', exc_info=True)\n         # Raise a generic 401 to\
      \ avoid leaking internal details\n         raise credentials_exception\n\nasync\
      \ def get_current_user(token: Optional[str] = Depends(oauth2_scheme)) -> str:\n\
      \    \"\"\"Dependency to get the current user from the access token in Authorization\
      \ header.\"\"\"\n    if token is None:\n        # This case happens if Authorization\
      \ header is missing or not using Bearer scheme\n        raise HTTPException(\n\
      \            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"\
      Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"\
      } # Signal that Bearer token is expected\n        )\n    # Decode and validate\
      \ the access token\n    return await _decode_token(token, \"access\")\n\nasync\
      \ def validate_refresh_token(credentials: Optional[HTTPAuthorizationCredentials]\
      \ = Depends(bearer_scheme)) -> str:\n    \"\"\"Dependency to validate a refresh\
      \ token passed in the Authorization: Bearer header.\"\"\"\n    if credentials\
      \ is None:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n\
      \            detail=\"Refresh token missing or invalid Authorization header\
      \ format\",\n            headers={\"WWW-Authenticate\": \"Bearer error=\\\"\
      invalid_request\\\", error_description=\\\"Refresh token required\\\"\"}\n \
      \       )\n    # Decode and validate the refresh token\n    return await _decode_token(credentials.credentials,\
      \ \"refresh\")\n\n# --- Middleware for Stats Update ---\n@app.middleware(\"\
      http\")\nasync def update_stats_middleware(request: Request, call_next):\n \
      \   \"\"\"Middleware to update request stats and add X-Process-Time header.\"\
      \"\"\n    start_time_mw = time.perf_counter()\n    try:\n        response =\
      \ await call_next(request)\n    except Exception as e:\n         # If an exception\
      \ occurs deeper in the stack, ensure we still log it and re-raise\n        \
      \ # This helps catch errors not handled by specific exception handlers\n   \
      \      process_time_mw = time.perf_counter() - start_time_mw\n         log_error(f\"\
      Unhandled exception during request processing ({request.method} {request.url.path})\
      \ after {process_time_mw:.4f}s: {e}\", exc_info=True)\n         # Let the global\
      \ exception handler create the final 500 response\n         raise e\n    finally:\n\
      \        # This block runs even if an exception occurred\n        process_time_mw\
      \ = time.perf_counter() - start_time_mw\n\n    # Add process time header regardless\
      \ of response status\n    try:\n       response.headers[\"X-Process-Time\"]\
      \ = f\"{process_time_mw:.4f}s\"\n    except NameError: # response might not\
      \ be defined if exception happened very early\n       pass\n\n\n    # Update\
      \ stats only for relevant API routes (exclude docs, logs, etc.)\n    route =\
      \ request.scope.get(\"route\")\n    if route and hasattr(route, 'path'):\n \
      \       route_template = route.path\n        # Define prefixes/paths to ignore\
      \ for request counting\n        # Adjusted to avoid counting stats/logs requests\
      \ themselves\n        ignored_prefixes = ('/docs', '/redoc', '/openapi.json',\
      \ '/logs', '/graphql', '/favicon.ico', '/stats', '/')\n        ignored_paths\
      \ = ('/',) # Add specific paths if needed\n\n        if route_template and \\\
      \n           not any(route_template.startswith(prefix) for prefix in ignored_prefixes)\
      \ and \\\n           route_template not in ignored_paths:\n            try:\n\
      \                await update_request_stats(route_template, request.method,\
      \ response.status_code)\n            except Exception as stats_e:\n        \
      \         # Log error if stats update fails, but don't fail the request\n  \
      \               log_error(f\"Failed to update request stats: {stats_e}\", icon_type='STATS',\
      \ exc_info=True)\n\n    return response\n\n# --- Helper Function for DB Lookups\
      \ with 404 ---\nasync def _get_queue_or_404(queue_name: str, conn=None) -> Queue:\n\
      \    \"\"\"Helper to fetch a Queue by name or raise HTTPException 404. Allows\
      \ using an existing transaction connection.\"\"\"\n    try:\n        query =\
      \ Queue.all()\n        if conn: # If a transaction connection is provided, use\
      \ it\n            query = query.using_connection(conn)\n        # Fetch the\
      \ queue by name\n        queue = await query.get(name=queue_name)\n        return\
      \ queue\n    except DoesNotExist:\n        log_warning(f\"Queue '{queue_name}'\
      \ not found in database.\", icon_type='DB')\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,\
      \ detail=f\"Queue '{queue_name}' not found\")\n    except Exception as e:\n\
      \        # Log unexpected DB errors\n        log_error(f\"Database error fetching\
      \ queue '{queue_name}': {e}\", icon_type='DB', exc_info=True)\n        # Raise\
      \ a generic 500 for unexpected DB issues\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Database error retrieving queue\")\n\n# --- API ROUTE DEFINITIONS\
      \ ---\n\n# --- General Endpoints ---\n@app.get(\"/\", tags=[\"General\"], summary=\"\
      Health Check\")\n@limiter.limit(\"5/second\") # Lower limit for basic check\n\
      async def index(request: Request):\n    \"\"\"Provides a basic health check,\
      \ server information, and current timestamp.\"\"\"\n    client_host = request.client.host\
      \ if request.client else 'N/A'\n    log_info(\"\U0001F310 GET / request\", icon_type='HTTP',\
      \ extra={\"client\": client_host})\n    return {\n        \"message\": f\"Welcome\
      \ to {settings.PROJECT_NAME}\",\n        \"status\": \"ok\",\n        \"version\"\
      : settings.VERSION,\n        \"timestamp\": datetime.now(timezone.utc).isoformat()\n\
      \    }\n\n# --- Authentication Endpoints ---\n@app.post(\"/login\", response_model=Token,\
      \ tags=[\"Authentication\"], summary=\"User Login\")\n@limiter.limit(\"10/minute\"\
      ) # Limit login attempts to mitigate brute-force\nasync def login_for_access_token(request:\
      \ Request, form_data: OAuth2PasswordRequestForm = Depends()):\n    \"\"\"\n\
      \    Handles user login using standard OAuth2 password flow.\n    **WARNING:\
      \ Uses hardcoded 'admin'/'admin' credentials. Replace with secure authentication.**\n\
      \    \"\"\"\n    client_host = request.client.host if request.client else 'N/A'\n\
      \    log_info(f\"\U0001F511 POST /login attempt for user: '{form_data.username}'\"\
      , icon_type='AUTH', extra={\"client\": client_host})\n\n    # --- !!! VERY IMPORTANT:\
      \ Replace this placeholder with secure password verification !!! ---\n    #\
      \ Example using passlib (install with `pip install passlib[bcrypt]`)\n    #\
      \ from passlib.context import CryptContext\n    # pwd_context = CryptContext(schemes=[\"\
      bcrypt\"], deprecated=\"auto\")\n    # async def verify_password(plain_password,\
      \ hashed_password):\n    #     return pwd_context.verify(plain_password, hashed_password)\n\
      \    # # Replace with actual user lookup and password hash retrieval from DB\n\
      \    # user = await get_user_from_db(form_data.username) # Hypothetical DB function\n\
      \    # if not user or not await verify_password(form_data.password, user.hashed_password):\n\
      \    #     log_warning(f\"Login failed for '{form_data.username}': Invalid credentials.\"\
      , icon_type='AUTH')\n    #     raise HTTPException(\n    #         status_code=status.HTTP_401_UNAUTHORIZED,\
      \ detail=\"Incorrect username or password\",\n    #         headers={\"WWW-Authenticate\"\
      : \"Bearer error=\\\"invalid_grant\\\"\"}, )\n    # --- !!! End of Security\
      \ Placeholder !!! ---\n\n    # --- Hardcoded credentials (FOR DEVELOPMENT/DEMO\
      \ ONLY) ---\n    if form_data.username == 'admin' and form_data.password ==\
      \ 'admin':\n        log_warning(\"\U0001F6A8 Using hardcoded 'admin'/'admin'\
      \ credentials for login. This is insecure!\", icon_type='AUTH')\n        # Credentials\
      \ match, generate tokens\n        access_token = await create_access_token(username=form_data.username)\n\
      \        refresh_token = await create_refresh_token(username=form_data.username)\n\
      \        log_success(f\"Tokens generated for '{form_data.username}'.\", icon_type='AUTH')\n\
      \        return Token(access_token=access_token, refresh_token=refresh_token)\n\
      \    else:\n        # Credentials do not match\n        log_warning(f\"Login\
      \ failed for '{form_data.username}': Invalid credentials.\", icon_type='AUTH')\n\
      \        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n\
      \            detail=\"Incorrect username or password\",\n            headers={\"\
      WWW-Authenticate\": \"Bearer error=\\\"invalid_grant\\\"\"}, # Standard OAuth2\
      \ error\n        )\n\n@app.post(\"/refresh\", response_model=Token, tags=[\"\
      Authentication\"], summary=\"Refresh Access Token\")\n@limiter.limit(\"20/minute\"\
      ) # Allow slightly more refreshes than logins\nasync def refresh_access_token(request:\
      \ Request, username: str = Depends(validate_refresh_token)):\n    \"\"\"\n \
      \   Issues a new access and refresh token using a valid refresh token provided\
      \ in the\n    `Authorization: Bearer <refresh_token>` header.\n    \"\"\"\n\
      \    client_host = request.client.host if request.client else 'N/A'\n    log_info(f\"\
      \U0001F511 POST /refresh request validated for user '{username}'\", icon_type='AUTH',\
      \ extra={\"client\": client_host})\n    # If validate_refresh_token dependency\
      \ succeeded, the username is valid\n    new_access_token = await create_access_token(username=username)\n\
      \    new_refresh_token = await create_refresh_token(username=username) # Issue\
      \ a new refresh token as well (optional, but good practice)\n    log_success(f\"\
      New access/refresh tokens generated for '{username}' via refresh.\", icon_type='AUTH')\n\
      \    return Token(access_token=new_access_token, refresh_token=new_refresh_token)\n\
      \n# --- Monitoring Endpoints ---\n@app.get(\"/stats\", response_model=StatsResponse,\
      \ tags=[\"Monitoring\"], summary=\"Get System Statistics\")\n@limiter.limit(\"\
      30/minute\") # Moderate limit for stats endpoint\nasync def get_stats(request:\
      \ Request, current_user: str = Depends(get_current_user)) -> StatsResponse:\n\
      \    \"\"\"Returns current application, system, and message broker statistics.\
      \ Requires authentication.\"\"\"\n    client_host = request.client.host if request.client\
      \ else 'N/A'\n    log_info(f\"\U0001F4CA GET /stats request by user '{current_user}'\"\
      , icon_type='STATS', extra={\"client\": client_host})\n\n    # Ensure broker\
      \ stats are reasonably fresh before returning\n    await update_broker_stats()\n\
      \n    # Collect system metrics using psutil in a non-blocking way\n    system_metrics\
      \ = {}\n    try:\n        process = psutil.Process(os.getpid())\n        # Define\
      \ the synchronous function to be run in a thread\n        def _get_psutil_data_sync():\n\
      \            # Use non-blocking calls or short intervals where possible\n  \
      \          mem_info = process.memory_info()\n            # Get CPU percents\
      \ (can be slightly blocking, use short interval)\n            proc_cpu = process.cpu_percent(interval=0.05)\
      \ # Very short interval\n            sys_cpu = psutil.cpu_percent(interval=0.05)\n\
      \            virt_mem = psutil.virtual_memory()\n\n            # Disk usage\
      \ (can be slow, especially on network drives)\n            disk_usage_data =\
      \ {}\n            try:\n                # Get only physical partitions (all=False)\
      \ to potentially speed up\n                partitions = psutil.disk_partitions(all=False)\n\
      \            except Exception as disk_e:\n                log_warning(f\"Could\
      \ not get disk partitions: {disk_e}\", icon_type='STATS')\n                partitions\
      \ = []\n\n            for part in partitions:\n                # Filter out\
      \ unwanted types and potentially problematic mounts\n                unwanted_fstypes\
      \ = ['squashfs', 'tmpfs', 'devtmpfs', 'fuse.gvfsd-fuse', 'overlay', 'autofs']\n\
      \                if 'loop' in part.device or 'snap' in part.device or part.fstype\
      \ in unwanted_fstypes:\n                    continue\n                try:\n\
      \                    # Check mountpoint exists before getting usage\n      \
      \              if os.path.exists(part.mountpoint):\n                       usage\
      \ = psutil.disk_usage(part.mountpoint)\n                       disk_usage_data[part.mountpoint]\
      \ = {\n                           \"total_gb\": round(usage.total / (1024**3),\
      \ 2),\n                           \"used_gb\": round(usage.used / (1024**3),\
      \ 2),\n                           \"free_gb\": round(usage.free / (1024**3),\
      \ 2),\n                           \"percent\": usage.percent\n             \
      \          }\n                except (FileNotFoundError, PermissionError, OSError)\
      \ as part_e:\n                    # Log errors for specific partitions but continue\n\
      \                    log_warning(f\"Could not get disk usage for {getattr(part,\
      \ 'mountpoint', 'N/A')}: {part_e}\", icon_type='STATS')\n\n            # Load\
      \ average (only available on Unix-like systems)\n            load_avg = os.getloadavg()\
      \ if hasattr(os, 'getloadavg') else \"N/A\"\n\n            return {\n      \
      \          \"cpu_percent\": sys_cpu,\n                \"memory_total_gb\": round(virt_mem.total\
      \ / (1024**3), 2),\n                \"memory_available_gb\": round(virt_mem.available\
      \ / (1024**3), 2),\n                \"memory_used_gb\": round(virt_mem.used\
      \ / (1024**3), 2),\n                \"memory_percent\": virt_mem.percent,\n\
      \                \"disk_usage\": disk_usage_data or {\"info\": \"No valid partitions\
      \ found or error reading usage.\"},\n                \"process_memory_rss_mb\"\
      : round(mem_info.rss / (1024**2), 2), # Resident Set Size\n                \"\
      process_memory_vms_mb\": round(mem_info.vms / (1024**2), 2), # Virtual Memory\
      \ Size\n                \"process_cpu_percent\": proc_cpu,\n               \
      \ \"load_average\": load_avg,\n                \"cpu_count_logical\": psutil.cpu_count(logical=True),\n\
      \                \"cpu_count_physical\": psutil.cpu_count(logical=False),\n\
      \                \"open_file_descriptors\": len(process.open_files()) if hasattr(process,\
      \ 'open_files') else \"N/A\",\n                \"thread_count\": process.num_threads()\
      \ if hasattr(process, 'num_threads') else \"N/A\",\n            }\n        #\
      \ Run the synchronous function in a separate thread\n        system_metrics\
      \ = await asyncio.to_thread(_get_psutil_data_sync)\n\n    except ImportError:\n\
      \        system_metrics[\"error\"] = \"psutil package not installed. Cannot\
      \ provide detailed system metrics.\"\n        log_warning(\"psutil package not\
      \ found. Install with `pip install psutil` for detailed system stats.\", icon_type='STATS')\n\
      \    except Exception as e:\n        log_warning(f\"Error collecting system\
      \ stats with psutil: {e}\", icon_type='STATS', exc_info=True)\n        system_metrics[\"\
      error\"] = f\"psutil data collection failed: {type(e).__name__}\"\n\n    # Prepare\
      \ response data safely using the lock\n    response_data = {}\n    async with\
      \ stats_lock:\n        # Create a copy to avoid modifying the global state directly\
      \ while processing\n        current_stats_copy = app_stats.copy()\n        #\
      \ Merge collected system metrics into the system info dictionary\n        current_stats_copy[\"\
      system\"] = {**current_stats_copy[\"system\"], **system_metrics}\n\n       \
      \ # Calculate uptime\n        start_time_dt = current_stats_copy[\"start_time\"\
      ]\n        uptime_delta = datetime.now(timezone.utc) - start_time_dt\n     \
      \   uptime_seconds = uptime_delta.total_seconds()\n        current_stats_copy[\"\
      uptime_seconds\"] = round(uptime_seconds, 2)\n\n        # Format uptime into\
      \ human-readable string (e.g., \"2d 3h 15m 30s\")\n        days, rem = divmod(int(uptime_seconds),\
      \ 86400)\n        hours, rem = divmod(rem, 3600)\n        minutes, seconds =\
      \ divmod(rem, 60)\n        parts = []\n        if days: parts.append(f\"{days}d\"\
      )\n        if hours: parts.append(f\"{hours}h\")\n        if minutes: parts.append(f\"\
      {minutes}m\")\n        if seconds or not parts: parts.append(f\"{seconds}s\"\
      ) # Always show seconds if other parts are zero\n        current_stats_copy[\"\
      uptime_human\"] = \" \".join(parts)\n\n        # Format start time and potential\
      \ error timestamp as ISO strings\n        current_stats_copy[\"start_time\"\
      ] = start_time_dt.isoformat()\n        if current_stats_copy.get(\"last_error_timestamp\"\
      ):\n             current_stats_copy[\"last_error_timestamp\"] = current_stats_copy[\"\
      last_error_timestamp\"].isoformat()\n\n        response_data = current_stats_copy\n\
      \n    log_success(f\"Stats returned for user '{current_user}'.\", icon_type='STATS')\n\
      \    # Validate the final structure against the Pydantic model before returning\n\
      \    try:\n        return StatsResponse.model_validate(response_data)\n    except\
      \ ValidationError as e:\n        log_critical(f\"Stats data failed Pydantic\
      \ validation: {e.errors()}\", icon_type='CRITICAL', extra={\"invalid_stats_data\"\
      : response_data})\n        # Return 500 if the generated stats don't match the\
      \ model schema\n        raise HTTPException(status_code=500, detail=\"Internal\
      \ Server Error: Failed to generate valid stats data.\")\n\n\n@app.get(\"/logs\"\
      , response_model=LogFileResponse, tags=[\"Monitoring\"], summary=\"List Log\
      \ Files\")\n@limiter.limit(\"10/minute\") # Less frequent access generally needed\n\
      async def list_log_files(request: Request, current_user: str = Depends(get_current_user)):\n\
      \    \"\"\"Lists available JSON log files found in the configured log directory.\
      \ Requires authentication.\"\"\"\n    client_host = request.client.host if request.client\
      \ else 'N/A'\n    log_info(f\"\U0001F4C4 GET /logs request by user '{current_user}'\"\
      , icon_type='LOGS', extra={\"client\": client_host})\n    try:\n        # Use\
      \ asyncio.to_thread for potentially blocking os.listdir\n        def list_dir_sync():\n\
      \            return os.listdir(settings.LOG_DIR)\n\n        log_files_all =\
      \ await asyncio.to_thread(list_dir_sync)\n\n        # Filter for .json files\
      \ and sort newest first (based on filename convention)\n        log_files_json\
      \ = sorted(\n            [f for f in log_files_all if f.endswith('.json') and\
      \ os.path.isfile(os.path.join(settings.LOG_DIR, f))],\n            reverse=True\
      \ # Assumes YYYYMMDD_HHMMSS prefix for sorting\n        )\n        log_success(f\"\
      Found {len(log_files_json)} JSON log files in '{settings.LOG_DIR}'.\", icon_type='LOGS')\n\
      \        return LogFileResponse(log_files=log_files_json)\n    except FileNotFoundError:\n\
      \        log_error(f\"Log directory '{settings.LOG_DIR}' configured but not\
      \ found.\", icon_type='LOGS')\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,\
      \ detail=\"Log directory not found on server\")\n    except OSError as e:\n\
      \        log_error(f\"Error listing log files in '{settings.LOG_DIR}': {e}\"\
      , exc_info=True, icon_type='LOGS')\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error accessing log directory\")\n\n@app.get(\"/logs/{filename:path}\"\
      , response_model=List[Dict[str, Any]], tags=[\"Monitoring\"], summary=\"Get\
      \ Log File Content\")\n@limiter.limit(\"60/minute\") # Allow more frequent access\
      \ to view individual logs\nasync def get_log_file(\n    request: Request,\n\
      \    filename: str = Path(..., description=\"The name of the JSON log file to\
      \ retrieve (e.g., broker_log_YYYYMMDD_HHMMSS_hash.json).\"),\n    start: Optional[int]\
      \ = FastQuery(None, ge=1, description=\"Start reading from this line number\
      \ (1-based index).\"),\n    end: Optional[int] = FastQuery(None, ge=1, description=\"\
      Stop reading at this line number (inclusive, 1-based index).\"),\n    tail:\
      \ Optional[int] = FastQuery(None, ge=1, le=10000, description=\"Retrieve only\
      \ the last N lines (max 10000). Overrides start/end if provided.\"),\n    current_user:\
      \ str = Depends(get_current_user)\n) -> List[Dict]:\n    \"\"\"\n    Retrieves\
      \ the content of a specific log file, parsing each line as JSON.\n    Supports\
      \ retrieving ranges of lines or the last N lines (tail). Requires authentication.\n\
      \    \"\"\"\n    # --- Security: Sanitize filename ---\n    # Prevents directory\
      \ traversal (e.g., ../../etc/passwd) and ensures it's likely a log file.\n \
      \   safe_filename = secure_filename(filename)\n    if not safe_filename or safe_filename\
      \ != filename or not safe_filename.startswith('broker_log_') or not safe_filename.endswith('.json'):\n\
      \        log_warning(f\"Invalid log file access attempt: '{filename}' by user\
      \ '{current_user}'\", icon_type='SEC')\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST,\
      \ detail=\"Invalid or potentially unsafe log filename provided.\")\n\n    log_path\
      \ = os.path.join(settings.LOG_DIR, safe_filename)\n    log_info(f\"\U0001F4C4\
      \ GET /logs/{safe_filename} request by '{current_user}' (start={start}, end={end},\
      \ tail={tail})\", icon_type='LOGS')\n\n    # --- File Reading and Parsing (in\
      \ thread to avoid blocking) ---\n    def read_and_parse_log_sync() -> Optional[List[Dict]]:\n\
      \        if not os.path.isfile(log_path):\n            log_warning(f\"Log file\
      \ not found at path: {log_path}\", icon_type='LOGS')\n            return None\
      \ # Signal file not found\n\n        lines_to_process: Union[deque, List[str]]\
      \ = []\n        try:\n            with open(log_path, 'r', encoding='utf-8')\
      \ as f:\n                if tail is not None and tail > 0:\n               \
      \     # Efficiently get last 'tail' lines using deque\n                    lines_to_process\
      \ = deque(f, maxlen=tail)\n                else:\n                    # Read\
      \ line by line, applying start/end limits if present\n                    lines_to_process\
      \ = []\n                    line_count_read = 0\n                    # Line\
      \ numbers are 1-based for user input, enumerate provides 0-based\n         \
      \           for line_num_0based, line in enumerate(f):\n                   \
      \     line_num_1based = line_num_0based + 1\n                        # Skip\
      \ lines before start\n                        if start is not None and line_num_1based\
      \ < start:\n                            continue\n                        #\
      \ Stop reading lines after end\n                        if end is not None and\
      \ line_num_1based > end:\n                            break\n              \
      \          # Add the line (strip whitespace)\n                        lines_to_process.append(line.strip())\n\
      \                        line_count_read += 1\n                        # Safety\
      \ break: Limit lines read if only start is given (prevent huge reads)\n    \
      \                    if start is not None and end is None and line_count_read\
      \ >= 10000:\n                            log_warning(f\"Log read for {safe_filename}\
      \ truncated at 10000 lines due to missing 'end' parameter.\", icon_type='LOGS')\n\
      \                            lines_to_process.append(json.dumps({\"_warning\"\
      : \"Result set truncated at 10000 lines (specify 'end' for more)\", \"_limit\"\
      : 10000}))\n                            break\n        except FileNotFoundError:\
      \ # Should be caught by os.path.isfile, but handle defensively\n           \
      \  log_warning(f\"Log file disappeared during read: {log_path}\", icon_type='LOGS')\n\
      \             return None\n        except Exception as read_exc:\n         \
      \   log_error(f\"Error reading log file '{safe_filename}': {read_exc}\", exc_info=True,\
      \ icon_type='LOGS')\n            # Return a list containing just the error for\
      \ the client\n            return [{\"_error\": f\"Failed to read file: {type(read_exc).__name__}.\
      \ Check server logs for details.\"}]\n\n        # --- Parse JSON lines ---\n\
      \        parsed_lines: List[Dict[str, Any]] = []\n        for i, line in enumerate(lines_to_process):\n\
      \            if not line: continue # Skip empty lines\n\n            line_num_info\
      \ = f\"tail_{i+1}\" if tail else (start or 1) + i # Approximate original line\
      \ number for context\n            try:\n                parsed_line_data = json.loads(line)\n\
      \                if isinstance(parsed_line_data, dict): # Ensure it's a dictionary\n\
      \                     parsed_lines.append(parsed_line_data)\n              \
      \  else:\n                     # Handle cases where a line is valid JSON but\
      \ not an object (e.g., just a string)\n                     parsed_lines.append({\"\
      _warning\": \"Line parsed but is not a JSON object\", \"_line\": line_num_info,\
      \ \"_type\": type(parsed_line_data).__name__, \"_raw\": line[:250]})\n     \
      \       except json.JSONDecodeError:\n                # Include error info and\
      \ truncated raw line for debugging\n                parsed_lines.append({\"\
      _error\": \"Invalid JSON format\", \"_line\": line_num_info, \"_raw\": line[:250]\
      \ + ('...' if len(line)>250 else '')})\n            except Exception as parse_exc:\n\
      \                 # Catch other potential parsing issues\n                 parsed_lines.append({\"\
      _error\": f\"Parsing error: {parse_exc}\", \"_line\": line_num_info, \"_raw\"\
      : line[:250] + ('...' if len(line)>250 else '')})\n        return parsed_lines\n\
      \n    try:\n        # Run the synchronous file I/O and parsing in a thread\n\
      \        result_lines = await asyncio.to_thread(read_and_parse_log_sync)\n\n\
      \        if result_lines is None:\n            # File was not found by the reading\
      \ function\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,\
      \ detail=f\"Log file '{safe_filename}' not found.\")\n\n        log_success(f\"\
      {len(result_lines)} log entries returned from '{safe_filename}'.\", icon_type='LOGS')\n\
      \        return result_lines\n    except HTTPException:\n        raise # Re-raise\
      \ expected HTTP exceptions (400, 404)\n    except Exception as e:\n        log_error(f\"\
      Unexpected error processing log file '{safe_filename}': {e}\", exc_info=True,\
      \ icon_type='LOGS')\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Unexpected error processing log file. Check server logs.\")\n\n#\
      \ --- Queue Management Endpoints ---\n@app.get(\"/queues\", response_model=List[QueueResponse],\
      \ tags=[\"Queues\"], summary=\"List All Queues\")\n@limiter.limit(\"60/minute\"\
      ) # Allow fairly frequent listing\nasync def list_queues(request: Request, current_user:\
      \ str = Depends(get_current_user)) -> List[QueueResponse]:\n    \"\"\"Lists\
      \ all available message queues, including a count of messages in each. Requires\
      \ authentication.\"\"\"\n    log_info(f\"\U0001F4CB GET /queues request by user\
      \ '{current_user}'\", icon_type='QUEUE')\n    try:\n        # Fetch all queues\
      \ ordered by name\n        queues = await Queue.all().order_by('name')\n   \
      \     if not queues:\n            log_info(\"No queues found in the database.\"\
      , icon_type='QUEUE')\n            return [] # Return empty list if no queues\
      \ exist\n\n        # Fetch message counts for all queues concurrently for efficiency\n\
      \        count_tasks = {q.id: Message.filter(queue_id=q.id).count() for q in\
      \ queues}\n        # Run count queries in parallel\n        message_counts_results\
      \ = await asyncio.gather(*count_tasks.values())\n        # Create a dictionary\
      \ mapping queue_id to its count\n        counts_dict = dict(zip(count_tasks.keys(),\
      \ message_counts_results))\n\n        # Build the response list, validating\
      \ each item with the Pydantic model\n        response_list = []\n        for\
      \ q in queues:\n            try:\n                 response_item = QueueResponse(\n\
      \                     id=q.id, name=q.name, created_at=q.created_at, updated_at=q.updated_at,\n\
      \                     message_count=counts_dict.get(q.id, 0) # Get count from\
      \ dict, default 0\n                 )\n                 response_list.append(response_item)\n\
      \            except ValidationError as e:\n                 # Log if a specific\
      \ queue fails validation, but continue with others\n                 log_error(f\"\
      Queue data validation failed for queue ID {q.id} ('{q.name}'): {e.errors()}\"\
      , icon_type='QUEUE')\n\n        log_success(f\"Returned {len(response_list)}\
      \ queues.\", icon_type='QUEUE')\n        return response_list\n    except Exception\
      \ as e:\n        log_error(f\"Error listing queues: {e}\", icon_type='CRITICAL',\
      \ exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error retrieving queue list from database\")\n\n@app.post(\"/queues\"\
      , response_model=QueueResponse, status_code=status.HTTP_201_CREATED, tags=[\"\
      Queues\"], summary=\"Create New Queue\")\n@limiter.limit(\"30/minute\") # Less\
      \ frequent operation than listing\nasync def create_queue(request: Request,\
      \ payload: QueueCreatePayload, current_user: str = Depends(get_current_user))\
      \ -> QueueResponse:\n    \"\"\"\n    Creates a new message queue. The name must\
      \ be unique. Requires authentication.\n    \"\"\"\n    queue_name = payload.name\n\
      \    log_info(f\"➕ POST /queues request by '{current_user}' to create queue\
      \ '{queue_name}'\", icon_type='QUEUE')\n    try:\n        # Use Tortoise's get_or_create\
      \ for atomicity.\n        # It attempts to get the queue, and if it doesn't\
      \ exist, creates it within a transaction.\n        new_queue, created = await\
      \ Queue.get_or_create(name=queue_name)\n\n        if not created:\n        \
      \    # The queue already existed\n            log_warning(f\"Queue '{queue_name}'\
      \ already exists. Creation request denied (409).\", icon_type='QUEUE')\n   \
      \         raise HTTPException(\n                status_code=status.HTTP_409_CONFLICT,\n\
      \                detail=f\"Queue with name '{queue_name}' already exists.\"\n\
      \            )\n\n        # Queue was successfully created\n        log_success(f\"\
      ✅ Queue '{queue_name}' created successfully (ID: {new_queue.id}).\", icon_type='QUEUE')\n\
      \        # Validate the newly created queue object against the response model\n\
      \        # Manually set message_count to 0 for the response, as it's definitely\
      \ empty\n        response = QueueResponse.model_validate(new_queue)\n      \
      \  response.message_count = 0\n        return response\n\n    except IntegrityError:\
      \ # Catch potential DB unique constraint violation as a fallback\n        log_warning(f\"\
      IntegrityError during queue creation for '{queue_name}'. Likely already exists\
      \ (concurrent request?).\", icon_type='DB')\n        raise HTTPException(status_code=status.HTTP_409_CONFLICT,\
      \ detail=f\"Queue with name '{queue_name}' already exists (database constraint\
      \ violation).\")\n    except ValidationError as e: # Catch Pydantic validation\
      \ errors on the payload\n        log_warning(f\"Queue creation validation error\
      \ for '{queue_name}': {e.errors()}\", icon_type='QUEUE')\n        # Return 422\
      \ for validation errors\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\
      \ detail=e.errors())\n    except Exception as e:\n        log_error(f\"Error\
      \ creating queue '{queue_name}': {e}\", icon_type='CRITICAL', exc_info=True)\n\
      \        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Unexpected error creating queue\")\n\n@app.get(\"/queues/{queue_name}\"\
      , response_model=QueueResponse, tags=[\"Queues\"], summary=\"Get Queue Details\"\
      )\n@limiter.limit(\"60/minute\")\nasync def get_queue(request: Request, queue_name:\
      \ str = Path(..., description=\"The name of the queue to retrieve.\"), current_user:\
      \ str = Depends(get_current_user)) -> QueueResponse:\n    \"\"\"Gets details\
      \ for a specific queue by its name, including current message count. Requires\
      \ authentication.\"\"\"\n    log_info(f\"\U0001F4E5 GET /queues/{queue_name}\
      \ request by user '{current_user}'\", icon_type='QUEUE')\n    try:\n       \
      \ # Use the helper function to get the queue or raise 404\n        queue = await\
      \ _get_queue_or_404(queue_name)\n        # Get the count of messages associated\
      \ with this queue\n        message_count = await Message.filter(queue_id=queue.id).count()\n\
      \        log_success(f\"Details for queue '{queue_name}' (ID: {queue.id}) returned.\"\
      , icon_type='QUEUE')\n        # Validate the queue object and add the count\
      \ before returning\n        response = QueueResponse.model_validate(queue)\n\
      \        response.message_count = message_count\n        return response\n \
      \   except HTTPException:\n        raise # Re-raise 404 from helper or any 500\
      \ from DB errors\n    except Exception as e:\n        log_error(f\"Unexpected\
      \ error getting queue details for '{queue_name}': {e}\", icon_type='CRITICAL',\
      \ exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error retrieving queue details\")\n\n@app.delete(\"/queues/{queue_name}\"\
      , status_code=status.HTTP_204_NO_CONTENT, tags=[\"Queues\"], summary=\"Delete\
      \ Queue\")\n@limiter.limit(\"10/minute\") # Deletion should be infrequent and\
      \ controlled\nasync def delete_queue(request: Request, queue_name: str = Path(...,\
      \ description=\"The name of the queue to delete.\"), current_user: str = Depends(get_current_user))\
      \ -> Response:\n    \"\"\"\n    Deletes a queue and all its associated messages\
      \ (due to CASCADE constraint in the Message model).\n    This operation is irreversible.\
      \ Requires authentication.\n    \"\"\"\n    log_info(f\"\U0001F5D1️ DELETE /queues/{queue_name}\
      \ request by user '{current_user}'\", icon_type='QUEUE')\n    try:\n       \
      \ # Find the queue first to ensure it exists (raises 404 if not)\n        queue\
      \ = await _get_queue_or_404(queue_name)\n        queue_id = queue.id # Get ID\
      \ for logging before deletion\n        log_pipeline(f\"Queue '{queue_name}'\
      \ (ID: {queue_id}) found. Proceeding with deletion...\")\n\n        # Perform\
      \ the delete operation. Tortoise handles cascading deletes based on the model's\
      \ ForeignKey definition.\n        await queue.delete()\n\n        log_success(f\"\
      ✅ Queue '{queue_name}' (ID: {queue_id}) and associated messages deleted successfully.\"\
      , icon_type='QUEUE')\n        # Return 204 No Content on successful deletion\n\
      \        return Response(status_code=status.HTTP_204_NO_CONTENT)\n    except\
      \ HTTPException:\n        raise # Re-raise 404 from helper\n    except Exception\
      \ as e:\n        log_error(f\"Error deleting queue '{queue_name}': {e}\", icon_type='CRITICAL',\
      \ exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error deleting queue\")\n\n\n# --- Message Operations Endpoints ---\n\
      @app.post(\"/queues/{queue_name}/messages\", response_model=MessagePublishResponse,\
      \ status_code=status.HTTP_201_CREATED, tags=[\"Messages\"], summary=\"Publish\
      \ Message\")\n@limiter.limit(settings.HIGH_TRAFFIC_RATE_LIMIT) # Allow high\
      \ traffic for publishing\nasync def publish_message(\n    request: Request,\n\
      \    payload: MessagePayload, # Request body containing the message content\n\
      \    queue_name: str = Path(..., description=\"The name of the target queue.\"\
      ),\n    current_user: str = Depends(get_current_user) # Requires authentication\n\
      ) -> MessagePublishResponse:\n    \"\"\"Publishes a new message with the given\
      \ content to the specified queue. Requires authentication.\"\"\"\n    # Log\
      \ only a preview to avoid logging potentially large/sensitive content fully\n\
      \    content_preview = payload.content[:80] + ('...' if len(payload.content)\
      \ > 80 else '')\n    log_info(f\"\U0001F4E4 POST /queues/{queue_name}/messages\
      \ request by '{current_user}'\", icon_type='MSG', extra={\"content_preview\"\
      : content_preview})\n    try:\n        # Ensure the target queue exists (raises\
      \ 404 if not)\n        queue = await _get_queue_or_404(queue_name)\n       \
      \ log_pipeline(f\"Queue '{queue_name}' (ID: {queue.id}) found. Creating message...\"\
      )\n\n        # Create the message in the database with 'pending' status\n  \
      \      new_message = await Message.create(queue=queue, content=payload.content,\
      \ status='pending')\n\n        log_success(f\"✅ Message ID {new_message.id}\
      \ published to queue '{queue_name}'.\", icon_type='MSG')\n        # Optionally,\
      \ update overall broker stats in the background if performance is critical\n\
      \        # background_tasks.add_task(update_broker_stats)\n        return MessagePublishResponse(message_id=new_message.id)\n\
      \    except HTTPException:\n        raise # Re-raise 404 if queue not found\n\
      \    except ValidationError as e: # Catch Pydantic validation errors on the\
      \ payload\n        log_warning(f\"Message publish validation error to '{queue_name}':\
      \ {e.errors()}\", icon_type='MSG')\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\
      \ detail=e.errors())\n    except Exception as e:\n        log_error(f\"Error\
      \ publishing message to queue '{queue_name}': {e}\", icon_type='CRITICAL', exc_info=True)\n\
      \        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error publishing message\")\n\n\n@app.get(\"/queues/{queue_name}/messages/consume\"\
      , response_model=Optional[MessageConsumeResponse], tags=[\"Messages\"], summary=\"\
      Consume Message\")\n@limiter.limit(settings.HIGH_TRAFFIC_RATE_LIMIT) # High\
      \ traffic endpoint\nasync def consume_message(\n    request: Request,\n    queue_name:\
      \ str = Path(..., description=\"The name of the queue to consume from.\"),\n\
      \    current_user: str = Depends(get_current_user) # Requires authentication\n\
      ) -> Optional[MessageConsumeResponse]:\n    \"\"\"\n    Atomically consumes\
      \ the oldest 'pending' message from the specified queue.\n    It finds the oldest\
      \ pending message, locks it, updates its status to 'processing',\n    and returns\
      \ it. Uses a database transaction with SELECT FOR UPDATE for atomicity.\n  \
      \  Returns the message details if successful, or `null` (empty 200 OK) if the\
      \ queue is empty.\n    Requires authentication.\n    \"\"\"\n    client_host\
      \ = request.client.host if request.client else 'N/A'\n    log_info(f\"\U0001F4E9\
      \ GET /queues/{queue_name}/messages/consume request by '{current_user}'\", icon_type='MSG',\
      \ extra={\"client\": client_host})\n\n    try:\n        # 1. Ensure queue exists\
      \ (outside transaction is fine, reduces transaction duration)\n        queue\
      \ = await _get_queue_or_404(queue_name)\n\n        # 2. Start Transaction for\
      \ atomic find-and-update\n        async with in_transaction(\"default\") as\
      \ tx: # \"default\" matches the connection name in Tortoise.init\n         \
      \   # 3. Find the oldest pending message, lock it for update, using the transaction\
      \ connection\n            #    *** THIS IS THE CORRECTED QUERY CHAIN (FIX APPLIED\
      \ HERE) ***\n            message = await Message.filter(queue=queue, status='pending')\
      \ \\\n                                   .order_by('created_at') \\\n      \
      \                             .using_connection(tx) \\\n                   \
      \                .select_for_update(skip_locked=True) \\\n                 \
      \                  .first() # Get the first matching message or None\n\n   \
      \         # 4. Check if a message was found\n            if not message:\n \
      \               # Queue is empty (or all messages are locked/processed/failed)\n\
      \                log_debug(f\"Queue '{queue_name}' is empty or no pending messages\
      \ available.\", icon_type='MSG')\n                # Return None, FastAPI handles\
      \ this as a 200 OK with null body for Optional[] return type\n             \
      \   return None\n\n            # 5. Update the message status and timestamp\
      \ within the transaction\n            message.status = 'processing'\n      \
      \      message.updated_at = datetime.now(timezone.utc) # Record time it entered\
      \ processing state\n\n            # 6. Save the changes using the transaction\
      \ connection\n            #    *** ENSURE SAVE USES THE TRANSACTION ***\n  \
      \          await message.save(using_connection=tx, update_fields=['status',\
      \ 'updated_at'])\n            # Transaction commits automatically upon exiting\
      \ the 'with' block successfully\n\n        # 7. Message successfully consumed\
      \ and status updated. Log and return details.\n        log_success(f\"✅ Message\
      \ ID {message.id} consumed from queue '{queue_name}' by '{current_user}' (status\
      \ -> processing).\", icon_type='MSG')\n        # Return the consumed message\
      \ details, validated by Pydantic\n        return MessageConsumeResponse(\n \
      \           message_id=message.id,\n            queue=queue_name,\n        \
      \    content=message.content,\n            status=message.status, # Should be\
      \ 'processing'\n            retrieved_at=message.updated_at # The time it was\
      \ marked as processing\n        )\n\n    except HTTPException:\n        # Re-raise\
      \ exceptions like 404 if the queue wasn't found initially\n        raise\n \
      \   except IntegrityError as e:\n        # This might occur if there's a very\
      \ rare race condition not caught by select_for_update\n        # or other DB\
      \ constraint issues during the update.\n        log_warning(f\"DB integrity\
      \ error during consumption from '{queue_name}': {e}\", icon_type='DB')\n   \
      \     raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Database\
      \ conflict during message consumption, please try again.\")\n    except Exception\
      \ as e:\n        # Catch any other unexpected errors during the process\n  \
      \      log_error(f\"Error consuming message from queue '{queue_name}': {type(e).__name__}:\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error consuming message\")\n\n\n@app.post(\"/messages/{message_id}/ack\"\
      , status_code=status.HTTP_200_OK, response_model=Dict[str, str], tags=[\"Messages\"\
      ], summary=\"Acknowledge Message\")\n@limiter.limit(settings.HIGH_TRAFFIC_RATE_LIMIT)\
      \ # High traffic expected\nasync def acknowledge_message(\n    request: Request,\n\
      \    background_tasks: BackgroundTasks, # Can be used for background tasks after\
      \ ack\n    message_id: int = Path(..., ge=1, description=\"The ID of the message\
      \ to acknowledge.\"),\n    current_user: str = Depends(get_current_user) # Requires\
      \ authentication\n) -> Dict[str, str]:\n    \"\"\"\n    Marks a message currently\
      \ in the 'processing' state as 'processed'.\n    Uses a transaction with SELECT\
      \ FOR UPDATE to prevent race conditions.\n    Requires authentication.\n   \
      \ \"\"\"\n    log_info(f\"✅ POST /messages/{message_id}/ack request by '{current_user}'\"\
      , icon_type='MSG')\n    try:\n        # Start transaction for atomic find-and-update\n\
      \        async with in_transaction(\"default\") as tx:\n            # Find the\
      \ message by ID *only if its status is 'processing'*, lock it, using the transaction\n\
      \            # *** CORRECTED QUERY AND HANDLING ***\n            message = await\
      \ Message.filter(id=message_id, status='processing') \\\n                  \
      \                 .using_connection(tx) \\\n                               \
      \    .select_for_update() \\\n                                   .get_or_none()\
      \ # Use get_or_none to handle not found gracefully\n\n            # Check if\
      \ the message was found in the correct state\n            if not message:\n\
      \                # If not found in 'processing' state, check if it exists at\
      \ all or has a different status\n                existing_msg_status = await\
      \ Message.filter(id=message_id) \\\n                                       \
      \            .using_connection(tx) \\\n                                    \
      \               .values_list('status', flat=True) \\\n                     \
      \                              .first() # Check status without locking again\n\
      \                if existing_msg_status:\n                    # Message exists\
      \ but is not 'processing' (e.g., already acked, failed, or still pending)\n\
      \                    log_warning(f\"ACK failed for message {message_id}: Expected\
      \ status 'processing', found '{existing_msg_status}'.\", icon_type='MSG')\n\
      \                    raise HTTPException(\n                        status_code=status.HTTP_409_CONFLICT,\n\
      \                        detail=f\"Message {message_id} is in status '{existing_msg_status}',\
      \ cannot ACK. Only 'processing' messages can be acknowledged.\"\n          \
      \          )\n                else:\n                    # Message ID does not\
      \ exist in the database\n                    log_warning(f\"ACK failed: Message\
      \ {message_id} not found.\", icon_type='MSG')\n                    raise HTTPException(\n\
      \                        status_code=status.HTTP_404_NOT_FOUND,\n          \
      \              detail=f\"Message with ID {message_id} not found.\"\n       \
      \             )\n\n            # Message found and is in 'processing' state.\
      \ Update status.\n            message.status = 'processed'\n            message.updated_at\
      \ = datetime.now(timezone.utc)\n            # Save changes using the transaction\
      \ connection\n            await message.save(using_connection=tx, update_fields=['status',\
      \ 'updated_at'])\n            # Transaction commits automatically here\n\n \
      \       log_success(f\"✅ Message ID {message_id} acknowledged successfully by\
      \ '{current_user}' (status -> processed).\", icon_type='MSG')\n        # Optionally\
      \ update overall broker stats in the background\n        # background_tasks.add_task(update_broker_stats)\n\
      \        return {\"detail\": f\"Message {message_id} acknowledged successfully.\"\
      }\n\n    except HTTPException:\n        raise # Re-raise 404, 409 from checks\n\
      \    except IntegrityError as e:\n         log_warning(f\"DB integrity error\
      \ during ACK for message {message_id}: {e}\", icon_type='DB')\n         raise\
      \ HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Database conflict\
      \ during message acknowledgement.\")\n    except Exception as e:\n        log_error(f\"\
      Error acknowledging message {message_id}: {e}\", icon_type='CRITICAL', exc_info=True)\n\
      \        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=\"Error acknowledging message\")\n\n\n@app.post(\"/messages/{message_id}/nack\"\
      , status_code=status.HTTP_200_OK, response_model=Dict[str, str], tags=[\"Messages\"\
      ], summary=\"Negative Acknowledge Message\")\n@limiter.limit(settings.HIGH_TRAFFIC_RATE_LIMIT)\
      \ # High traffic expected\nasync def negative_acknowledge_message(\n    request:\
      \ Request,\n    background_tasks: BackgroundTasks, # For potential background\
      \ tasks\n    message_id: int = Path(..., ge=1, description=\"The ID of the message\
      \ to NACK.\"),\n    requeue: bool = FastQuery(False, description=\"If true,\
      \ set status back to 'pending' for reprocessing. If false, set status to 'failed'.\"\
      ),\n    current_user: str = Depends(get_current_user) # Requires authentication\n\
      ) -> Dict[str, str]:\n    \"\"\"\n    Negatively acknowledges a message currently\
      \ in the 'processing' state.\n    This typically means processing failed.\n\
      \    - If `requeue` is true, the message status is set back to 'pending' to\
      \ be consumed again later.\n    - If `requeue` is false (default), the message\
      \ status is set to 'failed'.\n    Uses a transaction with SELECT FOR UPDATE.\
      \ Requires authentication.\n    \"\"\"\n    action = \"requeued (pending)\"\
      \ if requeue else \"marked as failed\"\n    log_info(f\"❌ POST /messages/{message_id}/nack\
      \ request by '{current_user}' (requeue={requeue})\", icon_type='MSG')\n    try:\n\
      \        # Start transaction for atomic find-and-update\n        async with\
      \ in_transaction(\"default\") as tx:\n            # Find the message by ID *only\
      \ if its status is 'processing'*, lock it, using the transaction\n         \
      \   # *** CORRECTED QUERY AND HANDLING ***\n            message = await Message.filter(id=message_id,\
      \ status='processing') \\\n                                   .using_connection(tx)\
      \ \\\n                                   .select_for_update() \\\n         \
      \                          .get_or_none()\n\n            # Check if the message\
      \ was found in the correct state\n            if not message:\n            \
      \    # If not found in 'processing' state, check if it exists at all or has\
      \ a different status\n                existing_msg_status = await Message.filter(id=message_id)\
      \ \\\n                                                   .using_connection(tx)\
      \ \\\n                                                   .values_list('status',\
      \ flat=True) \\\n                                                   .first()\n\
      \                if existing_msg_status:\n                    # Message exists\
      \ but is not 'processing'\n                    log_warning(f\"NACK failed for\
      \ message {message_id}: Expected status 'processing', found '{existing_msg_status}'.\"\
      , icon_type='MSG')\n                    raise HTTPException(\n             \
      \           status_code=status.HTTP_409_CONFLICT,\n                        detail=f\"\
      Message {message_id} is in status '{existing_msg_status}', cannot NACK. Only\
      \ 'processing' messages can be negatively acknowledged.\"\n                \
      \    )\n                else:\n                    # Message ID does not exist\n\
      \                    log_warning(f\"NACK failed: Message {message_id} not found.\"\
      , icon_type='MSG')\n                    raise HTTPException(\n             \
      \           status_code=status.HTTP_404_NOT_FOUND,\n                       \
      \ detail=f\"Message with ID {message_id} not found.\"\n                    )\n\
      \n            # Message found and is 'processing'. Determine the new status\
      \ based on 'requeue' flag.\n            new_status = 'pending' if requeue else\
      \ 'failed'\n            message.status = new_status\n            message.updated_at\
      \ = datetime.now(timezone.utc)\n            # Save changes using the transaction\
      \ connection\n            await message.save(using_connection=tx, update_fields=['status',\
      \ 'updated_at'])\n            # Transaction commits automatically here\n\n \
      \       log_success(f\"✅ Message ID {message_id} NACK'd successfully by '{current_user}'\
      \ (status -> {new_status}).\", icon_type='MSG')\n        # Optionally update\
      \ stats\n        # background_tasks.add_task(update_broker_stats)\n        return\
      \ {\"detail\": f\"Message {message_id} successfully {action}.\"}\n\n    except\
      \ HTTPException:\n        raise # Re-raise 404, 409\n    except IntegrityError\
      \ as e:\n         log_warning(f\"DB integrity error during NACK for message\
      \ {message_id}: {e}\", icon_type='DB')\n         raise HTTPException(status_code=status.HTTP_409_CONFLICT,\
      \ detail=\"Database conflict during message NACK operation.\")\n    except Exception\
      \ as e:\n        log_error(f\"Error NACK'ing message {message_id} (action: {action}):\
      \ {e}\", icon_type='CRITICAL', exc_info=True)\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\
      \ detail=f\"Error negatively acknowledging message (action: {action})\")\n\n\
      # --- GraphQL Setup ---\n# This section defines the Strawberry GraphQL schema\
      \ and resolvers, providing an alternative API interface.\nlog_info(\"\U0001F353\
      \ Configuring GraphQL endpoint with Strawberry...\", icon_type='GRAPHQL')\n\n\
      # --- GraphQL Object Types (mirroring Pydantic/ORM models) ---\n@strawberry.type(description=\"\
      Represents a message in a queue\")\nclass MessageGQL:\n    id: strawberry.ID\
      \ # Use strawberry.ID for GraphQL IDs\n    queue_name: str = strawberry.field(description=\"\
      Name of the queue this message belongs to\")\n    content: str\n    status:\
      \ str\n    created_at: datetime\n    updated_at: datetime\n\n    @classmethod\n\
      \    def from_orm(cls, model: Message, queue_name_str: str) -> \"MessageGQL\"\
      :\n         \"\"\"Helper to create MessageGQL from Tortoise Message model.\"\
      \"\"\n         return cls(\n             id=strawberry.ID(str(model.id)), #\
      \ Convert int ID to string for GraphQL ID\n             queue_name=queue_name_str,\n\
      \             content=model.content,\n             status=model.status,\n  \
      \           created_at=model.created_at,\n             updated_at=model.updated_at\n\
      \         )\n\n@strawberry.type(description=\"Represents a message queue\")\n\
      class QueueGQL:\n    id: strawberry.ID\n    name: str\n    created_at: datetime\n\
      \    updated_at: datetime\n\n    @strawberry.field(description=\"Retrieves the\
      \ current count of messages in this queue\")\n    async def message_count(self,\
      \ info: Info) -> int:\n        \"\"\"Resolver for the message count field.\"\
      \"\"\n        log_pipeline(f\"GQL: Resolving message_count for Queue ID {self.id}\"\
      , icon_type='GRAPHQL')\n        try:\n            # Convert Strawberry ID back\
      \ to int for DB query\n            return await Message.filter(queue_id=int(self.id)).count()\n\
      \        except ValueError: # Handle invalid ID format\n             log_error(f\"\
      GQL message_count: Invalid ID format '{self.id}'\", icon_type='GRAPHQL')\n \
      \            return 0\n        except Exception as e:\n            log_error(f\"\
      GQL message_count resolver error for queue ID {self.id}: {e}\", exc_info=True,\
      \ icon_type='GRAPHQL')\n            return 0 # Return 0 on unexpected errors\n\
      \n    @strawberry.field(description=\"Retrieves messages belonging to this queue,\
      \ with filtering and pagination\")\n    async def messages(\n        self, info:\
      \ Info, # Strawberry passes context via info\n        status: Optional[str]\
      \ = strawberry.field(default=None, description=\"Filter by message status (e.g.,\
      \ pending, processing, processed, failed)\"),\n        limit: int = strawberry.field(default=10,\
      \ description=\"Maximum number of messages to return (1-100)\"),\n        offset:\
      \ int = strawberry.field(default=0, description=\"Number of messages to skip\
      \ (for pagination)\")\n    ) -> List[MessageGQL]:\n        \"\"\"Resolver for\
      \ retrieving messages within a queue.\"\"\"\n        log_pipeline(f\"GQL: Resolving\
      \ messages for Queue ID {self.id} (status={status}, limit={limit}, offset={offset})\"\
      , icon_type='GRAPHQL')\n        # Validate status filter\n        valid_statuses\
      \ = ['pending', 'processing', 'processed', 'failed']\n        if status and\
      \ status not in valid_statuses:\n            raise ValueError(f\"Invalid status\
      \ filter: '{status}'. Must be one of {valid_statuses}.\")\n\n        # Clamp\
      \ limit to a reasonable range (prevent excessive data retrieval)\n        limit\
      \ = max(1, min(limit, 100))\n        # Ensure offset is non-negative\n     \
      \   offset = max(0, offset)\n\n        try:\n             queue_id_int = int(self.id)\
      \ # Convert GQL ID to int\n             # Build the query\n             query\
      \ = Message.filter(queue_id=queue_id_int)\n             if status:\n       \
      \          query = query.filter(status=status)\n             # Apply ordering,\
      \ offset, and limit\n             messages_db = await query.order_by('-created_at').offset(offset).limit(limit)\n\
      \n             # Convert ORM models to GraphQL types (Need queue name for MessageGQL)\n\
      \             return [MessageGQL.from_orm(m, queue_name_str=self.name) for m\
      \ in messages_db]\n        except ValueError as ve: # Handles invalid ID or\
      \ status\n             log_warning(f\"GQL messages resolver validation error\
      \ for queue ID {self.id}: {ve}\", icon_type='GRAPHQL')\n             raise ve\
      \ # Let Strawberry handle user input validation errors by raising them\n   \
      \     except Exception as e:\n             log_error(f\"GQL messages resolver\
      \ error for queue ID {self.id}: {e}\", exc_info=True, icon_type='GRAPHQL')\n\
      \             return [] # Return empty list on internal server error\n\n# ---\
      \ GraphQL Root Query Type ---\n@strawberry.type\nclass QueryGQL:\n    @strawberry.field(description=\"\
      Retrieves a list of all available message queues\")\n    async def all_queues(self,\
      \ info: Info) -> List[QueueGQL]:\n        \"\"\"Resolver for the root 'allQueues'\
      \ query.\"\"\"\n        log_info(\"\U0001F353 GraphQL Query: all_queues\", icon_type='GRAPHQL')\n\
      \        try:\n             queues_db = await Queue.all().order_by('name')\n\
      \             # Convert ORM models to GraphQL types\n             # Note: message_count\
      \ resolver for each queue will be called by Strawberry if requested in the GQL\
      \ query\n             return [\n                 QueueGQL(id=strawberry.ID(str(q.id)),\
      \ name=q.name, created_at=q.created_at, updated_at=q.updated_at)\n         \
      \        for q in queues_db\n             ]\n        except Exception as e:\n\
      \            log_error(f\"GraphQL 'all_queues' resolver error: {e}\", icon_type='GRAPHQL',\
      \ exc_info=True)\n            return [] # Return empty list on error\n\n   \
      \ @strawberry.field(description=\"Retrieves a specific message queue by its\
      \ unique name\")\n    async def queue_by_name(self, info: Info, name: str) ->\
      \ Optional[QueueGQL]:\n        \"\"\"Resolver for the root 'queueByName' query.\"\
      \"\"\n        log_info(f\"\U0001F353 GraphQL Query: queue_by_name (name='{name}')\"\
      , icon_type='GRAPHQL')\n        try:\n            queue_db = await Queue.get_or_none(name=name)\n\
      \            if queue_db:\n                # Convert ORM model to GraphQL type\n\
      \                return QueueGQL(id=strawberry.ID(str(queue_db.id)), name=queue_db.name,\
      \ created_at=queue_db.created_at, updated_at=queue_db.updated_at)\n        \
      \    else:\n                log_warning(f\"GraphQL: Queue '{name}' not found\
      \ via queue_by_name.\", icon_type='GRAPHQL')\n                return None #\
      \ Return null if not found\n        except Exception as e:\n            log_error(f\"\
      GraphQL 'queue_by_name' resolver error for name '{name}': {e}\", icon_type='GRAPHQL',\
      \ exc_info=True)\n            return None # Return null on error\n\n    @strawberry.field(description=\"\
      Retrieves a specific message by its unique ID\")\n    async def message_by_id(self,\
      \ info: Info, id: strawberry.ID) -> Optional[MessageGQL]:\n        \"\"\"Resolver\
      \ for the root 'messageById' query.\"\"\"\n        log_info(f\"\U0001F353 GraphQL\
      \ Query: message_by_id (id={id})\", icon_type='GRAPHQL')\n        try:\n   \
      \         message_id_int = int(id) # Convert GQL ID string to int\n        \
      \    # Fetch message and its related queue in one query\n            message_db\
      \ = await Message.get_or_none(id=message_id_int).select_related('queue')\n\n\
      \            if message_db and message_db.queue: # Ensure message and its queue\
      \ exist\n                # Convert ORM model to GraphQL type, passing the queue\
      \ name\n                return MessageGQL.from_orm(message_db, queue_name_str=message_db.queue.name)\n\
      \            else:\n                log_warning(f\"GraphQL: Message ID {id}\
      \ not found or has no associated queue.\", icon_type='GRAPHQL')\n          \
      \      return None # Return null if not found or queue is missing\n        except\
      \ (ValueError, DoesNotExist): # Handle invalid ID format or message not found\n\
      \            log_warning(f\"GraphQL: Message ID {id} not found or invalid format.\"\
      , icon_type='GRAPHQL')\n            return None\n        except Exception as\
      \ e:\n            log_error(f\"GraphQL 'message_by_id' resolver error for ID\
      \ {id}: {e}\", icon_type='GRAPHQL', exc_info=True)\n            return None\n\
      \n# --- GraphQL Root Mutation Type ---\n@strawberry.type\nclass MutationGQL:\n\
      \    @strawberry.mutation(description=\"Creates a new message queue\")\n   \
      \ async def create_queue(self, info: Info, name: str) -> QueueGQL:\n       \
      \  \"\"\"Resolver for the 'createQueue' mutation.\"\"\"\n         log_info(f\"\
      \U0001F353 GraphQL Mutation: create_queue (name='{name}')\", icon_type='GRAPHQL')\n\
      \         # Add authentication check using context if needed\n         # context\
      \ = info.context\n         # if not context.get(\"current_user\"): raise Exception(\"\
      Authentication required\")\n         try:\n             # Use get_or_create\
      \ for atomicity\n             new_queue, created = await Queue.get_or_create(name=name)\n\
      \             if not created:\n                 # Raise exception for Strawberry\
      \ to format as a GraphQL error\n                 raise Exception(f\"Queue with\
      \ name '{name}' already exists.\")\n             log_success(f\"GQL: Queue '{name}'\
      \ created (ID: {new_queue.id}).\", icon_type='QUEUE')\n             # Convert\
      \ ORM model to GQL type for the response\n             return QueueGQL(id=strawberry.ID(str(new_queue.id)),\
      \ name=new_queue.name, created_at=new_queue.created_at, updated_at=new_queue.updated_at)\n\
      \         except Exception as e:\n             log_error(f\"GraphQL 'create_queue'\
      \ mutation error for name '{name}': {e}\", icon_type='GRAPHQL', exc_info=True)\n\
      \             # Re-raise for Strawberry to handle, providing a user-friendly\
      \ message\n             raise Exception(f\"Failed to create queue '{name}':\
      \ {e}\")\n\n    @strawberry.mutation(description=\"Deletes a queue and all its\
      \ associated messages\")\n    async def delete_queue(self, info: Info, name:\
      \ str) -> bool:\n        \"\"\"Resolver for the 'deleteQueue' mutation.\"\"\"\
      \n        log_info(f\"\U0001F353 GraphQL Mutation: delete_queue (name='{name}')\"\
      , icon_type='GRAPHQL')\n        # Add authentication check if needed\n     \
      \   try:\n            queue = await Queue.get_or_none(name=name)\n         \
      \   if not queue:\n                raise Exception(f\"Queue with name '{name}'\
      \ not found.\")\n            await queue.delete() # Cascade delete handled by\
      \ ORM relationship\n            log_success(f\"GQL: Queue '{name}' deleted successfully.\"\
      , icon_type='QUEUE')\n            return True # Return true on success\n   \
      \     except Exception as e:\n            log_error(f\"GraphQL 'delete_queue'\
      \ mutation error for name '{name}': {e}\", icon_type='GRAPHQL', exc_info=True)\n\
      \            raise Exception(f\"Failed to delete queue '{name}': {e}\") # Re-raise\
      \ for Strawberry\n\n    @strawberry.mutation(description=\"Publishes a message\
      \ to a specified queue\")\n    async def publish_message(self, info: Info, queue_name:\
      \ str, content: str) -> MessageGQL:\n        \"\"\"Resolver for the 'publishMessage'\
      \ mutation.\"\"\"\n        log_info(f\"\U0001F353 GraphQL Mutation: publish_message\
      \ (queue='{queue_name}')\", icon_type='GRAPHQL')\n        # Add authentication\
      \ check if needed\n        try:\n            queue = await Queue.get_or_none(name=queue_name)\n\
      \            if not queue:\n                raise Exception(f\"Queue with name\
      \ '{queue_name}' not found.\")\n            # Create the message\n         \
      \   new_message = await Message.create(queue=queue, content=content, status='pending')\n\
      \            log_success(f\"GQL: Message ID {new_message.id} published to queue\
      \ '{queue_name}'.\", icon_type='MSG')\n            # Convert ORM model to GQL\
      \ type for the response\n            return MessageGQL.from_orm(new_message,\
      \ queue_name_str=queue_name)\n        except Exception as e:\n            log_error(f\"\
      GraphQL 'publish_message' mutation error to queue '{queue_name}': {e}\", icon_type='GRAPHQL',\
      \ exc_info=True)\n            raise Exception(f\"Failed to publish message to\
      \ queue '{queue_name}': {e}\")\n\n# --- GraphQL Context Getter ---\nasync def\
      \ get_graphql_context(\n    request: Request,\n    response: Response,\n   \
      \ background_tasks: BackgroundTasks,\n    # Use Bearer scheme for GraphQL Authorization\
      \ header\n    auth: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme)\n\
      ) -> Dict:\n    \"\"\"\n    Provides context dictionary accessible within GraphQL\
      \ resolvers via `info.context`.\n    Includes request/response objects, background\
      \ tasks, and attempts to authenticate the user.\n    \"\"\"\n    context = {\n\
      \        \"request\": request,\n        \"response\": response,\n        \"\
      background_tasks\": background_tasks,\n        \"current_user\": None # Default\
      \ to unauthenticated\n    }\n    if auth:\n        try:\n            # Attempt\
      \ to validate the access token provided in the Authorization: Bearer header\n\
      \            username = await _decode_token(auth.credentials, \"access\")\n\
      \            context[\"current_user\"] = username # Set username in context\
      \ if valid\n            log_debug(f\"\U0001F353 GraphQL request authenticated\
      \ for user: '{username}'\", icon_type='AUTH')\n        except HTTPException\
      \ as auth_exc:\n            # Log failed auth attempts but don't block the request\
      \ here.\n            # Individual resolvers should check `info.context[\"current_user\"\
      ]` if they require auth.\n            log_warning(f\"GraphQL authentication\
      \ failed: {auth_exc.detail} (Status: {auth_exc.status_code})\", icon_type='AUTH')\n\
      \            # Optionally, you could set an error flag in the context: context[\"\
      auth_error\"] = auth_exc.detail\n    else:\n         log_debug(\"\U0001F353\
      \ GraphQL request is unauthenticated (no Bearer token found).\", icon_type='AUTH')\n\
      \n    return context\n\n# --- GraphQL Schema and Router Setup ---\ngql_schema\
      \ = strawberry.Schema(query=QueryGQL, mutation=MutationGQL)\ngraphql_app = GraphQLRouter(\n\
      \    gql_schema,\n    context_getter=get_graphql_context, # Function to create\
      \ the context dict\n    graphiql=False, # Disable default GraphiQL\n    graphql_ide=\"\
      apollo-sandbox\" # Use Apollo Sandbox (provides more features) hosted by Strawberry\n\
      \    # Or set to None to disable the IDE: graphql_ide=None\n)\n# Include the\
      \ GraphQL router in the main FastAPI application\napp.include_router(graphql_app,\
      \ prefix=\"/graphql\", tags=[\"GraphQL\"], include_in_schema=True) # include_in_schema\
      \ adds it to OpenAPI docs\nlog_success(\"\U0001F353 GraphQL endpoint /graphql\
      \ configured with Apollo Sandbox IDE.\", icon_type='GRAPHQL')\n\n# --- Global\
      \ Exception Handlers ---\n# These handlers catch specific exceptions that might\
      \ occur anywhere in the app\n# and return standardized JSON error responses.\n\
      \n@app.exception_handler(DoesNotExist)\nasync def tortoise_does_not_exist_handler(request:\
      \ Request, exc: DoesNotExist):\n    \"\"\"Handles Tortoise ORM's DoesNotExist\
      \ errors globally, returning 404.\"\"\"\n    # Try to extract model name from\
      \ the exception string for better context\n    model_name_match = str(exc).split(\"\
      :\")\n    model_name = model_name_match[0].strip() if len(model_name_match)\
      \ > 0 else \"Resource\"\n    detail = f\"{model_name} not found.\"\n    client_host\
      \ = request.client.host if request.client else \"N/A\"\n    log_warning(f\"\
      Resource Not Found (DB DoesNotExist): {exc} ({request.method} {request.url.path})\"\
      , icon_type='DB', extra={\"client\": client_host})\n    return JSONResponse(\n\
      \        status_code=status.HTTP_404_NOT_FOUND,\n        content={\"detail\"\
      : detail}\n    )\n\n@app.exception_handler(IntegrityError)\nasync def tortoise_integrity_error_handler(request:\
      \ Request, exc: IntegrityError):\n    \"\"\"Handles Tortoise ORM's IntegrityError\
      \ (e.g., unique constraint violations), returning 409.\"\"\"\n    detail = \"\
      Database conflict occurred.\"\n    # Try to get more specific error info from\
      \ the exception arguments (driver-dependent)\n    if hasattr(exc, 'args') and\
      \ exc.args:\n        # Avoid leaking overly detailed SQL errors in production\n\
      \        error_info = str(exc.args[0]) if settings.APP_ENV == 'development'\
      \ else \"Constraint violation.\"\n        detail += f\" Details: {error_info}\"\
      \n    client_host = request.client.host if request.client else \"N/A\"\n   \
      \ log_warning(f\"Database Integrity Conflict: {exc} ({request.method} {request.url.path})\"\
      , icon_type='DB', extra={\"client\": client_host})\n    return JSONResponse(\n\
      \        status_code=status.HTTP_409_CONFLICT,\n        content={\"detail\"\
      : detail}\n    )\n\n@app.exception_handler(ValidationError)\nasync def pydantic_validation_exception_handler(request:\
      \ Request, exc: ValidationError):\n    \"\"\"Handles Pydantic validation errors\
      \ (e.g., invalid request body), returning 422.\"\"\"\n    client_host = request.client.host\
      \ if request.client else \"N/A\"\n    log_warning(f\"Request Validation Error\
      \ (Pydantic): {exc.errors()} ({request.method} {request.url.path})\", icon_type='HTTP',\
      \ extra={\"client\": client_host})\n    return JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n\
      \        # Use Pydantic's json() method for standardized error output\n    \
      \    content={\"detail\": \"Request validation failed\", \"errors\": json.loads(exc.json())}\n\
      \    )\n\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request:\
      \ Request, exc: HTTPException):\n    \"\"\"Handles FastAPI's built-in HTTPExceptions,\
      \ ensuring consistent logging and response format.\"\"\"\n    # Determine log\
      \ level based on status code (warnings for 4xx, errors for 5xx)\n    log_level\
      \ = log_warning if 400 <= exc.status_code < 500 else log_error\n    icon = 'HTTP'\
      \ if 400 <= exc.status_code < 500 else 'ERROR'\n    client_host = request.client.host\
      \ if request.client else \"N/A\"\n    log_level(\n        f\"HTTP Error Handled:\
      \ Status={exc.status_code}, Detail='{exc.detail}' ({request.method} {request.url.path})\"\
      ,\n        icon_type=icon,\n        extra={\"client\": client_host, \"headers\"\
      : exc.headers} # Log associated headers (like WWW-Authenticate)\n    )\n   \
      \ # Return the standard JSON response using details and headers from the exception\n\
      \    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"\
      detail\": exc.detail},\n        headers=getattr(exc, \"headers\", None) # Include\
      \ headers if the exception has them\n    )\n\n@app.exception_handler(Exception)\n\
      async def generic_exception_handler(request: Request, exc: Exception):\n   \
      \ \"\"\"\n    Handles any other unhandled exceptions as a generic 500 Internal\
      \ Server Error.\n    Logs the error critically and returns a safe error message\
      \ to the client.\n    \"\"\"\n    # Format traceback for logging\n    tb_str\
      \ = \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n\
      \    client_host = request.client.host if request.client else \"N/A\"\n    error_time\
      \ = datetime.now(timezone.utc)\n\n    # Log critically, including a traceback\
      \ summary\n    log_critical(\n        f\"Unhandled Internal Server Error: {type(exc).__name__}:\
      \ {exc} ({request.method} {request.url.path})\",\n        icon_type='CRITICAL',\n\
      \        exc_info=False, # Avoid duplicating full traceback if JsonFormatter\
      \ logs it\n        extra={\n            \"client\": client_host,\n         \
      \   # Log full traceback only in development for security/verbosity reasons\n\
      \            \"full_traceback\": tb_str if settings.APP_ENV == 'development'\
      \ else \"Traceback hidden in production\"\n        }\n    )\n    # Update app\
      \ stats to indicate the last error\n    async with stats_lock:\n        app_stats[\"\
      last_error\"] = f\"Unhandled {type(exc).__name__} at {request.method} {request.url.path}\"\
      \n        app_stats[\"last_error_timestamp\"] = error_time\n\n    # Return a\
      \ generic 500 response to the client\n    return JSONResponse(\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n\
      \        content={\"detail\": \"An unexpected internal server error occurred.\
      \ Please contact the administrator or check server logs.\"}\n    )\n\n# ---\
      \ Main Execution Block ---\nif __name__ == '__main__':\n    log_info(\"\U0001F3C1\
      \ Main execution block entered...\", icon_type='SETUP')\n\n    # --- SSL Certificate\
      \ Check/Generation ---\n    log_info(\"Checking for SSL certificate and key...\"\
      , icon_type='SEC')\n    try:\n        # Ensure certificate directory exists\n\
      \        os.makedirs(settings.CERT_DIR, exist_ok=True)\n        cert_exists\
      \ = os.path.exists(settings.CERT_FILE)\n        key_exists = os.path.exists(settings.KEY_FILE)\n\
      \n        if cert_exists and key_exists:\n            log_success(f\"\U0001F6E1\
      ️ SSL Certificate '{os.path.basename(settings.CERT_FILE)}' and Key '{os.path.basename(settings.KEY_FILE)}'\
      \ found in '{settings.CERT_DIR}'.\", icon_type='SEC')\n            # Optional:\
      \ Add check for certificate expiry here if needed\n        else:\n         \
      \   missing = [f for f, exists in [(os.path.basename(settings.CERT_FILE), cert_exists),\
      \ (os.path.basename(settings.KEY_FILE), key_exists)] if not exists]\n      \
      \      log_warning(f\"SSL file(s) not found: {', '.join(missing)}. Attempting\
      \ to generate new self-signed certificate for 'localhost'...\", icon_type='SEC')\n\
      \            try:\n                if not generate_self_signed_cert(settings.CERT_FILE,\
      \ settings.KEY_FILE, common_name=\"localhost\"):\n                    log_critical(\"\
      Critical failure generating self-signed SSL certificates. Cannot start server\
      \ with HTTPS.\", icon_type='CRITICAL')\n                    sys.exit(1) # Exit\
      \ if generation fails\n                else:\n                     log_success(\"\
      ✅ Successfully generated new self-signed SSL certificate and key.\", icon_type='SEC')\n\
      \            except Exception as cert_gen_e:\n                log_critical(f\"\
      Unexpected error during certificate generation: {cert_gen_e}\", icon_type='CRITICAL',\
      \ exc_info=True)\n                sys.exit(1)\n    except Exception as setup_e:\n\
      \        log_critical(f\"Unexpected error during initial certificate setup check:\
      \ {setup_e}\", icon_type='CRITICAL', exc_info=True)\n        sys.exit(1)\n\n\
      \    # --- Log Final Configuration Summary ---\n    log_info(\"=== Configuration\
      \ Summary ===\", icon_type='SETUP')\n    log_info(f\"  Project: {settings.PROJECT_NAME}\
      \ v{settings.VERSION}\", icon_type='INFO')\n    log_info(f\"  Environment: {settings.APP_ENV}\"\
      , icon_type='INFO')\n    log_info(f\"  Log Level: {settings.LOG_LEVEL_STR}\"\
      , icon_type='LOGS')\n    log_info(f\"  JWT Secret: {'Set via Env Var' if 'JWT_SECRET_KEY'\
      \ in os.environ and 'CHANGE_ME' not in settings.JWT_SECRET_KEY else 'Using Generated/Default\
      \ (INSECURE FOR PROD)'}\", icon_type='AUTH')\n    log_info(f\"  DB Path: {settings.DB_PATH}\"\
      , icon_type='DB')\n    log_info(f\"  Rate Limit (Default): {settings.DEFAULT_RATE_LIMIT}\"\
      , icon_type='RATELIMIT')\n    log_info(f\"  Rate Limit (High Traffic): {settings.HIGH_TRAFFIC_RATE_LIMIT}\"\
      , icon_type='RATELIMIT')\n    log_info(f\"  CORS Origins: {settings.ALLOWED_ORIGINS}\"\
      , icon_type='HTTP')\n    log_info(f\"  Log Dir: {settings.LOG_DIR} (Current\
      \ File: {os.path.basename(LOG_FILENAME)})\", icon_type='LOGS')\n    log_info(f\"\
      \  Cert Dir: {settings.CERT_DIR}\", icon_type='SEC')\n    log_info(f\"============================\"\
      , icon_type='SETUP')\n\n    # --- Determine Uvicorn settings ---\n    # Enable\
      \ auto-reload only in development environment\n    reload_enabled = settings.APP_ENV\
      \ == \"development\"\n    if reload_enabled:\n        log_warning(\"Running\
      \ in DEVELOPMENT mode with auto-reload enabled.\", icon_type='SETUP')\n\n  \
      \  # Map FastAPI log level to Uvicorn log level string\n    # Uvicorn levels:\
      \ 'critical', 'error', 'warning', 'info', 'debug', 'trace'\n    log_level_uvicorn\
      \ = settings.LOG_LEVEL_STR.lower()\n    # Adjust if necessary (e.g., FastAPI\
      \ DEBUG might map to uvicorn debug/trace)\n    if log_level_uvicorn == 'debug'\
      \ and reload_enabled:\n        log_level_uvicorn = 'debug' # Keep debug for\
      \ dev reload\n    elif log_level_uvicorn == 'debug':\n        log_level_uvicorn\
      \ = 'info' # Default to info if not in dev reload\n\n    # --- Start Uvicorn\
      \ Server ---\n    log_info(f\"\U0001F310\U0001F680 Starting Uvicorn server on\
      \ https://0.0.0.0:{settings.API_PORT}\", icon_type='STARTUP', extra={\"reload\"\
      : reload_enabled, \"log_level\": log_level_uvicorn})\n    log_info(f\"   Access\
      \ API root at: https://localhost:{settings.API_PORT}/\", icon_type='HTTP')\n\
      \    log_info(f\"   Swagger UI docs:  https://localhost:{settings.API_PORT}/docs\"\
      , icon_type='HTTP')\n    log_info(f\"   ReDoc docs:       https://localhost:{settings.API_PORT}/redoc\"\
      , icon_type='HTTP')\n    log_info(f\"   GraphQL endpoint: https://localhost:{settings.API_PORT}/graphql\
      \ (Apollo Sandbox IDE)\", icon_type='GRAPHQL')\n    log_info(\"   Press Ctrl+C\
      \ to stop the server.\", icon_type='INFO')\n\n    try:\n        uvicorn.run(\n\
      \            \"__main__:app\", # Points to the 'app' instance in this file when\
      \ run directly\n            host=\"0.0.0.0\", # Listen on all available network\
      \ interfaces\n            port=settings.API_PORT,\n            log_level=log_level_uvicorn,\
      \ # Set Uvicorn's internal logging level\n            ssl_keyfile=settings.KEY_FILE,\
      \ # Path to SSL private key\n            ssl_certfile=settings.CERT_FILE, #\
      \ Path to SSL certificate\n            reload=reload_enabled, # Enable auto-reload\
      \ if in development\n            use_colors=True # Use colors in Uvicorn's console\
      \ output if terminal supports it\n            # Consider adding access_log=False\
      \ in production if logs are handled elsewhere to reduce noise\n            #\
      \ access_log=(settings.APP_ENV == \"development\")\n        )\n    except KeyboardInterrupt:\n\
      \        # Handle Ctrl+C gracefully\n        log_info(\"\\n\U0001F6A6 Server\
      \ shutdown requested via Keyboard Interrupt (Ctrl+C).\", icon_type='SHUTDOWN')\n\
      \    except SystemExit as e:\n         # Handle sys.exit() calls (e.g., from\
      \ failed startup checks)\n         log_info(f\"\U0001F6A6 Server exited with\
      \ code {e.code}.\", icon_type='SHUTDOWN')\n    except Exception as e:\n    \
      \    # Catch potential errors during Uvicorn startup (e.g., port already in\
      \ use)\n        log_critical(f\"❌ Fatal: Failed to start or run Uvicorn server:\
      \ {e}\", exc_info=True)\n        sys.exit(1) # Exit with error code if Uvicorn\
      \ fails to start\n    finally:\n        # This block runs after Uvicorn stops,\
      \ regardless of the reason\n        log_info(\"\U0001F3C1 Uvicorn server process\
      \ has finished.\", icon_type='SHUTDOWN')"
    tamanho: 0.10 MB
  meu_bloco.json:
    caminho_completo: .\meu_bloco.json
    json_info:
      numero_de_linhas: 5
      tamanho: 0.00 MB
    numero_de_linhas: 5
    tamanho: 0.00 MB
  mypy.ini:
    caminho_completo: .\mypy.ini
    numero_de_linhas: 54
    tamanho: 0.00 MB
  pyproject.toml:
    caminho_completo: .\pyproject.toml
    numero_de_linhas: 62
    tamanho: 0.00 MB
  pytest.ini:
    caminho_completo: .\pytest.ini
    numero_de_linhas: 33
    tamanho: 0.00 MB
  readmev1.md:
    caminho_completo: .\readmev1.md
    numero_de_linhas: 439
    tamanho: 0.02 MB
  requirements.txt:
    caminho_completo: .\requirements.txt
    numero_de_linhas: 12
    tamanho: 0.00 MB
  tortoise_config.py:
    caminho_completo: .\tortoise_config.py
    classes: []
    functions: []
    imports:
    - module: typing
      names:
      - List
    numero_de_linhas: 28
    source_code: "\"\"\"\nConfiguração do Tortoise ORM para o Message Broker.\nAutor:\
      \ Elias Andrade\nData: 2024-03-19\nVersão: 1.0.0\n\"\"\"\n\nfrom typing import\
      \ List\n\nTORTOISE_ORM = {\n    \"connections\": {\n        \"default\": \"\
      sqlite://db.sqlite3\"\n    },\n    \"apps\": {\n        \"models\": {\n    \
      \        \"models\": [\"message_broker_v1\"],  # Nome do módulo onde estão os\
      \ modelos\n            \"default_connection\": \"default\",\n        }\n   \
      \ },\n    \"use_tz\": False,\n    \"timezone\": \"America/Sao_Paulo\"\n}\n\n\
      # Lista de modelos para migração automática\nMODELS: List[str] = [\n    \"message_broker_v1.Queue\"\
      ,\n    \"message_broker_v1.Message\"\n] "
    tamanho: 0.00 MB
  webdashv1.py:
    caminho_completo: .\webdashv1.py
    classes: []
    functions:
    - docstring: Tenta fazer login na API principal e armazena os tokens.
      end_lineno: 137
      lineno: 67
      name: login_to_api
    - docstring: Busca dados de /stats da API principal, handling authentication.
      end_lineno: 286
      lineno: 140
      name: fetch_api_data
    - docstring: Executa o loop do agendador em uma thread separada.
      end_lineno: 310
      lineno: 289
      name: run_scheduler
    - docstring: Serve the main dashboard HTML page, rendering the template string.
      end_lineno: 978
      lineno: 964
      name: serve_dashboard
    - docstring: Endpoint for the frontend JavaScript to fetch the collected data.
      end_lineno: 1013
      lineno: 981
      name: get_dashboard_data
    imports:
    - asname: null
      name: os
    - asname: null
      name: time
    - asname: null
      name: threading
    - asname: null
      name: logging
    - module: collections
      names:
      - deque
    - module: threading
      names:
      - Lock
    - module: datetime
      names:
      - datetime
      - timezone
    - asname: null
      name: json
    - asname: null
      name: requests
    - asname: null
      name: schedule
    - module: flask
      names:
      - Flask
      - Response
      - jsonify
      - render_template_string
    - module: flask_cors
      names:
      - CORS
    - asname: null
      name: urllib3
    numero_de_linhas: 1049
    source_code: "# dashboard_server.py\nimport os\nimport time\nimport threading\n\
      import logging\nfrom collections import deque\nfrom threading import Lock\n\
      from datetime import datetime, timezone\nimport json # Import json for potential\
      \ decoding errors\n\nimport requests # Para fazer requisições à API principal\n\
      import schedule # Para agendar a coleta de dados\nfrom flask import Flask, Response,\
      \ jsonify, render_template_string\nfrom flask_cors import CORS\n\n# --- Configuração\
      \ ---\nDASHBOARD_PORT = 8333\nAPI_BASE_URL = os.environ.get(\"API_BASE_URL\"\
      , \"https://127.0.0.1:8777\") # Use HTTPS for the API\nAPI_STATS_URL = f\"{API_BASE_URL}/stats\"\
      \nAPI_LOGIN_URL = f\"{API_BASE_URL}/login\"\n\n# Credentials for the dashboard\
      \ to access the main API\n# !!! Use environment variables in production !!!\n\
      API_USERNAME = os.environ.get(\"API_USER\", \"admin\")\nAPI_PASSWORD = os.environ.get(\"\
      API_PASS\", \"admin\")\n\nFETCH_INTERVAL_SECONDS = 5 # Intervalo de coleta de\
      \ dados\nMAX_CHART_HISTORY = 60 # Pontos no histórico para gráficos de linha\n\
      \n# --- Configuração de Logging ---\n# Use basicConfig or integrate with Flask's\
      \ logger\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s\
      \ - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n\
      )\nlogger = logging.getLogger('webdashv1') # Use a specific name for the dashboard\
      \ logger\n\n# --- Estado Global e Controle ---\napp_state = {\n    \"latest_stats\"\
      : {},\n    \"last_error\": None,\n    \"last_successful_fetch\": None,\n   \
      \ \"api_token\": None, # Store only the access token\n    \"refresh_token\"\
      : None, # Store refresh token if API supports refresh logic\n    \"login_needed\"\
      : True,\n    \"is_fetching\": False, # Flag to prevent concurrent fetches\n\
      \    # Histórico para gráficos\n    \"time_labels\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \    \"request_history\": deque(maxlen=MAX_CHART_HISTORY), # Deltas por intervalo\n\
      \    \"message_status_history\": {\n        \"pending\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \        \"processing\": deque(maxlen=MAX_CHART_HISTORY),\n        \"failed\"\
      : deque(maxlen=MAX_CHART_HISTORY),\n        \"processed\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \    },\n    \"performance_history\": {\n        \"cpu\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \        \"memory\": deque(maxlen=MAX_CHART_HISTORY),\n    },\n    \"previous_total_requests\"\
      : 0 # Para calcular delta\n}\ndata_lock = Lock() # Para proteger o acesso concorrente\
      \ ao app_state\n\n# --- Funções de Coleta e Processamento ---\n\ndef login_to_api():\n\
      \    \"\"\"Tenta fazer login na API principal e armazena os tokens.\"\"\"\n\
      \    global app_state\n    logger.info(f\"Attempting login to API at {API_LOGIN_URL}...\"\
      )\n    try:\n        # Use data payload for standard OAuth2 Password Flow\n\
      \        login_data = {'username': API_USERNAME, 'password': API_PASSWORD}\n\
      \n        # IMPORTANT: Disable SSL verification ONLY for local dev with self-signed\
      \ certs\n        # In production, use verify=True or path to your CA bundle\n\
      \        response = requests.post(API_LOGIN_URL, data=login_data, verify=False,\
      \ timeout=10)\n\n        response.raise_for_status() # Lança exceção para erros\
      \ HTTP 4xx/5xx\n\n        token_data = response.json()\n        if \"access_token\"\
      \ in token_data and \"refresh_token\" in token_data:\n            with data_lock:\n\
      \                app_state[\"api_token\"] = token_data[\"access_token\"]\n \
      \               app_state[\"refresh_token\"] = token_data[\"refresh_token\"\
      ] # Store if needed later\n                app_state[\"login_needed\"] = False\n\
      \                app_state[\"last_error\"] = None # Limpa erro de login anterior\n\
      \            logger.info(\"API login successful.\")\n            return True\n\
      \        else:\n            logger.error(\"API login response missing 'access_token'\
      \ or 'refresh_token'.\")\n            with data_lock:\n                app_state[\"\
      last_error\"] = \"Login response missing tokens\"\n                app_state[\"\
      api_token\"] = None\n                app_state[\"refresh_token\"] = None\n \
      \               app_state[\"login_needed\"] = True\n            return False\n\
      \n    except requests.exceptions.RequestException as e:\n        status_code\
      \ = getattr(e.response, 'status_code', 'N/A')\n        error_detail = f\"Status:\
      \ {status_code}\"\n        try:\n             # Try to get more details from\
      \ response body\n             if e.response is not None:\n                 #\
      \ Check content type before assuming JSON\n                 content_type = e.response.headers.get('Content-Type',\
      \ '')\n                 if 'application/json' in content_type:\n           \
      \         error_json = e.response.json()\n                    error_detail +=\
      \ f\" - Detail: {error_json.get('detail', error_json)}\"\n                 else:\n\
      \                    error_detail += f\" - Response: {e.response.text[:200]}\"\
      \ # Log snippet of non-JSON\n        except Exception: pass # Ignore if can't\
      \ read response body or decode JSON\n        logger.error(f\"API login failed\
      \ ({error_detail}): {e}\")\n        with data_lock:\n            app_state[\"\
      last_error\"] = f\"Login request failed: {e}\" # Store generic error\n     \
      \       app_state[\"api_token\"] = None\n            app_state[\"refresh_token\"\
      ] = None\n            app_state[\"login_needed\"] = True\n        return False\n\
      \    except json.JSONDecodeError as e:\n        # Handle cases where the response\
      \ is not valid JSON\n        response_text = getattr(e, 'doc', '') or getattr(e.response,\
      \ 'text', 'N/A')\n        logger.error(f\"API login failed: Could not decode\
      \ JSON response. Response text: {response_text[:500]}\")\n        with data_lock:\n\
      \            app_state[\"last_error\"] = \"Login response was not valid JSON\"\
      \n            app_state[\"api_token\"] = None\n            app_state[\"refresh_token\"\
      ] = None\n            app_state[\"login_needed\"] = True\n        return False\n\
      \    except Exception as e:\n        logger.error(f\"Unexpected error during\
      \ API login: {e}\", exc_info=True) # Log traceback for unexpected errors\n \
      \       with data_lock:\n            app_state[\"last_error\"] = f\"Unexpected\
      \ login error: {e}\"\n            app_state[\"api_token\"] = None\n        \
      \    app_state[\"refresh_token\"] = None\n            app_state[\"login_needed\"\
      ] = True\n        return False\n\n\ndef fetch_api_data():\n    \"\"\"Busca dados\
      \ de /stats da API principal, handling authentication.\"\"\"\n    global app_state\n\
      \n    # Prevent concurrent fetches\n    with data_lock:\n        if app_state.get(\"\
      is_fetching\", False):\n            logger.debug(\"Fetch cycle skipped, already\
      \ fetching.\")\n            return\n        app_state[\"is_fetching\"] = True\n\
      \        # Get needed state values under lock\n        token = app_state[\"\
      api_token\"]\n        login_needed = app_state[\"login_needed\"]\n\n    # Release\
      \ lock before network I/O\n    logger.debug(\"Starting data fetch cycle...\"\
      )\n\n    try: # Outer try for managing the 'is_fetching' flag\n        # ---\
      \ Handle Authentication ---\n        if login_needed or not token:\n       \
      \     logger.warning(\"Login required or token missing, attempting login...\"\
      )\n            if not login_to_api():\n                logger.error(\"Fetch\
      \ cycle aborted due to login failure.\")\n                # Update state with\
      \ login error if login_to_api didn't already\n                with data_lock:\n\
      \                     if app_state[\"last_error\"] is None:\n              \
      \           app_state[\"last_error\"] = \"Login required but failed\"\n    \
      \            return # Exit fetch cycle\n\n            # Re-fetch the token after\
      \ successful login (under lock)\n            with data_lock:\n             \
      \   token = app_state[\"api_token\"]\n                # If token is *still*\
      \ missing after a successful login_to_api call, something is wrong\n       \
      \         if not token:\n                     logger.error(\"CRITICAL: Token\
      \ still missing after supposedly successful login. Aborting fetch cycle.\")\n\
      \                     app_state[\"last_error\"] = \"Internal dashboard error:\
      \ Token lost after login.\"\n                     return\n\n        # --- Fetch\
      \ Stats Data ---\n        logger.debug(f\"Fetching stats from {API_STATS_URL}...\"\
      )\n        headers = {'Authorization': f'Bearer {token}', 'Accept': 'application/json'}\n\
      \n        try:\n            # Again, disable SSL verification ONLY for local\
      \ dev\n            response = requests.get(API_STATS_URL, headers=headers, verify=False,\
      \ timeout=10)\n\n            # Check for auth errors first\n            if response.status_code\
      \ == 401 or response.status_code == 403:\n                logger.warning(f\"\
      API returned {response.status_code} (Unauthorized/Forbidden) fetching stats.\
      \ Token likely expired. Forcing re-login.\")\n                with data_lock:\n\
      \                    app_state[\"api_token\"] = None\n                    #\
      \ Decide whether to clear refresh token too\n                    # app_state[\"\
      refresh_token\"] = None\n                    app_state[\"login_needed\"] = True\n\
      \                    app_state[\"last_error\"] = f\"API Auth error ({response.status_code}).\
      \ Re-login needed.\"\n                # No need to call login_to_api() here,\
      \ it will happen at the start of the next cycle\n                return # Abort\
      \ this cycle\n\n            # Check for other HTTP errors\n            response.raise_for_status()\n\
      \n            # ---- Process Successful Response ----\n            try:\n  \
      \              stats = response.json()\n                now = datetime.now(timezone.utc)\n\
      \                logger.debug(\"Stats received successfully from API.\")\n\n\
      \                # Process and update the state under lock\n               \
      \ with data_lock:\n                    app_state[\"latest_stats\"] = stats #\
      \ Store the raw stats object\n                    app_state[\"last_successful_fetch\"\
      ] = now.isoformat()\n                    app_state[\"last_error\"] = None #\
      \ Clear errors on success\n\n                    # --- Update history deques\
      \ ---\n                    current_time_label = now.strftime(\"%H:%M:%S\")\n\
      \                    app_state[\"time_labels\"].append(current_time_label)\n\
      \n                    # Calculate request delta safely\n                   \
      \ current_total_requests = stats.get(\"requests_total\")\n                 \
      \   request_delta = 0 # Default if calculation fails\n                    if\
      \ isinstance(current_total_requests, int):\n                        prev_total\
      \ = app_state[\"previous_total_requests\"]\n                        # Ensure\
      \ previous total is also valid before subtracting\n                        request_delta\
      \ = max(0, current_total_requests - (prev_total if isinstance(prev_total, int)\
      \ else 0))\n                        app_state[\"previous_total_requests\"] =\
      \ current_total_requests # Update for next cycle\n                    else:\n\
      \                        logger.warning(f\"`requests_total` missing or not integer\
      \ in stats response: {current_total_requests}. Delta calculation skipped.\"\
      )\n\n                    app_state[\"request_history\"].append(request_delta)\n\
      \n                    # Update message history (use .get with default 0)\n \
      \                   app_state[\"message_status_history\"][\"pending\"].append(stats.get(\"\
      messages_pending\", 0))\n                    app_state[\"message_status_history\"\
      ][\"processing\"].append(stats.get(\"messages_processing\", 0))\n          \
      \          app_state[\"message_status_history\"][\"failed\"].append(stats.get(\"\
      messages_failed\", 0))\n                    app_state[\"message_status_history\"\
      ][\"processed\"].append(stats.get(\"messages_processed\", 0))\n\n          \
      \          # Update performance history (handle potential missing nested keys\
      \ gracefully)\n                    system_stats = stats.get(\"system\", {})\
      \ # Default to empty dict if 'system' is missing\n                    cpu =\
      \ system_stats.get(\"process_cpu_percent\")\n                    mem = system_stats.get(\"\
      process_memory_mb\")\n                    # Ensure values are numeric before\
      \ appending, default to 0\n                    app_state[\"performance_history\"\
      ][\"cpu\"].append(cpu if isinstance(cpu, (int, float)) else 0)\n           \
      \         app_state[\"performance_history\"][\"memory\"].append(mem if isinstance(mem,\
      \ (int, float)) else 0)\n\n                logger.debug(\"Dashboard state updated\
      \ with new stats and history.\")\n\n            except json.JSONDecodeError\
      \ as e:\n                 logger.error(f\"Failed to decode JSON response from\
      \ API /stats: {e}. Response text: {response.text[:500]}\")\n               \
      \  with data_lock:\n                      app_state[\"last_error\"] = \"API\
      \ /stats response is not valid JSON\"\n            except Exception as processing_e:\
      \ # Catch errors during state update/processing\n                logger.error(f\"\
      Error processing received stats: {processing_e}\", exc_info=True)\n        \
      \        with data_lock:\n                    app_state[\"last_error\"] = f\"\
      Error processing stats: {processing_e}\"\n\n\n        # --- Handle Request Errors\
      \ ---\n        except requests.exceptions.Timeout:\n            logger.error(\"\
      API /stats request timed out.\")\n            with data_lock: app_state[\"last_error\"\
      ] = \"API Timeout fetching stats\"\n        except requests.exceptions.ConnectionError\
      \ as e:\n            logger.error(f\"API /stats connection error: {e}\")\n \
      \           with data_lock: app_state[\"last_error\"] = f\"API Connection Error:\
      \ {e}\"\n        except requests.exceptions.RequestException as e:\n       \
      \      # This catches other HTTP errors (like 500, 404 etc.) not handled above\n\
      \            status_code = getattr(e.response, 'status_code', 'N/A')\n     \
      \       logger.error(f\"API /stats request failed (Status: {status_code}): {e}\"\
      )\n            error_detail = f\"API Request Failed: {e}\"\n            try:\
      \ # Attempt to get response details\n                if e.response is not None:\n\
      \                    error_detail += f\" - Response: {e.response.text[:200]}\"\
      \n            except Exception: pass\n            with data_lock:\n        \
      \         # Avoid overwriting a more specific auth error logged earlier in the\
      \ cycle\n                 if app_state.get(\"last_error\") is None or \"Auth\
      \ error\" not in app_state[\"last_error\"]:\n                     app_state[\"\
      last_error\"] = error_detail\n\n    except Exception as outer_e:\n         #\
      \ Catch errors in the logic before the actual request (e.g., token fetching\
      \ issues missed)\n        logger.error(f\"Unexpected error during data fetch\
      \ setup: {outer_e}\", exc_info=True)\n        with data_lock:\n            app_state[\"\
      last_error\"] = f\"Unexpected Fetch Error: {outer_e}\"\n\n    finally:\n   \
      \     # Ensure the fetching flag is reset even if errors occur\n        with\
      \ data_lock:\n            app_state[\"is_fetching\"] = False\n\n\ndef run_scheduler():\n\
      \    \"\"\"Executa o loop do agendador em uma thread separada.\"\"\"\n    logger.info(\"\
      Scheduler thread started.\")\n    # Perform an initial fetch immediately before\
      \ starting the scheduled loop\n    # This helps populate the dashboard faster\
      \ on startup\n    try:\n        fetch_api_data()\n    except Exception as e:\n\
      \         logger.error(f\"Error during initial data fetch: {e}\", exc_info=True)\n\
      \n    # Schedule the regular fetching task\n    schedule.every(FETCH_INTERVAL_SECONDS).seconds.do(fetch_api_data)\n\
      \n    # Keep the scheduler running\n    while True:\n        try:\n        \
      \    schedule.run_pending()\n        except Exception as e:\n             #\
      \ Catch potential errors within the scheduler loop itself or the scheduled job\n\
      \             logger.error(f\"Error in scheduler loop: {e}\", exc_info=True)\n\
      \             # Avoid busy-waiting in case of continuous errors\n        time.sleep(1)\
      \ # Check schedule every second\n\n# --- Flask App ---\napp = Flask(__name__)\n\
      # Configure Flask logger to use our handler setup\napp.logger.handlers = logger.handlers\n\
      app.logger.setLevel(logger.level)\nCORS(app) # Enable CORS for all routes by\
      \ default\n\n# --- HTML Content (CRITICAL FIX: Added {% raw %} blocks) ---\n\
      HTML_CONTENT = \"\"\"\n<!DOCTYPE html>\n<html lang=\"pt-BR\">\n<head>\n    <meta\
      \ charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width,\
      \ initial-scale=1.0\">\n    <title>API Metrics Dashboard</title>\n    <script\
      \ src=\"https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js\"></script>\n\
      \    {# Add a simple Favicon using SVG inline #}\n    <link rel=\"icon\" href=\"\
      data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220\
      \ 0 100 100%22><path d=%22M8.3 25L41.7 8.3L75 25L41.7 41.7L8.3 25Z%22 stroke=%22%2300bcd4%22\
      \ stroke-width=%2210%22 fill=%22none%22/><path d=%22M8.3 75L41.7 58.3L75 75L41.7\
      \ 91.7L8.3 75Z%22 stroke=%22%2300bcd4%22 stroke-width=%2210%22 fill=%22none%22/><path\
      \ d=%22M8.3 50H75%22 stroke=%22%2300bcd4%22 stroke-width=%2210%22 fill=%22none%22/></svg>\"\
      >\n\n    {# ***** START RAW BLOCK FOR CSS ***** #}\n    {% raw %}\n    <style>\n\
      \        /* Reset and Base Styles */\n        * { box-sizing: border-box; margin:\
      \ 0; padding: 0; }\n        @keyframes fadeIn { from { opacity: 0; } to { opacity:\
      \ 1; } }\n        @keyframes highlight-value {\n            0%, 100% { transform:\
      \ scale(1); color: #fff; }\n            50% { transform: scale(1.05); color:\
      \ #80deea; } /* Lighter cyan highlight */\n        }\n        .value-changed\
      \ .card-value { animation: highlight-value 0.4s ease-out; }\n\n        body\
      \ {\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\"\
      , Roboto, Helvetica, Arial, sans-serif;\n            background: #181a1f; color:\
      \ #e0e0e0; line-height: 1.5;\n            display: flex; flex-direction: column;\
      \ min-height: 100vh; overflow-x: hidden;\n        }\n\n        /* Header */\n\
      \        .app-header {\n            background: #21242a; padding: 10px 25px;\
      \ display: flex; align-items: center;\n            border-bottom: 1px solid\
      \ #3a3d4a; position: sticky; top: 0; z-index: 10;\n            box-shadow: 0\
      \ 2px 8px rgba(0, 0, 0, 0.2); flex-wrap: wrap;\n        }\n        .logo { display:\
      \ flex; align-items: center; margin-right: 20px; color: #00bcd4; /* Color for\
      \ SVG */ }\n        .logo svg { margin-right: 8px; }\n        .logo-text-main\
      \ { font-weight: 700; font-size: 1.4em; letter-spacing: 1px; color: #fff;}\n\
      \        .logo-text-sub { font-size: 0.45em; color: #a0a0a0; text-transform:\
      \ uppercase; line-height: 1; display: block; font-weight: normal; letter-spacing:\
      \ 0.5px; margin-top: -2px;}\n        .main-title { flex-grow: 1; text-align:\
      \ center; font-size: 1.2em; font-weight: 500; color: #c5c5c5; margin: 5px 15px;\
      \ }\n        .status-indicator { font-size: 0.95em; font-weight: 500; margin:\
      \ 5px 0; text-align: right; min-width: 150px; transition: color 0.3s ease; }\n\
      \        .status-indicator.live { color: #4caf50; } /* Green */\n        .status-indicator.error\
      \ { color: #f44336; font-weight: bold; } /* Red */\n        .status-indicator.stale\
      \ { color: #ff9800; } /* Orange */\n        .status-indicator.fetching { color:\
      \ #03a9f4; } /* Blue */\n\n        /* Main Content Grid */\n        .main-content\
      \ {\n            display: grid;\n            grid-template-columns: repeat(auto-fit,\
      \ minmax(200px, 1fr)); /* Responsive grid */\n            gap: 20px;\n     \
      \       padding: 25px;\n            flex-grow: 1;\n        }\n\n        /* Card\
      \ Styles */\n        .status-card {\n            border-radius: 8px; box-shadow:\
      \ 0 4px 12px rgba(0, 0, 0, 0.25);\n            overflow: hidden; display: flex;\
      \ flex-direction: column;\n            color: #ffffff; border: 1px solid rgba(255,\
      \ 255, 255, 0.08);\n            animation: fadeIn 0.5s ease-out forwards;\n\
      \            background-color: #2a2d35; /* Default background */\n         \
      \   transition: background-color 0.3s ease, transform 0.2s ease;\n        }\n\
      \        .status-card:hover { transform: translateY(-3px); box-shadow: 0 6px\
      \ 16px rgba(0, 0, 0, 0.3); }\n\n        /* Semantic background colors with gradients\
      \ */\n        .bg-pending { background: linear-gradient(135deg, #ffb74d, #ffa726);\
      \ } /* Orange */\n        .bg-processing { background: linear-gradient(135deg,\
      \ #64b5f6, #42a5f5); } /* Blue */\n        .bg-failed { background: linear-gradient(135deg,\
      \ #e57373, #ef5350); } /* Red */\n        .bg-processed { background: linear-gradient(135deg,\
      \ #81c784, #66bb6a); } /* Green */\n        .bg-requests { background: linear-gradient(135deg,\
      \ #7986cb, #5c6bc0); } /* Indigo */\n        .bg-cpu { background: linear-gradient(135deg,\
      \ #ba68c8, #ab47bc); } /* Purple */\n        .bg-mem { background: linear-gradient(135deg,\
      \ #4dd0e1, #26c6da); } /* Cyan */\n        .bg-uptime { background: linear-gradient(135deg,\
      \ #90a4ae, #78909c); } /* Blue Grey */\n\n        .card-main-content { padding:\
      \ 15px 20px 20px 20px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); text-align:\
      \ center; flex-grow: 1; display: flex; flex-direction: column; justify-content:\
      \ center; }\n        .card-title { font-size: 0.8em; font-weight: 600; color:\
      \ rgba(255, 255, 255, 0.85); margin-bottom: 10px; text-transform: uppercase;\
      \ letter-spacing: 0.5px; }\n        .card-value { font-size: 2.3em; font-weight:\
      \ 700; line-height: 1; color: #ffffff; display: block; transition: transform\
      \ 0.2s ease; }\n\n        /* Chart Section */\n        .charts-section {\n \
      \            display: grid;\n             grid-template-columns: repeat(auto-fit,\
      \ minmax(350px, 1fr)); /* Responsive grid */\n             gap: 20px;\n    \
      \         padding: 0 25px 25px 25px; /* Padding below cards */\n        }\n\
      \        .chart-card {\n            background: #2a2d35; border-radius: 8px;\
      \ padding: 20px;\n            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.25);\n\
      \            border: 1px solid rgba(255, 255, 255, 0.08);\n            animation:\
      \ fadeIn 0.6s ease-out forwards;\n            display: flex; flex-direction:\
      \ column; /* Ensure title and canvas stack */\n        }\n         .chart-title\
      \ {\n            font-size: 1em; font-weight: 600; color: #e0e0e0; margin-bottom:\
      \ 15px; text-align: center;\n         }\n        .chart-container {\n      \
      \      height: 250px; /* Fixed height for charts */\n            position: relative;\n\
      \            flex-grow: 1; /* Allow chart container to fill space */\n     \
      \   }\n        .chart-container canvas { display: block; width: 100%; height:\
      \ 100%; }\n\n        /* Footer */\n        .app-footer { text-align: center;\
      \ padding: 15px; margin-top: auto; font-size: 0.85em; color: #888; border-top:\
      \ 1px solid #3a3d4a; background: #1f2128; }\n        .app-footer #backend-status\
      \ { font-weight: 500; display: block; margin-top: 5px; transition: color 0.3s\
      \ ease;}\n        .app-footer #backend-status.error { color: #f44336; font-weight:\
      \ bold; }\n        .app-footer #backend-status.success { color: #bdc3c7; } /*\
      \ Subtle color for success */\n\n        /* Responsive */\n        @media (max-width:\
      \ 768px) {\n            .main-content, .charts-section { padding: 15px; gap:\
      \ 15px; }\n            .app-header { padding: 8px 15px; flex-direction: column;\
      \ align-items: flex-start; }\n            .main-title { text-align: left; margin:\
      \ 8px 0; }\n            .status-indicator { align-self: flex-end; margin-top:\
      \ -25px; } /* Adjust positioning */\n            .card-value { font-size: 2em;\
      \ }\n            .charts-section { grid-template-columns: 1fr; } /* Single column\
      \ charts */\n        }\n         @media (max-width: 480px) {\n             .card-value\
      \ { font-size: 1.8em; }\n             .main-content { grid-template-columns:\
      \ 1fr 1fr; } /* 2 columns on small */\n         }\n    </style>\n    {% endraw\
      \ %}\n    {# ***** END RAW BLOCK FOR CSS ***** #}\n\n</head>\n<body>\n\n   \
      \ <header class=\"app-header\">\n         <div class=\"logo\">\n           \
      \ <svg width=\"25\" height=\"25\" viewBox=\"0 0 100 100\" fill=\"none\" xmlns=\"\
      http://www.w3.org/2000/svg\">\n                <path d=\"M8.33331 25L41.6666\
      \ 8.33331L75 25L41.6666 41.6666L8.33331 25Z\" stroke=\"currentColor\" stroke-width=\"\
      10\"/>\n                <path d=\"M8.33331 75L41.6666 58.3333L75 75L41.6666\
      \ 91.6666L8.33331 75Z\" stroke=\"currentColor\" stroke-width=\"10\"/>\n    \
      \            <path d=\"M8.33331 50H75\" stroke=\"currentColor\" stroke-width=\"\
      10\"/>\n            </svg>\n            <div>\n                <span class=\"\
      logo-text-main\">API</span>\n                <span class=\"logo-text-sub\">Metrics\
      \ Dashboard</span>\n            </div>\n        </div>\n        <h1 class=\"\
      main-title\">Live System Status</h1>\n        <div id=\"status-indicator\" class=\"\
      status-indicator stale\">Initializing...</div>\n    </header>\n\n    <main class=\"\
      main-content\" id=\"metric-cards\">\n        <!-- Cards dynamically updated\
      \ by JS - add relevant bg class -->\n        <div class=\"status-card bg-pending\"\
      \ id=\"card-pending-msgs\"><div class=\"card-main-content\"><div class=\"card-title\"\
      >Pending Msgs</div><div class=\"card-value\">--</div></div></div>\n        <div\
      \ class=\"status-card bg-processing\" id=\"card-processing-msgs\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Processing Msgs</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-failed\"\
      \ id=\"card-failed-msgs\"><div class=\"card-main-content\"><div class=\"card-title\"\
      >Failed Msgs</div><div class=\"card-value\">--</div></div></div>\n        <div\
      \ class=\"status-card bg-processed\" id=\"card-processed-msgs\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Processed Msgs</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-requests\"\
      \ id=\"card-total-requests\"><div class=\"card-main-content\"><div class=\"\
      card-title\">Total Reqs</div><div class=\"card-value\">--</div></div></div>\n\
      \        <div class=\"status-card bg-cpu\" id=\"card-process-cpu\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Process CPU %</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-mem\"\
      \ id=\"card-process-mem\"><div class=\"card-main-content\"><div class=\"card-title\"\
      >Process Mem (MB)</div><div class=\"card-value\">--</div></div></div>\n    \
      \    <div class=\"status-card bg-uptime\" id=\"card-uptime\"><div class=\"card-main-content\"\
      ><div class=\"card-title\">Uptime</div><div class=\"card-value\">--</div></div></div>\n\
      \    </main>\n\n    <section class=\"charts-section\">\n        <!-- Chart Canvases\
      \ -->\n        <div class=\"chart-card\">\n            <div class=\"chart-title\"\
      >\U0001F4C8 Requests / Interval (Last <span id=\"req-chart-points\">N</span>\
      \ points)</div>\n            <div class=\"chart-container\"><canvas id=\"requestsChart\"\
      ></canvas></div>\n        </div>\n        <div class=\"chart-card\">\n     \
      \       <div class=\"chart-title\">✉️ Message Status (Last <span id=\"msg-chart-points\"\
      >N</span> points)</div>\n            <div class=\"chart-container\"><canvas\
      \ id=\"messageStatusChart\"></canvas></div>\n        </div>\n        <div class=\"\
      chart-card\">\n            <div class=\"chart-title\">⚙️ Performance (Last <span\
      \ id=\"perf-chart-points\">N</span> points)</div>\n            <div class=\"\
      chart-container\"><canvas id=\"performanceChart\"></canvas></div>\n        </div>\n\
      \        <div class=\"chart-card\">\n            <div class=\"chart-title\"\
      >\U0001F6E3️ Requests by Route (Current Totals)</div>\n            <div class=\"\
      chart-container\"><canvas id=\"requestsByRouteChart\"></canvas></div>\n    \
      \    </div>\n         <div class=\"chart-card\">\n            <div class=\"\
      chart-title\">\U0001F6A6 Requests by Status Code (Current Totals)</div>\n  \
      \          <div class=\"chart-container\"><canvas id=\"requestsByStatusChart\"\
      ></canvas></div>\n        </div>\n    </section>\n\n    <footer class=\"app-footer\"\
      >\n        Real-time API Metrics Dashboard\n        <span id=\"backend-status\"\
      \ class=\"success\">Initializing...</span>\n    </footer>\n\n    {# ***** START\
      \ RAW BLOCK FOR JAVASCRIPT ***** #}\n    {% raw %}\n    <script>\n        //\
      \ --- CONFIGURATION (Injected by Flask) ---\n        // Use 'const' for variables\
      \ that don't change after init\n        const DASHBOARD_DATA_URL = '/api/dashboard_data';\n\
      \        const POLLING_INTERVAL_MS = {{ FETCH_INTERVAL_SECONDS * 1000 }};\n\
      \        const MAX_CHART_HISTORY = {{ MAX_CHART_HISTORY }};\n\n        // ---\
      \ GLOBAL STATE ---\n        let chartInstances = {}; // Use let as it's reassigned\
      \ during init\n        let fetchDataIntervalId = null;\n        let lastKnownError\
      \ = null; // Track the last error shown to avoid redundant updates\n\n     \
      \   // --- DOM Elements Cache (Ensure caching happens after DOM is loaded) ---\n\
      \        let statusIndicator = null;\n        let backendStatusSpan = null;\n\
      \        let cardValueElements = {};\n\n        function cacheDOMElements()\
      \ {\n            statusIndicator = document.getElementById('status-indicator');\n\
      \            backendStatusSpan = document.getElementById('backend-status');\n\
      \            cardValueElements = {\n                pendingMsgs: document.querySelector('#card-pending-msgs\
      \ .card-value'),\n                processingMsgs: document.querySelector('#card-processing-msgs\
      \ .card-value'),\n                failedMsgs: document.querySelector('#card-failed-msgs\
      \ .card-value'),\n                processedMsgs: document.querySelector('#card-processed-msgs\
      \ .card-value'),\n                totalRequests: document.querySelector('#card-total-requests\
      \ .card-value'),\n                processCpu: document.querySelector('#card-process-cpu\
      \ .card-value'),\n                processMem: document.querySelector('#card-process-mem\
      \ .card-value'),\n                uptime: document.querySelector('#card-uptime\
      \ .card-value')\n            };\n             // Update chart point display\
      \ spans dynamically based on config\n             try { // Add try-catch for\
      \ robustness if elements don't exist\n                document.getElementById('req-chart-points').textContent\
      \ = MAX_CHART_HISTORY;\n                document.getElementById('msg-chart-points').textContent\
      \ = MAX_CHART_HISTORY;\n                document.getElementById('perf-chart-points').textContent\
      \ = MAX_CHART_HISTORY;\n             } catch (e) {\n                console.warn(\"\
      Could not update chart point labels:\", e);\n             }\n        }\n\n\n\
      \        // --- UTILITY FUNCTIONS ---\n        function formatNumber(num) {\n\
      \            // Ensure input is treated as a number, return '--' if invalid\n\
      \            const number = Number(num);\n            return (num === null ||\
      \ num === undefined || isNaN(number)) ? '--' : number.toLocaleString('pt-BR');\n\
      \        }\n        function formatPercentage(num) {\n            const number\
      \ = Number(num);\n            return (num === null || num === undefined || isNaN(number))\
      \ ? '--' : number.toFixed(1) + '%';\n        }\n        function formatMemory(num)\
      \ {\n            const number = Number(num);\n            return (num === null\
      \ || num === undefined || isNaN(number)) ? '--' : number.toFixed(1) + ' MB';\n\
      \        }\n        // Updates a card's value, formats it, and adds a highlight\
      \ effect\n        function updateCardValue(element, newValue, formatter = formatNumber)\
      \ {\n            if (!element) return; // Guard against null elements\n    \
      \        const formattedValue = formatter(newValue);\n            // Only update\
      \ if the value actually changed or if it's currently '--'\n            if (element.textContent\
      \ !== formattedValue) {\n                element.textContent = formattedValue;\n\
      \                const card = element.closest('.status-card'); // Find the parent\
      \ card\n                if (card) {\n                    // Simple highlight:\
      \ quick class toggle\n                    card.classList.add('value-changed');\n\
      \                    setTimeout(() => card.classList.remove('value-changed'),\
      \ 400); // Remove after animation duration\n                    // // Reflow\
      \ method (more complex, sometimes needed for rapid changes)\n              \
      \      // card.classList.remove('value-changed');\n                    // void\
      \ card.offsetWidth; // Force reflow\n                    // card.classList.add('value-changed');\n\
      \                }\n            }\n        }\n        // Generates colors for\
      \ categorical charts, cycling through a base palette\n        function generateColors(count)\
      \ {\n             // Palette adjusted for better contrast/variety\n        \
      \     const baseColors = ['#64b5f6', '#81c784', '#ffb74d', '#e57373', '#ba68c8',\
      \ '#4dd0e1', '#fff176', '#7986cb', '#a1887f', '#90a4ae'];\n             const\
      \ colors = [];\n             for (let i = 0; i < count; i++) {\n           \
      \      colors.push(baseColors[i % baseColors.length]);\n             }\n   \
      \          return colors;\n         }\n        // Resets card values to '--'\
      \ (e.g., on initial load or error)\n         function clearCards() {\n     \
      \        Object.values(cardValueElements).forEach(el => { if(el) el.textContent\
      \ = '--'; });\n         }\n        // Function to safely update chart data,\
      \ handling potential missing datasets\n        function updateChartData(chartInstance,\
      \ newLabels, newDatasetsData) {\n            // Guard clauses\n            if\
      \ (!chartInstance || !chartInstance.data || !chartInstance.data.datasets) {\n\
      \                console.warn(\"Attempted to update non-existent or invalid\
      \ chart instance.\");\n                return;\n            }\n            if\
      \ (!Array.isArray(newLabels)) newLabels = [];\n            if (!Array.isArray(newDatasetsData))\
      \ newDatasetsData = [];\n\n            chartInstance.data.labels = newLabels;\n\
      \n            // Update each dataset present in the chart instance\n       \
      \     chartInstance.data.datasets.forEach((dataset, index) => {\n          \
      \      // Check if corresponding new data exists\n                if (newDatasetsData[index]\
      \ !== undefined && Array.isArray(newDatasetsData[index])) {\n              \
      \      dataset.data = newDatasetsData[index];\n\n                    // Update\
      \ colors dynamically ONLY for categorical charts needing it (like bar/doughnut)\n\
      \                    // Check if backgroundColor is an array (indicating categorical)\n\
      \                    if (chartInstance.config.type === 'bar' || chartInstance.config.type\
      \ === 'doughnut') {\n                       if(Array.isArray(dataset.backgroundColor))\
      \ {\n                           dataset.backgroundColor = generateColors(newDatasetsData[index].length);\n\
      \                       }\n                       // You might need similar\
      \ logic for borderColor if needed\n                    }\n                }\
      \ else {\n                    // If no new data for this dataset index, clear\
      \ it\n                    dataset.data = [];\n                    console.warn(`No\
      \ data provided for dataset index ${index} in chart. Clearing it.`);\n     \
      \           }\n            });\n\n            // Update the chart without animation\
      \ for smoother live updates\n            chartInstance.update('none');\n   \
      \     }\n\n\n        // --- CHART INITIALIZATION ---\n         function initializeCharts()\
      \ {\n             console.log(\"Initializing charts...\");\n             //\
      \ Set Chart.js defaults for better appearance\n             Chart.defaults.color\
      \ = '#e0e0e0'; // Default font color for scales, legends, tooltips\n       \
      \      Chart.defaults.borderColor = 'rgba(255, 255, 255, 0.1)'; // Default grid\
      \ line color\n\n             const defaultLineOptions = {\n                \
      \ responsive: true, maintainAspectRatio: false, // Essential for resizing\n\
      \                 animation: { duration: 250, easing: 'linear' }, // Subtle\
      \ animation\n                 plugins: {\n                     legend: {\n \
      \                        display: true, position: 'bottom',\n              \
      \           labels: { padding: 10, boxWidth: 12, font: { size: 11 } }\n    \
      \                 },\n                     tooltip: {\n                    \
      \     mode: 'index', intersect: false, // Show tooltips for all datasets at\
      \ that index\n                         backgroundColor: 'rgba(0,0,0,0.8)', titleFont:\
      \ { weight: 'bold' },\n                         bodySpacing: 4, padding: 8,\
      \ boxPadding: 4\n                     }\n                 },\n             \
      \    scales: {\n                     x: {\n                         ticks: {\
      \ maxRotation: 0, autoSkip: true, maxTicksLimit: 10, font: { size: 10 } },\n\
      \                         grid: { display: true } // Keep X grid lines subtle\n\
      \                     },\n                     y: { // Default Y axis (can be\
      \ overridden)\n                         ticks: { beginAtZero: true, font: {\
      \ size: 10 }, precision: 0 }, // Default to whole numbers if possible\n    \
      \                     grid: { display: true, color: 'rgba(255, 255, 255, 0.08)'\
      \ } // Lighter Y grid\n                     }\n                 },\n       \
      \          elements: {\n                     line: { tension: 0.2, borderWidth:\
      \ 1.5 }, // Slight curve, standard width\n                     point: { radius:\
      \ 0, hitRadius: 10, hoverRadius: 4 } // No points normally, larger hit area\n\
      \                 },\n                 interaction: { // Improve hover interaction\n\
      \                     mode: 'nearest', axis: 'x', intersect: false\n       \
      \          }\n             };\n\n             // Requests Chart (Line)\n   \
      \          const reqCtx = document.getElementById('requestsChart')?.getContext('2d');\n\
      \             if (reqCtx) {\n                 const reqOptions = JSON.parse(JSON.stringify(defaultLineOptions));\
      \ // Clone options\n                 chartInstances.requests = new Chart(reqCtx,\
      \ {\n                     type: 'line',\n                     data: { labels:\
      \ [], datasets: [{\n                         label: 'Requests/Interval', data:\
      \ [],\n                         borderColor: '#64b5f6', backgroundColor: 'rgba(100,\
      \ 181, 246, 0.2)', fill: true\n                     }] },\n                \
      \     options: reqOptions\n                 });\n             } else { console.error(\"\
      Canvas element #requestsChart not found\"); }\n\n             // Message Status\
      \ Chart (Line)\n             const msgCtx = document.getElementById('messageStatusChart')?.getContext('2d');\n\
      \              if (msgCtx) {\n                 const msgOptions = JSON.parse(JSON.stringify(defaultLineOptions));\
      \ // Clone options\n                 msgOptions.elements.line.borderWidth =\
      \ 2; // Make lines slightly thicker\n                 chartInstances.messageStatus\
      \ = new Chart(msgCtx, {\n                     type: 'line',\n              \
      \       data: { labels: [], datasets: [\n                         { label: 'Pending',\
      \ data: [], borderColor: '#ffb74d', fill: false }, // Orange\n             \
      \            { label: 'Processing', data: [], borderColor: '#64b5f6', fill:\
      \ false }, // Blue\n                         { label: 'Failed', data: [], borderColor:\
      \ '#e57373', fill: false },    // Red\n                         { label: 'Processed',\
      \ data: [], borderColor: '#81c784', fill: false }  // Green\n              \
      \       ] },\n                     options: msgOptions\n                 });\n\
      \             } else { console.error(\"Canvas element #messageStatusChart not\
      \ found\"); }\n\n             // Performance Chart (Line with Multi-Axis)\n\
      \             const perfCtx = document.getElementById('performanceChart')?.getContext('2d');\n\
      \              if (perfCtx) {\n                 const perfOptions = JSON.parse(JSON.stringify(defaultLineOptions));\
      \ // Clone options\n                 // Define specific Y axes\n           \
      \      perfOptions.scales.yCpu = { // Use unique IDs\n                     type:\
      \ 'linear', position: 'left', title: { display: true, text: 'CPU (%)', color:\
      \ '#ba68c8' },\n                     ticks: { color: '#ba68c8', suggestedMax:\
      \ 100, beginAtZero: true, precision: 1 }, // Allow decimals\n              \
      \       grid: { drawOnChartArea: true } // Primary axis grid\n             \
      \    };\n                 perfOptions.scales.yMem = { // Use unique IDs\n  \
      \                   type: 'linear', position: 'right', title: { display: true,\
      \ text: 'Memory (MB)', color: '#4dd0e1' },\n                     ticks: { color:\
      \ '#4dd0e1', beginAtZero: true, precision: 1 }, // Allow decimals\n        \
      \             grid: { drawOnChartArea: false }, // No grid for secondary axis\n\
      \                 };\n                 delete perfOptions.scales.y; // Remove\
      \ the default 'y' scale\n\n                 chartInstances.performance = new\
      \ Chart(perfCtx, {\n                     type: 'line',\n                   \
      \  data: { labels: [], datasets: [\n                         { label: 'Process\
      \ CPU %', data: [], borderColor: '#ba68c8', fill: false, yAxisID: 'yCpu' },\
      \ // Purple\n                         { label: 'Process Mem (MB)', data: [],\
      \ borderColor: '#4dd0e1', fill: false, yAxisID: 'yMem' } // Cyan\n         \
      \            ] },\n                     options: perfOptions\n             \
      \    });\n             } else { console.error(\"Canvas element #performanceChart\
      \ not found\"); }\n\n             // Default options for Bar/Doughnut\n    \
      \         const defaultCategoricalOptions = {\n                 responsive:\
      \ true, maintainAspectRatio: false,\n                 plugins: {\n         \
      \            legend: { display: true, position: 'bottom', labels: { padding:\
      \ 10, boxWidth: 12, font: { size: 11 } } },\n                     tooltip: {\
      \ backgroundColor: 'rgba(0,0,0,0.8)', titleFont: { weight: 'bold' }, bodySpacing:\
      \ 4, padding: 8, boxPadding: 4 }\n                 }\n             };\n\n  \
      \           // Requests by Route Chart (Horizontal Bar)\n             const\
      \ routeCtx = document.getElementById('requestsByRouteChart')?.getContext('2d');\n\
      \             if (routeCtx) {\n                 const routeOptions = JSON.parse(JSON.stringify(defaultCategoricalOptions));\
      \ // Clone options\n                 routeOptions.indexAxis = 'y'; // Make bars\
      \ horizontal\n                 routeOptions.plugins.legend.display = false;\
      \ // Hide legend (often too many items)\n                 routeOptions.scales\
      \ = {\n                     x: { ticks: { precision: 0, beginAtZero: true },\
      \ grid: { color: 'rgba(255,255,255,0.08)' } }, // X axis = count\n         \
      \            y: { ticks: { font: { size: 10 } }, grid: { display: false } }\
      \ // Y axis = route name\n                 };\n                 chartInstances.requestsByRoute\
      \ = new Chart(routeCtx, {\n                     type: 'bar',\n             \
      \        data: { labels: [], datasets: [{ label: 'Count', data: [], backgroundColor:\
      \ [] }] }, // Colors generated dynamically\n                     options: routeOptions\n\
      \                 });\n             } else { console.error(\"Canvas element\
      \ #requestsByRouteChart not found\"); }\n\n             // Requests by Status\
      \ Chart (Doughnut)\n             const statusCtx = document.getElementById('requestsByStatusChart')?.getContext('2d');\n\
      \              if (statusCtx) {\n                 const statusOptions = JSON.parse(JSON.stringify(defaultCategoricalOptions));\n\
      \                 statusOptions.plugins.legend.position = 'right'; // Position\
      \ legend for doughnut\n                 chartInstances.requestsByStatus = new\
      \ Chart(statusCtx, {\n                     type: 'doughnut',\n             \
      \        data: { labels: [], datasets: [{\n                         label: 'Count',\
      \ data: [], backgroundColor: [], borderWidth: 1, hoverOffset: 8\n          \
      \           }] }, // Colors generated dynamically\n                     options:\
      \ statusOptions\n                 });\n             } else { console.error(\"\
      Canvas element #requestsByStatusChart not found\"); }\n\n             console.log(\"\
      Charts initialized.\");\n         }\n\n\n        // --- DATA FETCHING AND PROCESSING\
      \ ---\n        async function fetchData() {\n            // Indicate fetching\
      \ visually, but only if not already showing an error\n            if (statusIndicator\
      \ && !statusIndicator.classList.contains('error')) {\n                 statusIndicator.textContent\
      \ = 'Fetching...';\n                 statusIndicator.className = 'status-indicator\
      \ fetching';\n            }\n            console.debug(`[${new Date().toLocaleTimeString()}]\
      \ Fetching data from ${DASHBOARD_DATA_URL}`);\n\n            try {\n       \
      \         const response = await fetch(DASHBOARD_DATA_URL);\n\n            \
      \    // Handle HTTP errors from the dashboard server itself\n              \
      \  if (!response.ok) {\n                    let errorMsg = `Error fetching dashboard\
      \ data: ${response.status} ${response.statusText}`;\n                    try\
      \ {\n                        // Attempt to get more detail from the response\
      \ body\n                        const errorData = await response.json();\n \
      \                       errorMsg += ` - ${errorData.error || JSON.stringify(errorData)}`;\n\
      \                    } catch (e) { /* Ignore if response body is not JSON or\
      \ empty */ }\n                    throw new Error(errorMsg); // Throw to be\
      \ caught by the outer catch block\n                }\n\n                const\
      \ data = await response.json();\n                // console.debug(\"Raw dashboard\
      \ data received:\", JSON.stringify(data, null, 2)); // Verbose logging if needed\n\
      \n                // --- Process the received data ---\n                // Check\
      \ if the dashboard backend reported an error *during its API fetch*\n      \
      \          if (data.error) {\n                    if (lastKnownError !== data.error)\
      \ { // Avoid spamming the same error\n                        console.error(\"\
      Dashboard backend reported API fetch error:\", data.error);\n              \
      \          if (statusIndicator) {\n                            statusIndicator.textContent\
      \ = 'API Error';\n                            statusIndicator.className = 'status-indicator\
      \ error';\n                        }\n                        if (backendStatusSpan)\
      \ {\n                            backendStatusSpan.textContent = `API Error:\
      \ ${data.error}`;\n                            backendStatusSpan.className =\
      \ 'error';\n                        }\n                        lastKnownError\
      \ = data.error;\n                        // Decide whether to clear data or\
      \ leave stale data visible\n                        // clearCards(); // Option:\
      \ Clear cards on backend error\n                    }\n                // Check\
      \ if the stats object is missing or empty\n                } else if (!data.latest_stats\
      \ || Object.keys(data.latest_stats).length === 0) {\n                    if\
      \ (lastKnownError !== \"Empty stats data\") { // Avoid spamming\n          \
      \              console.warn(\"Dashboard backend returned empty 'latest_stats'\
      \ data.\");\n                        if (statusIndicator) {\n              \
      \              statusIndicator.textContent = 'No Data';\n                  \
      \          statusIndicator.className = 'status-indicator stale';\n         \
      \               }\n                         const fetchTime = data.last_successful_fetch\
      \ ? new Date(data.last_successful_fetch).toLocaleString('pt-BR') : 'Never';\n\
      \                        if (backendStatusSpan) {\n                        \
      \    backendStatusSpan.textContent = `No stats received. Last API fetch: ${fetchTime}`;\n\
      \                            backendStatusSpan.className = 'error'; // Style\
      \ as error/warning\n                        }\n                        lastKnownError\
      \ = \"Empty stats data\";\n                        clearCards(); // Clear cards\
      \ if no valid data received\n                        // Optionally clear charts\
      \ too\n                        Object.values(chartInstances).forEach(chart =>\
      \ updateChartData(chart, [], []));\n                    }\n                }\
      \ else {\n                    // --- Success Case: Valid data received ---\n\
      \                    if (statusIndicator) {\n                        statusIndicator.textContent\
      \ = 'Live';\n                        statusIndicator.className = 'status-indicator\
      \ live';\n                    }\n                    const fetchTime = data.last_successful_fetch\
      \ ? new Date(data.last_successful_fetch).toLocaleString('pt-BR') : 'Never';\n\
      \                    if (backendStatusSpan) {\n                        backendStatusSpan.textContent\
      \ = `Last API fetch: ${fetchTime}`;\n                        backendStatusSpan.className\
      \ = 'success';\n                    }\n                    updateDashboardUI(data);\
      \ // Update UI with fresh data\n                    lastKnownError = null; //\
      \ Clear the tracked error on success\n                }\n\n            } catch\
      \ (error) {\n                // --- Error fetching data *from the dashboard\
      \ server itself* ---\n                if (lastKnownError !== error.message)\
      \ { // Avoid spamming\n                    console.error(\"Error fetching or\
      \ processing dashboard data from /api/dashboard_data:\", error);\n         \
      \           if (statusIndicator) {\n                        statusIndicator.textContent\
      \ = 'Dashboard Error';\n                        statusIndicator.className =\
      \ 'status-indicator error';\n                    }\n                    if (backendStatusSpan)\
      \ {\n                        backendStatusSpan.textContent = `Dashboard Fetch\
      \ Error: ${error.message}`;\n                        backendStatusSpan.className\
      \ = 'error';\n                    }\n                    lastKnownError = error.message;\n\
      \                    clearCards(); // Clear cards on dashboard fetch error\n\
      \                    // Optionally clear charts too\n                    Object.values(chartInstances).forEach(chart\
      \ => updateChartData(chart, [], []));\n                 }\n            }\n \
      \       }\n\n        // --- UI UPDATE FUNCTION ---\n        function updateDashboardUI(data)\
      \ {\n            // Basic validation already done in fetchData, but double-check\
      \ core objects\n            if (!data?.latest_stats || !data?.history) {\n \
      \                console.warn(\"updateDashboardUI called with incomplete data\
      \ structure. Aborting UI update.\");\n                 return;\n           \
      \ }\n\n            const stats = data.latest_stats;\n            const history\
      \ = data.history;\n            const timeLabels = history.time_labels || [];\
      \ // Use default empty array\n\n            // console.debug(\"Updating UI with\
      \ stats:\", stats); // Uncomment for detailed debug\n\n            // --- Update\
      \ Cards ---\n            updateCardValue(cardValueElements.pendingMsgs, stats.messages_pending);\n\
      \            updateCardValue(cardValueElements.processingMsgs, stats.messages_processing);\n\
      \            updateCardValue(cardValueElements.failedMsgs, stats.messages_failed);\n\
      \            updateCardValue(cardValueElements.processedMsgs, stats.messages_processed);\n\
      \            updateCardValue(cardValueElements.totalRequests, stats.requests_total);\n\
      \            // Use optional chaining (?.) and nullish coalescing (??) for safer\
      \ access\n            updateCardValue(cardValueElements.processCpu, stats.system?.process_cpu_percent\
      \ ?? null, formatPercentage);\n            updateCardValue(cardValueElements.processMem,\
      \ stats.system?.process_memory_mb ?? null, formatMemory);\n            updateCardValue(cardValueElements.uptime,\
      \ stats.uptime_human, (val) => val || '--');\n\n            // --- Update Line\
      \ Charts ---\n            updateChartData(chartInstances.requests, timeLabels,\
      \ [\n                history.request_history || []\n            ]);\n      \
      \      updateChartData(chartInstances.messageStatus, timeLabels, [\n       \
      \         history.message_status?.pending || [],\n                history.message_status?.processing\
      \ || [],\n                history.message_status?.failed || [],\n          \
      \      history.message_status?.processed || []\n            ]);\n          \
      \   updateChartData(chartInstances.performance, timeLabels, [\n            \
      \    history.performance?.cpu || [],\n                history.performance?.memory\
      \ || []\n             ]);\n\n            // --- Update Categorical Charts (Bar\
      \ and Doughnut) ---\n            // Requests by Route (Horizontal Bar)\n   \
      \         if (chartInstances.requestsByRoute) {\n                const routes\
      \ = stats.requests_by_route || {};\n                // Sort routes alphabetically\
      \ for consistent order\n                const routeLabels = Object.keys(routes).sort();\n\
      \                const routeData = routeLabels.map(route => {\n            \
      \        // Sum counts for all methods under this route path\n             \
      \       const methods = routes[route] || {};\n                    return Object.values(methods).reduce((sum,\
      \ count) => sum + (Number(count) || 0), 0);\n                });\n         \
      \        updateChartData(chartInstances.requestsByRoute, routeLabels, [routeData]);\n\
      \            }\n\n             // Requests by Status (Doughnut)\n          \
      \   if (chartInstances.requestsByStatus) {\n                const statuses =\
      \ stats.requests_by_status || {};\n                // Sort status codes numerically\
      \ for logical chart order\n                const statusLabels = Object.keys(statuses).sort((a,\
      \ b) => Number(a) - Number(b));\n                const statusData = statusLabels.map(status\
      \ => statuses[status] || 0);\n                updateChartData(chartInstances.requestsByStatus,\
      \ statusLabels, [statusData]);\n            }\n            // console.debug(\"\
      UI update complete.\"); // Uncomment for detailed debug\n        }\n\n     \
      \   // --- Initialization ---\n        document.addEventListener('DOMContentLoaded',\
      \ () => {\n            console.log(\"DOM Loaded. Initializing dashboard.\");\n\
      \            cacheDOMElements(); // Cache elements now that DOM is ready\n \
      \           initializeCharts(); // Setup chart structures\n            clearCards();\
      \ // Set initial card values to '--'\n            fetchData(); // Perform the\
      \ first data fetch immediately\n\n            // Start polling for new data\
      \ after the first fetch attempt\n            if (fetchDataIntervalId) clearInterval(fetchDataIntervalId);\
      \ // Clear previous interval if any\n            fetchDataIntervalId = setInterval(fetchData,\
      \ POLLING_INTERVAL_MS);\n            console.log(`Started polling data every\
      \ ${POLLING_INTERVAL_MS / 1000} seconds.`);\n        });\n\n    </script>\n\
      \    {% endraw %}\n    {# ***** END RAW BLOCK FOR JAVASCRIPT ***** #}\n\n</body>\n\
      </html>\n\"\"\"\n\n\n# --- Flask Routes ---\n\n@app.route('/')\ndef serve_dashboard():\n\
      \    \"\"\"Serve the main dashboard HTML page, rendering the template string.\"\
      \"\"\n    logger.info(\"Serving dashboard HTML page.\")\n    try:\n        #\
      \ Render the HTML, injecting configuration variables into the JS template sections\n\
      \        return render_template_string(\n            HTML_CONTENT,\n       \
      \     FETCH_INTERVAL_SECONDS=FETCH_INTERVAL_SECONDS,\n            MAX_CHART_HISTORY=MAX_CHART_HISTORY\n\
      \        )\n    except Exception as e:\n        # Catch potential Jinja errors\
      \ during rendering itself\n        logger.error(f\"Error rendering dashboard\
      \ template: {e}\", exc_info=True)\n        # Return a simple error page if template\
      \ rendering fails\n        return f\"<h1>Internal Server Error</h1><p>Failed\
      \ to render dashboard template: {e}</p>\", 500\n\n@app.route('/api/dashboard_data')\n\
      def get_dashboard_data():\n    \"\"\"Endpoint for the frontend JavaScript to\
      \ fetch the collected data.\"\"\"\n    logger.debug(\"Request received for /api/dashboard_data\"\
      )\n    with data_lock:\n        # Create a snapshot of the current state to\
      \ avoid holding the lock during serialization\n        # Ensure all deques are\
      \ converted to lists for JSON compatibility\n        try:\n            data_to_send\
      \ = {\n                # Shallow copy is usually fine for dicts of primitives/strings/numbers\n\
      \                \"latest_stats\": app_state.get(\"latest_stats\", {}).copy(),\n\
      \                \"history\": {\n                    \"time_labels\": list(app_state.get(\"\
      time_labels\", [])),\n                    \"request_history\": list(app_state.get(\"\
      request_history\", [])),\n                    \"message_status\": {\n      \
      \                  \"pending\": list(app_state.get(\"message_status_history\"\
      , {}).get(\"pending\", [])),\n                        \"processing\": list(app_state.get(\"\
      message_status_history\", {}).get(\"processing\", [])),\n                  \
      \      \"failed\": list(app_state.get(\"message_status_history\", {}).get(\"\
      failed\", [])),\n                        \"processed\": list(app_state.get(\"\
      message_status_history\", {}).get(\"processed\", [])),\n                   \
      \ },\n                    \"performance\": {\n                        \"cpu\"\
      : list(app_state.get(\"performance_history\", {}).get(\"cpu\", [])),\n     \
      \                   \"memory\": list(app_state.get(\"performance_history\",\
      \ {}).get(\"memory\", [])),\n                    }\n                },\n   \
      \             \"last_successful_fetch\": app_state.get(\"last_successful_fetch\"\
      ),\n                \"error\": app_state.get(\"last_error\") # Pass the last\
      \ known error (if any)\n            }\n        except Exception as e:\n    \
      \        logger.error(f\"Error preparing data for /api/dashboard_data: {e}\"\
      , exc_info=True)\n            return jsonify({\"error\": \"Failed to prepare\
      \ data\", \"detail\": str(e)}), 500\n\n    # logger.debug(f\"Returning dashboard\
      \ data: {json.dumps(data_to_send)}\") # Be careful logging large data\n    return\
      \ jsonify(data_to_send)\n\n# --- Inicialização ---\nif __name__ == '__main__':\n\
      \    # Disable warnings for insecure HTTPS requests (verify=False) made by this\
      \ script\n    # ONLY use this in development with self-signed certs you trust.\n\
      \    try:\n        import urllib3\n        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\
      \        logger.warning(\"SSL certificate verification is disabled for API requests\
      \ made by this dashboard. THIS IS INSECURE FOR PRODUCTION.\")\n    except ImportError:\n\
      \        logger.warning(\"urllib3 not found, cannot disable InsecureRequestWarning.\"\
      )\n    except Exception as e:\n        logger.warning(f\"Could not disable urllib3\
      \ warnings: {e}\")\n\n    # Start the scheduler thread in the background\n \
      \   # daemon=True ensures the thread exits when the main Flask process exits\n\
      \    scheduler_thread = threading.Thread(target=run_scheduler, name=\"SchedulerThread\"\
      , daemon=True)\n    scheduler_thread.start()\n\n    logger.info(f\"Starting\
      \ Dashboard server on http://0.0.0.0:{DASHBOARD_PORT}\")\n    logger.info(f\"\
      Attempting to fetch data from API at {API_BASE_URL} every {FETCH_INTERVAL_SECONDS}\
      \ seconds.\")\n\n    # Run the Flask app\n    # Use debug=False in production\
      \ to avoid security risks and duplicate scheduler runs\n    # Disable reloader\
      \ when using threads to avoid issues.\n    # Consider using a production-ready\
      \ WSGI server like Gunicorn or Waitress.\n    # Example using Waitress (install\
      \ first: pip install waitress):\n    # from waitress import serve\n    # serve(app,\
      \ host='0.0.0.0', port=DASHBOARD_PORT)\n    try:\n        logger.info(\"Using\
      \ Flask's built-in development server. Not recommended for production.\")\n\
      \        app.run(host='0.0.0.0', port=DASHBOARD_PORT, debug=False, use_reloader=False)\n\
      \    except KeyboardInterrupt:\n        logger.info(\"Dashboard server stopped\
      \ by user (Ctrl+C).\")\n    except Exception as e:\n        logger.critical(f\"\
      Dashboard server failed to start or crashed: {e}\", exc_info=True)"
    tamanho: 0.06 MB
  webdashv2-clean.py:
    caminho_completo: .\webdashv2-clean.py
    classes: []
    functions:
    - docstring: Attempts to log in to the main API and stores the tokens.
      end_lineno: 125
      lineno: 67
      name: login_to_api
    - docstring: Fetches /stats data from the main API, handles authentication.
      end_lineno: 244
      lineno: 127
      name: fetch_api_data
    - docstring: Runs the scheduler loop in a separate thread.
      end_lineno: 258
      lineno: 247
      name: run_scheduler
    - docstring: Serve the main dashboard HTML page.
      end_lineno: 636
      lineno: 624
      name: serve_dashboard
    - docstring: Endpoint for frontend JS to fetch collected data.
      end_lineno: 659
      lineno: 639
      name: get_dashboard_data
    imports:
    - asname: null
      name: os
    - asname: null
      name: time
    - asname: null
      name: threading
    - asname: null
      name: logging
    - module: collections
      names:
      - deque
    - module: threading
      names:
      - Lock
    - module: datetime
      names:
      - datetime
      - timezone
    - asname: null
      name: json
    - asname: null
      name: requests
    - asname: null
      name: schedule
    - module: flask
      names:
      - Flask
      - Response
      - jsonify
      - render_template_string
    - module: flask_cors
      names:
      - CORS
    - asname: null
      name: urllib3
    numero_de_linhas: 687
    source_code: "# dashboard_server.py\nimport os\nimport time\nimport threading\n\
      import logging\nfrom collections import deque\nfrom threading import Lock\n\
      from datetime import datetime, timezone\nimport json # For handling potential\
      \ decoding errors\n\nimport requests # To make requests to the main API\nimport\
      \ schedule # To schedule data collection\nfrom flask import Flask, Response,\
      \ jsonify, render_template_string\nfrom flask_cors import CORS\n\n# --- Configuration\
      \ ---\nDASHBOARD_PORT = 8333\n# Default to HTTPS and the simplified API port\n\
      API_BASE_URL = os.environ.get(\"API_BASE_URL\", \"https://127.0.0.1:8777\")\n\
      API_STATS_URL = f\"{API_BASE_URL}/stats\"\nAPI_LOGIN_URL = f\"{API_BASE_URL}/login\"\
      \n\n# Credentials for the dashboard to access the main API\n# !!! Use environment\
      \ variables in production !!!\nAPI_USERNAME = os.environ.get(\"API_USER\", \"\
      admin\") # Default 'admin'\nAPI_PASSWORD = os.environ.get(\"API_PASS\", \"admin\"\
      ) # Default 'admin'\n\nFETCH_INTERVAL_SECONDS = 5 # Data collection interval\n\
      MAX_CHART_HISTORY = 60 # History points for line charts\n\n# --- Logging Configuration\
      \ ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s\
      \ - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n\
      )\nlogger = logging.getLogger('DashboardServer') # Specific name\n\n# --- Global\
      \ State and Control ---\napp_state = {\n    \"latest_stats\": {},\n    \"last_error\"\
      : None,\n    \"last_successful_fetch\": None,\n    \"api_token\": None, # Store\
      \ only the access token\n    \"refresh_token\": None, # Kept for potential future\
      \ use if API adds refresh logic\n    \"login_needed\": True,\n    \"is_fetching\"\
      : False,\n    # History for charts\n    \"time_labels\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \    \"request_history\": deque(maxlen=MAX_CHART_HISTORY), # Deltas per interval\n\
      \    \"message_status_history\": {\n        \"pending\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \        \"processing\": deque(maxlen=MAX_CHART_HISTORY),\n        \"failed\"\
      : deque(maxlen=MAX_CHART_HISTORY),\n        \"processed\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \    },\n    \"performance_history\": {\n        \"cpu\": deque(maxlen=MAX_CHART_HISTORY),\n\
      \        \"memory\": deque(maxlen=MAX_CHART_HISTORY),\n    },\n    \"previous_total_requests\"\
      : 0 # For calculating delta\n}\ndata_lock = Lock() # Protect concurrent access\
      \ to app_state\n\n# --- Collection and Processing Functions ---\n\ndef login_to_api():\n\
      \    \"\"\"Attempts to log in to the main API and stores the tokens.\"\"\"\n\
      \    global app_state\n    logger.info(f\"Attempting login to API at {API_LOGIN_URL}...\"\
      )\n    try:\n        login_data = {'username': API_USERNAME, 'password': API_PASSWORD}\n\
      \        # IMPORTANT: Disable SSL verification ONLY for local dev with self-signed\
      \ certs\n        response = requests.post(API_LOGIN_URL, data=login_data, verify=False,\
      \ timeout=10)\n        response.raise_for_status() # Raise exception for HTTP\
      \ 4xx/5xx errors\n\n        token_data = response.json()\n        if \"access_token\"\
      \ in token_data and \"refresh_token\" in token_data:\n            with data_lock:\n\
      \                app_state[\"api_token\"] = token_data[\"access_token\"]\n \
      \               app_state[\"refresh_token\"] = token_data[\"refresh_token\"\
      ]\n                app_state[\"login_needed\"] = False\n                app_state[\"\
      last_error\"] = None # Clear previous login error\n            logger.info(\"\
      API login successful.\")\n            return True\n        else:\n         \
      \   logger.error(\"API login response missing 'access_token' or 'refresh_token'.\"\
      )\n            with data_lock:\n                app_state[\"last_error\"] =\
      \ \"Login response missing tokens\"\n                app_state[\"api_token\"\
      ] = None\n                app_state[\"refresh_token\"] = None\n            \
      \    app_state[\"login_needed\"] = True\n            return False\n\n    except\
      \ requests.exceptions.RequestException as e:\n        status_code = getattr(e.response,\
      \ 'status_code', 'N/A')\n        error_detail = f\"Status: {status_code}\"\n\
      \        try: # Try to get more details\n            if e.response is not None:\n\
      \                content_type = e.response.headers.get('Content-Type', '')\n\
      \                if 'application/json' in content_type:\n                  \
      \ error_json = e.response.json()\n                   error_detail += f\" - Detail:\
      \ {error_json.get('detail', error_json)}\"\n                else: error_detail\
      \ += f\" - Response: {e.response.text[:200]}\"\n        except Exception: pass\n\
      \        logger.error(f\"API login failed ({error_detail}): {e}\")\n       \
      \ with data_lock:\n            app_state[\"last_error\"] = f\"Login request\
      \ failed: {e}\"\n            app_state[\"api_token\"] = None\n            app_state[\"\
      refresh_token\"] = None\n            app_state[\"login_needed\"] = True\n  \
      \      return False\n    except json.JSONDecodeError as e:\n        response_text\
      \ = getattr(e.response, 'text', 'N/A')\n        logger.error(f\"API login failed:\
      \ Could not decode JSON. Response: {response_text[:500]}\")\n        with data_lock:\n\
      \            app_state[\"last_error\"] = \"Login response not valid JSON\"\n\
      \            app_state[\"api_token\"] = None; app_state[\"refresh_token\"] =\
      \ None; app_state[\"login_needed\"] = True\n        return False\n    except\
      \ Exception as e:\n        logger.error(f\"Unexpected error during API login:\
      \ {e}\", exc_info=True)\n        with data_lock:\n            app_state[\"last_error\"\
      ] = f\"Unexpected login error: {e}\"\n            app_state[\"api_token\"] =\
      \ None; app_state[\"refresh_token\"] = None; app_state[\"login_needed\"] = True\n\
      \        return False\n\ndef fetch_api_data():\n    \"\"\"Fetches /stats data\
      \ from the main API, handles authentication.\"\"\"\n    global app_state\n\n\
      \    with data_lock: # Prevent concurrent fetches\n        if app_state.get(\"\
      is_fetching\", False):\n            logger.debug(\"Fetch skipped, already fetching.\"\
      )\n            return\n        app_state[\"is_fetching\"] = True\n        token\
      \ = app_state[\"api_token\"]\n        login_needed = app_state[\"login_needed\"\
      ]\n\n    logger.debug(\"Starting data fetch cycle...\")\n    try:\n        #\
      \ --- Handle Authentication ---\n        if login_needed or not token:\n   \
      \         logger.warning(\"Login required or token missing, attempting login...\"\
      )\n            if not login_to_api():\n                logger.error(\"Fetch\
      \ cycle aborted: login failed.\")\n                with data_lock:\n       \
      \              if app_state[\"last_error\"] is None: app_state[\"last_error\"\
      ] = \"Login required but failed\"\n                return\n            with\
      \ data_lock: # Re-fetch token after successful login\n                token\
      \ = app_state[\"api_token\"]\n                if not token: # Should not happen\
      \ if login_to_api returned True\n                     logger.error(\"CRITICAL:\
      \ Token missing after successful login. Aborting.\")\n                     app_state[\"\
      last_error\"] = \"Internal dashboard error: Token lost.\"\n                \
      \     return\n\n        # --- Fetch Stats Data ---\n        logger.debug(f\"\
      Fetching stats from {API_STATS_URL}...\")\n        headers = {'Authorization':\
      \ f'Bearer {token}', 'Accept': 'application/json'}\n        try:\n         \
      \   # Disable SSL verification ONLY for local dev\n            response = requests.get(API_STATS_URL,\
      \ headers=headers, verify=False, timeout=10)\n\n            # Check for auth\
      \ errors first -> triggers re-login next cycle\n            if response.status_code\
      \ in [401, 403]:\n                logger.warning(f\"API auth error ({response.status_code})\
      \ fetching stats. Forcing re-login next cycle.\")\n                with data_lock:\n\
      \                    app_state[\"api_token\"] = None\n                    app_state[\"\
      login_needed\"] = True\n                    app_state[\"last_error\"] = f\"\
      API Auth error ({response.status_code}). Re-login needed.\"\n              \
      \  return # Abort this cycle\n\n            response.raise_for_status() # Check\
      \ for other HTTP errors (5xx, 404 etc.)\n\n            # ---- Process Successful\
      \ Response ----\n            try:\n                stats = response.json()\n\
      \                now = datetime.now(timezone.utc)\n                logger.debug(\"\
      Stats received successfully.\")\n\n                with data_lock: # Update\
      \ state under lock\n                    app_state[\"latest_stats\"] = stats\n\
      \                    app_state[\"last_successful_fetch\"] = now.isoformat()\n\
      \                    app_state[\"last_error\"] = None # Clear errors on success\n\
      \n                    # Update history deques\n                    current_time_label\
      \ = now.strftime(\"%H:%M:%S\")\n                    app_state[\"time_labels\"\
      ].append(current_time_label)\n\n                    # Calculate request delta\n\
      \                    current_total = stats.get(\"requests_total\")\n       \
      \             delta = 0\n                    if isinstance(current_total, int):\n\
      \                        prev_total = app_state[\"previous_total_requests\"\
      ]\n                        delta = max(0, current_total - (prev_total if isinstance(prev_total,\
      \ int) else 0))\n                        app_state[\"previous_total_requests\"\
      ] = current_total\n                    else: logger.warning(\"`requests_total`\
      \ missing or invalid in stats.\")\n                    app_state[\"request_history\"\
      ].append(delta)\n\n                    # Message history (safe access with .get)\n\
      \                    app_state[\"message_status_history\"][\"pending\"].append(stats.get(\"\
      messages_pending\", 0))\n                    app_state[\"message_status_history\"\
      ][\"processing\"].append(stats.get(\"messages_processing\", 0))\n          \
      \          app_state[\"message_status_history\"][\"failed\"].append(stats.get(\"\
      messages_failed\", 0))\n                    app_state[\"message_status_history\"\
      ][\"processed\"].append(stats.get(\"messages_processed\", 0))\n\n          \
      \          # Performance history (safe access)\n                    sys_stats\
      \ = stats.get(\"system\", {})\n                    cpu = sys_stats.get(\"process_cpu_percent\"\
      )\n                    mem = sys_stats.get(\"process_memory_mb\")\n        \
      \            app_state[\"performance_history\"][\"cpu\"].append(cpu if isinstance(cpu,\
      \ (int, float)) else 0)\n                    app_state[\"performance_history\"\
      ][\"memory\"].append(mem if isinstance(mem, (int, float)) else 0)\n\n      \
      \          logger.debug(\"Dashboard state updated.\")\n\n            except\
      \ json.JSONDecodeError as e:\n                 logger.error(f\"Failed to decode\
      \ JSON from API /stats: {e}. Response: {response.text[:500]}\")\n          \
      \       with data_lock: app_state[\"last_error\"] = \"API /stats response not\
      \ valid JSON\"\n            except Exception as processing_e:\n            \
      \    logger.error(f\"Error processing received stats: {processing_e}\", exc_info=True)\n\
      \                with data_lock: app_state[\"last_error\"] = f\"Error processing\
      \ stats: {processing_e}\"\n\n        # --- Handle Request Errors ---\n     \
      \   except requests.exceptions.Timeout:\n            logger.error(\"API /stats\
      \ request timed out.\")\n            with data_lock: app_state[\"last_error\"\
      ] = \"API Timeout fetching stats\"\n        except requests.exceptions.ConnectionError\
      \ as e:\n            logger.error(f\"API /stats connection error: {e}\")\n \
      \           with data_lock: app_state[\"last_error\"] = f\"API Connection Error:\
      \ {e}\"\n        except requests.exceptions.RequestException as e:\n       \
      \     status_code = getattr(e.response, 'status_code', 'N/A')\n            logger.error(f\"\
      API /stats request failed (Status: {status_code}): {e}\")\n            error_detail\
      \ = f\"API Request Failed: {e}\"\n            try:\n                if e.response\
      \ is not None: error_detail += f\" - Response: {e.response.text[:200]}\"\n \
      \           except Exception: pass\n            with data_lock:\n          \
      \       # Avoid overwriting a more specific auth error\n                 if\
      \ app_state.get(\"last_error\") is None or \"Auth error\" not in app_state[\"\
      last_error\"]:\n                     app_state[\"last_error\"] = error_detail\n\
      \n    except Exception as outer_e:\n        logger.error(f\"Unexpected error\
      \ during fetch setup: {outer_e}\", exc_info=True)\n        with data_lock: app_state[\"\
      last_error\"] = f\"Unexpected Fetch Error: {outer_e}\"\n    finally:\n     \
      \   with data_lock: app_state[\"is_fetching\"] = False # Ensure flag is reset\n\
      \n\ndef run_scheduler():\n    \"\"\"Runs the scheduler loop in a separate thread.\"\
      \"\"\n    logger.info(\"Scheduler thread started.\")\n    try: # Perform an\
      \ initial fetch immediately\n        fetch_api_data()\n    except Exception\
      \ as e: logger.error(f\"Error during initial fetch: {e}\", exc_info=True)\n\n\
      \    schedule.every(FETCH_INTERVAL_SECONDS).seconds.do(fetch_api_data)\n   \
      \ while True:\n        try: schedule.run_pending()\n        except Exception\
      \ as e: logger.error(f\"Error in scheduler loop: {e}\", exc_info=True)\n   \
      \     time.sleep(1)\n\n# --- Flask App ---\napp = Flask(__name__)\napp.logger.handlers\
      \ = logger.handlers # Use configured logger\napp.logger.setLevel(logger.level)\n\
      CORS(app) # Enable CORS\n\n# --- HTML Content ---\n# Uses Jinja templating for\
      \ configuration injection.\n# IMPORTANT: {% raw %} blocks are critical for CSS\
      \ and JS containing {{ }} syntax.\nHTML_CONTENT = \"\"\"\n<!DOCTYPE html>\n\
      <html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"\
      viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>API\
      \ Metrics Dashboard</title>\n    <script src=\"https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js\"\
      ></script>\n    <link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22\
      \ viewBox=%220 0 100 100%22><path d=%22M8.3 25L41.7 8.3L75 25L41.7 41.7L8.3\
      \ 25Z%22 stroke=%22%2300bcd4%22 stroke-width=%2210%22 fill=%22none%22/><path\
      \ d=%22M8.3 75L41.7 58.3L75 75L41.7 91.7L8.3 75Z%22 stroke=%22%2300bcd4%22 stroke-width=%2210%22\
      \ fill=%22none%22/><path d=%22M8.3 50H75%22 stroke=%22%2300bcd4%22 stroke-width=%2210%22\
      \ fill=%22none%22/></svg>\">\n\n    {# ***** START RAW BLOCK FOR CSS ***** #}\n\
      \    {% raw %}\n    <style>\n        * { box-sizing: border-box; margin: 0;\
      \ padding: 0; }\n        @keyframes fadeIn { from { opacity: 0; } to { opacity:\
      \ 1; } }\n        @keyframes highlight-value {\n            0%, 100% { transform:\
      \ scale(1); color: #fff; }\n            50% { transform: scale(1.05); color:\
      \ #80deea; }\n        }\n        .value-changed .card-value { animation: highlight-value\
      \ 0.4s ease-out; }\n        body { font-family: system-ui, sans-serif; background:\
      \ #181a1f; color: #e0e0e0; line-height: 1.5; display: flex; flex-direction:\
      \ column; min-height: 100vh; overflow-x: hidden; }\n        .app-header { background:\
      \ #21242a; padding: 10px 25px; display: flex; align-items: center; border-bottom:\
      \ 1px solid #3a3d4a; position: sticky; top: 0; z-index: 10; box-shadow: 0 2px\
      \ 8px rgba(0, 0, 0, 0.2); flex-wrap: wrap; }\n        .logo { display: flex;\
      \ align-items: center; margin-right: 20px; color: #00bcd4; }\n        .logo\
      \ svg { margin-right: 8px; }\n        .logo-text-main { font-weight: 700; font-size:\
      \ 1.4em; letter-spacing: 1px; color: #fff;}\n        .logo-text-sub { font-size:\
      \ 0.45em; color: #a0a0a0; text-transform: uppercase; line-height: 1; display:\
      \ block; font-weight: normal; letter-spacing: 0.5px; margin-top: -2px;}\n  \
      \      .main-title { flex-grow: 1; text-align: center; font-size: 1.2em; font-weight:\
      \ 500; color: #c5c5c5; margin: 5px 15px; }\n        .status-indicator { font-size:\
      \ 0.95em; font-weight: 500; margin: 5px 0; text-align: right; min-width: 150px;\
      \ transition: color 0.3s ease; }\n        .status-indicator.live { color: #4caf50;\
      \ }\n        .status-indicator.error { color: #f44336; font-weight: bold; }\n\
      \        .status-indicator.stale { color: #ff9800; }\n        .status-indicator.fetching\
      \ { color: #03a9f4; }\n        .main-content { display: grid; grid-template-columns:\
      \ repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; padding: 25px; flex-grow:\
      \ 1; }\n        .status-card { border-radius: 8px; box-shadow: 0 4px 12px rgba(0,\
      \ 0, 0, 0.25); overflow: hidden; display: flex; flex-direction: column; color:\
      \ #ffffff; border: 1px solid rgba(255, 255, 255, 0.08); animation: fadeIn 0.5s\
      \ ease-out forwards; background-color: #2a2d35; transition: background-color\
      \ 0.3s ease, transform 0.2s ease; }\n        .status-card:hover { transform:\
      \ translateY(-3px); box-shadow: 0 6px 16px rgba(0, 0, 0, 0.3); }\n        .bg-pending\
      \ { background: linear-gradient(135deg, #ffb74d, #ffa726); }\n        .bg-processing\
      \ { background: linear-gradient(135deg, #64b5f6, #42a5f5); }\n        .bg-failed\
      \ { background: linear-gradient(135deg, #e57373, #ef5350); }\n        .bg-processed\
      \ { background: linear-gradient(135deg, #81c784, #66bb6a); }\n        .bg-requests\
      \ { background: linear-gradient(135deg, #7986cb, #5c6bc0); }\n        .bg-cpu\
      \ { background: linear-gradient(135deg, #ba68c8, #ab47bc); }\n        .bg-mem\
      \ { background: linear-gradient(135deg, #4dd0e1, #26c6da); }\n        .bg-uptime\
      \ { background: linear-gradient(135deg, #90a4ae, #78909c); }\n        .card-main-content\
      \ { padding: 15px 20px 20px 20px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);\
      \ text-align: center; flex-grow: 1; display: flex; flex-direction: column; justify-content:\
      \ center; }\n        .card-title { font-size: 0.8em; font-weight: 600; color:\
      \ rgba(255, 255, 255, 0.85); margin-bottom: 10px; text-transform: uppercase;\
      \ letter-spacing: 0.5px; }\n        .card-value { font-size: 2.3em; font-weight:\
      \ 700; line-height: 1; color: #ffffff; display: block; transition: transform\
      \ 0.2s ease; }\n        .charts-section { display: grid; grid-template-columns:\
      \ repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; padding: 0 25px 25px 25px;\
      \ }\n        .chart-card { background: #2a2d35; border-radius: 8px; padding:\
      \ 20px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.25); border: 1px solid rgba(255,\
      \ 255, 255, 0.08); animation: fadeIn 0.6s ease-out forwards; display: flex;\
      \ flex-direction: column; }\n        .chart-title { font-size: 1em; font-weight:\
      \ 600; color: #e0e0e0; margin-bottom: 15px; text-align: center; }\n        .chart-container\
      \ { height: 250px; position: relative; flex-grow: 1; }\n        .chart-container\
      \ canvas { display: block; width: 100%; height: 100%; }\n        .app-footer\
      \ { text-align: center; padding: 15px; margin-top: auto; font-size: 0.85em;\
      \ color: #888; border-top: 1px solid #3a3d4a; background: #1f2128; }\n     \
      \   .app-footer #backend-status { font-weight: 500; display: block; margin-top:\
      \ 5px; transition: color 0.3s ease;}\n        .app-footer #backend-status.error\
      \ { color: #f44336; font-weight: bold; }\n        .app-footer #backend-status.success\
      \ { color: #bdc3c7; }\n        @media (max-width: 768px) {\n            .main-content,\
      \ .charts-section { padding: 15px; gap: 15px; }\n            .app-header { padding:\
      \ 8px 15px; flex-direction: column; align-items: flex-start; }\n           \
      \ .main-title { text-align: left; margin: 8px 0; }\n            .status-indicator\
      \ { align-self: flex-end; margin-top: -25px; }\n            .card-value { font-size:\
      \ 2em; }\n            .charts-section { grid-template-columns: 1fr; }\n    \
      \    }\n        @media (max-width: 480px) {\n             .card-value { font-size:\
      \ 1.8em; }\n             .main-content { grid-template-columns: 1fr 1fr; }\n\
      \         }\n    </style>\n    {% endraw %}\n    {# ***** END RAW BLOCK FOR\
      \ CSS ***** #}\n</head>\n<body>\n    <header class=\"app-header\">\n       \
      \  <div class=\"logo\">\n            <svg width=\"25\" height=\"25\" viewBox=\"\
      0 0 100 100\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"\
      M8.3 25L41.7 8.3L75 25L41.7 41.7L8.3 25Z\" stroke=\"currentColor\" stroke-width=\"\
      10\"/><path d=\"M8.3 75L41.7 58.3L75 75L41.7 91.7L8.3 75Z\" stroke=\"currentColor\"\
      \ stroke-width=\"10\"/><path d=\"M8.3 50H75\" stroke=\"currentColor\" stroke-width=\"\
      10\"/></svg>\n            <div><span class=\"logo-text-main\">API</span><span\
      \ class=\"logo-text-sub\">Metrics Dashboard</span></div>\n        </div>\n \
      \       <h1 class=\"main-title\">Live System Status</h1>\n        <div id=\"\
      status-indicator\" class=\"status-indicator stale\">Initializing...</div>\n\
      \    </header>\n\n    <main class=\"main-content\" id=\"metric-cards\">\n  \
      \      <div class=\"status-card bg-pending\" id=\"card-pending-msgs\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Pending Msgs</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-processing\"\
      \ id=\"card-processing-msgs\"><div class=\"card-main-content\"><div class=\"\
      card-title\">Processing Msgs</div><div class=\"card-value\">--</div></div></div>\n\
      \        <div class=\"status-card bg-failed\" id=\"card-failed-msgs\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Failed Msgs</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-processed\"\
      \ id=\"card-processed-msgs\"><div class=\"card-main-content\"><div class=\"\
      card-title\">Processed Msgs</div><div class=\"card-value\">--</div></div></div>\n\
      \        <div class=\"status-card bg-requests\" id=\"card-total-requests\"><div\
      \ class=\"card-main-content\"><div class=\"card-title\">Total Reqs</div><div\
      \ class=\"card-value\">--</div></div></div>\n        <div class=\"status-card\
      \ bg-cpu\" id=\"card-process-cpu\"><div class=\"card-main-content\"><div class=\"\
      card-title\">Process CPU %</div><div class=\"card-value\">--</div></div></div>\n\
      \        <div class=\"status-card bg-mem\" id=\"card-process-mem\"><div class=\"\
      card-main-content\"><div class=\"card-title\">Process Mem (MB)</div><div class=\"\
      card-value\">--</div></div></div>\n        <div class=\"status-card bg-uptime\"\
      \ id=\"card-uptime\"><div class=\"card-main-content\"><div class=\"card-title\"\
      >Uptime</div><div class=\"card-value\">--</div></div></div>\n    </main>\n\n\
      \    <section class=\"charts-section\">\n        <div class=\"chart-card\">\n\
      \            <div class=\"chart-title\">\U0001F4C8 Requests / Interval (Last\
      \ <span id=\"req-chart-points\">N</span> points)</div>\n            <div class=\"\
      chart-container\"><canvas id=\"requestsChart\"></canvas></div>\n        </div>\n\
      \        <div class=\"chart-card\">\n            <div class=\"chart-title\"\
      >✉️ Message Status (Last <span id=\"msg-chart-points\">N</span> points)</div>\n\
      \            <div class=\"chart-container\"><canvas id=\"messageStatusChart\"\
      ></canvas></div>\n        </div>\n        <div class=\"chart-card\">\n     \
      \       <div class=\"chart-title\">⚙️ Performance (Last <span id=\"perf-chart-points\"\
      >N</span> points)</div>\n            <div class=\"chart-container\"><canvas\
      \ id=\"performanceChart\"></canvas></div>\n        </div>\n        <div class=\"\
      chart-card\">\n            <div class=\"chart-title\">\U0001F6E3️ Requests by\
      \ Route (Current Totals)</div>\n            <div class=\"chart-container\"><canvas\
      \ id=\"requestsByRouteChart\"></canvas></div>\n        </div>\n         <div\
      \ class=\"chart-card\">\n            <div class=\"chart-title\">\U0001F6A6 Requests\
      \ by Status Code (Current Totals)</div>\n            <div class=\"chart-container\"\
      ><canvas id=\"requestsByStatusChart\"></canvas></div>\n        </div>\n    </section>\n\
      \n    <footer class=\"app-footer\">\n        Real-time API Metrics Dashboard\n\
      \        <span id=\"backend-status\" class=\"success\">Initializing...</span>\n\
      \    </footer>\n\n    <script>\n        // --- Injected Variables (Jinja WILL\
      \ process this part) ---\n        const DASHBOARD_DATA_URL = '/api/dashboard_data';\n\
      \        // Use config values injected by Flask\n        const POLLING_INTERVAL_MS\
      \ = {{ FETCH_INTERVAL_SECONDS * 1000 }}; // NOT in raw\n        const MAX_CHART_HISTORY\
      \ = {{ MAX_CHART_HISTORY }};             // NOT in raw\n\n        // --- Start\
      \ Raw Block (Jinja IGNORES the rest) ---\n        {% raw %}\n\n        // ---\
      \ The rest of your original JavaScript logic ---\n        let chartInstances\
      \ = {};\n        let fetchDataIntervalId = null;\n        let lastKnownError\
      \ = null;\n        let statusIndicator = null;\n        let backendStatusSpan\
      \ = null;\n        let cardValueElements = {};\n\n        function cacheDOMElements()\
      \ {\n            statusIndicator = document.getElementById('status-indicator');\n\
      \            backendStatusSpan = document.getElementById('backend-status');\n\
      \            cardValueElements = {\n                pendingMsgs: document.querySelector('#card-pending-msgs\
      \ .card-value'),\n                processingMsgs: document.querySelector('#card-processing-msgs\
      \ .card-value'),\n                failedMsgs: document.querySelector('#card-failed-msgs\
      \ .card-value'),\n                processedMsgs: document.querySelector('#card-processed-msgs\
      \ .card-value'),\n                totalRequests: document.querySelector('#card-total-requests\
      \ .card-value'),\n                processCpu: document.querySelector('#card-process-cpu\
      \ .card-value'),\n                processMem: document.querySelector('#card-process-mem\
      \ .card-value'),\n                uptime: document.querySelector('#card-uptime\
      \ .card-value')\n            };\n             try {\n                // NOTE:\
      \ MAX_CHART_HISTORY is available here because it was defined above\n       \
      \         document.getElementById('req-chart-points').textContent = MAX_CHART_HISTORY;\n\
      \                document.getElementById('msg-chart-points').textContent = MAX_CHART_HISTORY;\n\
      \                document.getElementById('perf-chart-points').textContent =\
      \ MAX_CHART_HISTORY;\n             } catch (e) { console.warn(\"Could not update\
      \ chart point labels:\", e); }\n        }\n\n        function formatNumber(num)\
      \ { return (num === null || num === undefined || isNaN(Number(num))) ? '--'\
      \ : Number(num).toLocaleString(); }\n        function formatPercentage(num)\
      \ { return (num === null || num === undefined || isNaN(Number(num))) ? '--'\
      \ : Number(num).toFixed(1) + '%'; }\n        function formatMemory(num) { return\
      \ (num === null || num === undefined || isNaN(Number(num))) ? '--' : Number(num).toFixed(1)\
      \ + ' MB'; }\n\n        function updateCardValue(element, newValue, formatter\
      \ = formatNumber) {\n            if (!element) return;\n            const formattedValue\
      \ = formatter(newValue);\n            if (element.textContent !== formattedValue)\
      \ {\n                element.textContent = formattedValue;\n               \
      \ const card = element.closest('.status-card');\n                if (card) {\
      \ card.classList.add('value-changed'); setTimeout(() => card.classList.remove('value-changed'),\
      \ 400); }\n            }\n        }\n\n        function generateColors(count)\
      \ {\n             const baseColors = ['#64b5f6', '#81c784', '#ffb74d', '#e57373',\
      \ '#ba68c8', '#4dd0e1', '#fff176', '#7986cb', '#a1887f', '#90a4ae'];\n     \
      \        return Array.from({ length: count }, (_, i) => baseColors[i % baseColors.length]);\n\
      \         }\n\n         function clearCards() { Object.values(cardValueElements).forEach(el\
      \ => { if(el) el.textContent = '--'; }); }\n\n        function updateChartData(chartInstance,\
      \ newLabels = [], newDatasetsData = []) {\n            if (!chartInstance?.data?.datasets)\
      \ return;\n            chartInstance.data.labels = newLabels;\n            chartInstance.data.datasets.forEach((dataset,\
      \ index) => {\n                // Ensure we have data for this dataset, default\
      \ to empty array if not\n                const dataForDataset = (Array.isArray(newDatasetsData)\
      \ && Array.isArray(newDatasetsData[index]))\n                              \
      \         ? newDatasetsData[index] : [];\n                dataset.data = dataForDataset;\n\
      \n                // Regenerate colors for bar/doughnut if data length changes\n\
      \                if ((chartInstance.config.type === 'bar' || chartInstance.config.type\
      \ === 'doughnut') && Array.isArray(dataset.backgroundColor)) {\n           \
      \        dataset.backgroundColor = generateColors(dataForDataset.length);\n\
      \                   // Also handle potential borderColor array for doughnuts\n\
      \                   if (Array.isArray(dataset.borderColor)) {\n            \
      \           dataset.borderColor = dataset.backgroundColor.map(color => color.replace(')',\
      \ ', 0.7)').replace('rgb', 'rgba')); // Example border adjustment\n        \
      \           }\n                }\n            });\n            chartInstance.update('none');\
      \ // Use 'none' for smoother updates without full re-animation\n        }\n\n\
      \        function initializeCharts() {\n             console.log(\"Initializing\
      \ charts...\");\n             Chart.defaults.color = '#e0e0e0';\n          \
      \   Chart.defaults.borderColor = 'rgba(255, 255, 255, 0.1)';\n\n           \
      \  const defaultLineOptions = { responsive: true, maintainAspectRatio: false,\
      \ animation: { duration: 250 }, plugins: { legend: { position: 'bottom', labels:\
      \ { padding: 10, boxWidth: 12, font: { size: 11 } } }, tooltip: { mode: 'index',\
      \ intersect: false, backgroundColor: 'rgba(0,0,0,0.8)' } }, scales: { x: { ticks:\
      \ { maxRotation: 0, autoSkip: true, maxTicksLimit: 10, font: { size: 10 } }\
      \ }, y: { ticks: { beginAtZero: true, font: { size: 10 }, precision: 0 }, grid:\
      \ { color: 'rgba(255, 255, 255, 0.08)' } } }, elements: { line: { tension: 0.2,\
      \ borderWidth: 1.5 }, point: { radius: 0, hitRadius: 10, hoverRadius: 4 } },\
      \ interaction: { mode: 'nearest', axis: 'x', intersect: false } };\n       \
      \      const defaultCategoricalOptions = { responsive: true, maintainAspectRatio:\
      \ false, plugins: { legend: { position: 'bottom', labels: { padding: 10, boxWidth:\
      \ 12, font: { size: 11 } } }, tooltip: { backgroundColor: 'rgba(0,0,0,0.8)'\
      \ } } };\n\n             // Requests Chart (Line)\n             const reqCtx\
      \ = document.getElementById('requestsChart')?.getContext('2d');\n          \
      \   if (reqCtx) chartInstances.requests = new Chart(reqCtx, { type: 'line',\
      \ data: { labels: [], datasets: [{ label: 'Requests/Interval', data: [], borderColor:\
      \ '#64b5f6', backgroundColor: 'rgba(100, 181, 246, 0.2)', fill: true }] }, options:\
      \ JSON.parse(JSON.stringify(defaultLineOptions)) }); else console.error(\"#requestsChart\
      \ not found\");\n\n             // Message Status Chart (Line)\n           \
      \  const msgCtx = document.getElementById('messageStatusChart')?.getContext('2d');\n\
      \             if (msgCtx) { const opts = JSON.parse(JSON.stringify(defaultLineOptions));\
      \ opts.elements.line.borderWidth = 2; chartInstances.messageStatus = new Chart(msgCtx,\
      \ { type: 'line', data: { labels: [], datasets: [ { label: 'Pending', data:\
      \ [], borderColor: '#ffb74d', fill: false }, { label: 'Processing', data: [],\
      \ borderColor: '#64b5f6', fill: false }, { label: 'Failed', data: [], borderColor:\
      \ '#e57373', fill: false }, { label: 'Processed', data: [], borderColor: '#81c784',\
      \ fill: false } ] }, options: opts }); } else console.error(\"#messageStatusChart\
      \ not found\");\n\n             // Performance Chart (Line, Multi-Axis)\n  \
      \           const perfCtx = document.getElementById('performanceChart')?.getContext('2d');\n\
      \             if (perfCtx) { const opts = JSON.parse(JSON.stringify(defaultLineOptions));\
      \ opts.scales.yCpu = { type: 'linear', position: 'left', title: { display: true,\
      \ text: 'CPU (%)', color: '#ba68c8' }, ticks: { color: '#ba68c8', suggestedMax:\
      \ 100, beginAtZero: true, precision: 1 } }; opts.scales.yMem = { type: 'linear',\
      \ position: 'right', title: { display: true, text: 'Memory (MB)', color: '#4dd0e1'\
      \ }, ticks: { color: '#4dd0e1', beginAtZero: true, precision: 1 }, grid: { drawOnChartArea:\
      \ false } }; delete opts.scales.y; chartInstances.performance = new Chart(perfCtx,\
      \ { type: 'line', data: { labels: [], datasets: [ { label: 'Process CPU %',\
      \ data: [], borderColor: '#ba68c8', fill: false, yAxisID: 'yCpu' }, { label:\
      \ 'Process Mem (MB)', data: [], borderColor: '#4dd0e1', fill: false, yAxisID:\
      \ 'yMem' } ] }, options: opts }); } else console.error(\"#performanceChart not\
      \ found\");\n\n             // Requests by Route (Bar)\n             const routeCtx\
      \ = document.getElementById('requestsByRouteChart')?.getContext('2d');\n   \
      \          if (routeCtx) { const opts = JSON.parse(JSON.stringify(defaultCategoricalOptions));\
      \ opts.indexAxis = 'y'; opts.plugins.legend.display = false; opts.scales = {\
      \ x: { ticks: { precision: 0, beginAtZero: true }, grid: { color: 'rgba(255,255,255,0.08)'\
      \ } }, y: { ticks: { font: { size: 10 } }, grid: { display: false } } }; chartInstances.requestsByRoute\
      \ = new Chart(routeCtx, { type: 'bar', data: { labels: [], datasets: [{ label:\
      \ 'Count', data: [], backgroundColor: [] }] }, options: opts }); } else console.error(\"\
      #requestsByRouteChart not found\");\n\n             // Requests by Status (Doughnut)\n\
      \             const statusCtx = document.getElementById('requestsByStatusChart')?.getContext('2d');\n\
      \             if (statusCtx) { const opts = JSON.parse(JSON.stringify(defaultCategoricalOptions));\
      \ opts.plugins.legend.position = 'right'; chartInstances.requestsByStatus =\
      \ new Chart(statusCtx, { type: 'doughnut', data: { labels: [], datasets: [{\
      \ label: 'Count', data: [], backgroundColor: [], borderWidth: 1, hoverOffset:\
      \ 8 }] }, options: opts }); } else console.error(\"#requestsByStatusChart not\
      \ found\");\n             console.log(\"Charts initialized.\");\n         }\n\
      \n        async function fetchData() {\n            if (statusIndicator && !statusIndicator.classList.contains('error'))\
      \ {\n                 statusIndicator.textContent = 'Fetching...'; statusIndicator.className\
      \ = 'status-indicator fetching';\n            }\n            console.debug(`[${new\
      \ Date().toLocaleTimeString()}] Fetching ${DASHBOARD_DATA_URL}`);\n\n      \
      \      try {\n                const response = await fetch(DASHBOARD_DATA_URL);\n\
      \                if (!response.ok) {\n                    let errorMsg = `Error\
      \ fetching dashboard data: ${response.status} ${response.statusText}`;\n   \
      \                 try { const errData = await response.json(); errorMsg += `\
      \ - ${errData.error || JSON.stringify(errData)}`; } catch (e) {}\n         \
      \           throw new Error(errorMsg);\n                }\n                const\
      \ data = await response.json();\n\n                if (data.error) { // Check\
      \ if backend reported an API fetch error\n                    if (lastKnownError\
      \ !== data.error) {\n                        console.error(\"Dashboard backend\
      \ reported API error:\", data.error);\n                        if (statusIndicator)\
      \ { statusIndicator.textContent = 'API Error'; statusIndicator.className = 'status-indicator\
      \ error'; }\n                        if (backendStatusSpan) { backendStatusSpan.textContent\
      \ = `API Error: ${data.error}`; backendStatusSpan.className = 'error'; }\n \
      \                       lastKnownError = data.error;\n                     \
      \   // clearCards(); // Optional: clear cards on backend error\n           \
      \         }\n                } else if (!data.latest_stats || Object.keys(data.latest_stats).length\
      \ === 0) { // Check for empty stats\n                     if (lastKnownError\
      \ !== \"Empty stats data\") {\n                        console.warn(\"Dashboard\
      \ backend returned empty 'latest_stats'.\");\n                        if (statusIndicator)\
      \ { statusIndicator.textContent = 'No Data'; statusIndicator.className = 'status-indicator\
      \ stale'; }\n                        const fetchTime = data.last_successful_fetch\
      \ ? new Date(data.last_successful_fetch).toLocaleString() : 'Never';\n     \
      \                   if (backendStatusSpan) { backendStatusSpan.textContent =\
      \ `No API stats received. Last fetch: ${fetchTime}`; backendStatusSpan.className\
      \ = 'error'; }\n                        lastKnownError = \"Empty stats data\"\
      ;\n                        clearCards();\n                        Object.values(chartInstances).forEach(chart\
      \ => updateChartData(chart)); // Clear charts\n                    }\n     \
      \           } else { // Success Case\n                    if (statusIndicator)\
      \ { statusIndicator.textContent = 'Live'; statusIndicator.className = 'status-indicator\
      \ live'; }\n                    const fetchTime = data.last_successful_fetch\
      \ ? new Date(data.last_successful_fetch).toLocaleString() : 'Never';\n     \
      \               if (backendStatusSpan) { backendStatusSpan.textContent = `Last\
      \ API fetch: ${fetchTime}`; backendStatusSpan.className = 'success'; }\n   \
      \                 updateDashboardUI(data);\n                    lastKnownError\
      \ = null;\n                }\n            } catch (error) { // Error fetching\
      \ from dashboard server itself\n                if (lastKnownError !== error.message)\
      \ {\n                    console.error(\"Error fetching or processing dashboard\
      \ data:\", error);\n                    if (statusIndicator) { statusIndicator.textContent\
      \ = 'Dashboard Error'; statusIndicator.className = 'status-indicator error';\
      \ }\n                    if (backendStatusSpan) { backendStatusSpan.textContent\
      \ = `Dashboard Fetch Error: ${error.message}`; backendStatusSpan.className =\
      \ 'error'; }\n                    lastKnownError = error.message;\n        \
      \            clearCards();\n                    Object.values(chartInstances).forEach(chart\
      \ => updateChartData(chart)); // Clear charts\n                 }\n        \
      \    }\n        }\n\n        function updateDashboardUI(data) {\n          \
      \   // Defend against incomplete data structures\n            if (!data || typeof\
      \ data !== 'object') { console.error(\"updateDashboardUI called with invalid\
      \ data:\", data); return; }\n            const stats = data.latest_stats ||\
      \ {};\n            const history = data.history || {};\n            const timeLabels\
      \ = history.time_labels || [];\n\n            // Update Cards\n            updateCardValue(cardValueElements.pendingMsgs,\
      \ stats.messages_pending);\n            updateCardValue(cardValueElements.processingMsgs,\
      \ stats.messages_processing);\n            updateCardValue(cardValueElements.failedMsgs,\
      \ stats.messages_failed);\n            updateCardValue(cardValueElements.processedMsgs,\
      \ stats.messages_processed);\n            updateCardValue(cardValueElements.totalRequests,\
      \ stats.requests_total);\n            // Access system stats safely\n      \
      \      const systemStats = stats.system || {};\n            updateCardValue(cardValueElements.processCpu,\
      \ systemStats.process_cpu_percent ?? null, formatPercentage);\n            updateCardValue(cardValueElements.processMem,\
      \ systemStats.process_memory_mb ?? null, formatMemory);\n            updateCardValue(cardValueElements.uptime,\
      \ stats.uptime_human, (val) => val || '--');\n\n            // Update Line Charts\
      \ (ensure history data exists)\n            const requestHistory = history.request_history\
      \ || [];\n            const messageStatusHistory = history.message_status ||\
      \ {};\n            const performanceHistory = history.performance || {};\n\n\
      \            updateChartData(chartInstances.requests, timeLabels, [ requestHistory\
      \ ]);\n            updateChartData(chartInstances.messageStatus, timeLabels,\
      \ [\n                messageStatusHistory.pending || [], messageStatusHistory.processing\
      \ || [],\n                messageStatusHistory.failed || [], messageStatusHistory.processed\
      \ || []\n            ]);\n            updateChartData(chartInstances.performance,\
      \ timeLabels, [\n                performanceHistory.cpu || [], performanceHistory.memory\
      \ || []\n            ]);\n\n            // Update Categorical Charts\n     \
      \       if (chartInstances.requestsByRoute) {\n                const routes\
      \ = stats.requests_by_route || {};\n                const routeLabels = Object.keys(routes).sort();\n\
      \                // Ensure routes[r] exists and is an object before using Object.values\n\
      \                const routeData = routeLabels.map(r => typeof routes[r] ===\
      \ 'object' && routes[r] !== null ? Object.values(routes[r]).reduce((s, c) =>\
      \ s + (Number(c) || 0), 0) : 0);\n                updateChartData(chartInstances.requestsByRoute,\
      \ routeLabels, [routeData]);\n            }\n            if (chartInstances.requestsByStatus)\
      \ {\n                const statuses = stats.requests_by_status || {};\n    \
      \            const statusLabels = Object.keys(statuses).sort((a, b) => Number(a)\
      \ - Number(b));\n                const statusData = statusLabels.map(s => statuses[s]\
      \ || 0);\n                updateChartData(chartInstances.requestsByStatus, statusLabels,\
      \ [statusData]);\n            }\n        }\n\n        document.addEventListener('DOMContentLoaded',\
      \ () => {\n            console.log(\"DOM Loaded. Initializing dashboard.\");\n\
      \            cacheDOMElements();\n            initializeCharts();\n        \
      \    clearCards();\n            fetchData(); // Initial fetch\n            if\
      \ (fetchDataIntervalId) clearInterval(fetchDataIntervalId);\n             //\
      \ NOTE: POLLING_INTERVAL_MS is available here because it was defined above\n\
      \            fetchDataIntervalId = setInterval(fetchData, POLLING_INTERVAL_MS);\n\
      \            console.log(`Polling data every ${POLLING_INTERVAL_MS / 1000}s.`);\n\
      \        });\n\n        // --- End Raw Block ---\n        {% endraw %}\n   \
      \ </script>\n</body>\n</html>\n\"\"\"\n\n\n# --- Flask Routes ---\n\n@app.route('/')\n\
      def serve_dashboard():\n    \"\"\"Serve the main dashboard HTML page.\"\"\"\n\
      \    logger.info(\"Serving dashboard HTML page.\")\n    try:\n        # Render\
      \ HTML, injecting Python config into JS template parts\n        return render_template_string(\n\
      \            HTML_CONTENT,\n            FETCH_INTERVAL_SECONDS=FETCH_INTERVAL_SECONDS,\n\
      \            MAX_CHART_HISTORY=MAX_CHART_HISTORY\n        )\n    except Exception\
      \ as e:\n        logger.error(f\"Error rendering dashboard template: {e}\",\
      \ exc_info=True)\n        return f\"<h1>Internal Server Error</h1><p>Failed\
      \ to render template: {e}</p>\", 500\n\n@app.route('/api/dashboard_data')\n\
      def get_dashboard_data():\n    \"\"\"Endpoint for frontend JS to fetch collected\
      \ data.\"\"\"\n    logger.debug(\"Request received for /api/dashboard_data\"\
      )\n    with data_lock:\n        try:\n            # Create snapshot, convert\
      \ deques to lists for JSON\n            data_to_send = {\n                \"\
      latest_stats\": app_state.get(\"latest_stats\", {}).copy(),\n              \
      \  \"history\": {\n                    \"time_labels\": list(app_state.get(\"\
      time_labels\", [])),\n                    \"request_history\": list(app_state.get(\"\
      request_history\", [])),\n                    \"message_status\": {k: list(v)\
      \ for k, v in app_state.get(\"message_status_history\", {}).items()},\n    \
      \                \"performance\": {k: list(v) for k, v in app_state.get(\"performance_history\"\
      , {}).items()}\n                },\n                \"last_successful_fetch\"\
      : app_state.get(\"last_successful_fetch\"),\n                \"error\": app_state.get(\"\
      last_error\") # Pass last known error\n            }\n        except Exception\
      \ as e:\n            logger.error(f\"Error preparing data for /api/dashboard_data:\
      \ {e}\", exc_info=True)\n            return jsonify({\"error\": \"Failed to\
      \ prepare data\", \"detail\": str(e)}), 500\n    return jsonify(data_to_send)\n\
      \n# --- Initialization ---\nif __name__ == '__main__':\n    # Optionally disable\
      \ warnings for insecure HTTPS requests made by this script\n    try:\n     \
      \   import urllib3\n        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\
      \        logger.warning(\"SSL certificate verification disabled for API requests\
      \ by dashboard. INSECURE FOR PRODUCTION.\")\n    except Exception as e: logger.warning(f\"\
      Could not disable urllib3 warnings: {e}\")\n\n    # Start the scheduler thread\n\
      \    scheduler_thread = threading.Thread(target=run_scheduler, name=\"SchedulerThread\"\
      , daemon=True)\n    scheduler_thread.start()\n\n    logger.info(f\"Starting\
      \ Dashboard server on http://0.0.0.0:{DASHBOARD_PORT}\")\n    logger.info(f\"\
      Fetching data from API ({API_BASE_URL}) every {FETCH_INTERVAL_SECONDS} seconds.\"\
      )\n\n    # Run Flask app (use a production server like Waitress/Gunicorn for\
      \ real deployment)\n    try:\n        # from waitress import serve\n       \
      \ # logger.info(\"Starting server with Waitress...\")\n        # serve(app,\
      \ host='0.0.0.0', port=DASHBOARD_PORT)\n        logger.info(\"Using Flask's\
      \ development server (use Waitress/Gunicorn for production).\")\n        app.run(host='0.0.0.0',\
      \ port=DASHBOARD_PORT, debug=False, use_reloader=False)\n    except KeyboardInterrupt:\n\
      \        logger.info(\"Dashboard server stopped.\")\n    except Exception as\
      \ e:\n        logger.critical(f\"Dashboard server failed: {e}\", exc_info=True)"
    tamanho: 0.04 MB
  webdocv1.py:
    caminho_completo: .\webdocv1.py
    classes: []
    functions:
    - docstring: Serves the main HTML documentation page.
      end_lineno: 975
      lineno: 972
      name: serve_documentation
    imports:
    - asname: null
      name: os
    - module: flask
      names:
      - Flask
      - Response
    numero_de_linhas: 982
    source_code: "# doc_server.py\nimport os\nfrom flask import Flask, Response\n\n\
      # --- Configuration ---\nDOC_SERVER_PORT = 8112\nAPI_BASE_URL = \"https://localhost:8777\"\
      \ # The actual base URL of your running API\n\n# --- Flask App ---\napp = Flask(__name__)\n\
      \n# --- HTML Content with Embedded CSS ---\n# Note: Using f-string for easy\
      \ embedding of API_BASE_URL\nHTML_CONTENT = f\"\"\"\n<!DOCTYPE html>\n<html\
      \ lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\"\
      \ content=\"width=device-width, initial-scale=1.0\">\n    <title>\U0001F680\
      \ Message Broker API Documentation</title>\n    <link rel=\"preconnect\" href=\"\
      https://fonts.googleapis.com\">\n    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\"\
      \ crossorigin>\n    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap\"\
      \ rel=\"stylesheet\">\n    <style>\n        :root {{\n            --primary-color:\
      \ #6a11cb;\n            --secondary-color: #2575fc;\n            --card-bg:\
      \ rgba(255, 255, 255, 0.95);\n            --text-color: #333;\n            --heading-color:\
      \ #fff;\n            --code-bg: #f0f2f5;\n            --code-text: #333;\n \
      \           --shadow-light: rgba(0, 0, 0, 0.1);\n            --shadow-medium:\
      \ rgba(0, 0, 0, 0.15);\n            --border-radius: 8px;\n            --success-color:\
      \ #28a745;\n            --error-color: #dc3545;\n            --warning-color:\
      \ #ffc107;\n            --info-color: #17a2b8;\n        }}\n\n        * {{\n\
      \            margin: 0;\n            padding: 0;\n            box-sizing: border-box;\n\
      \        }}\n\n        body {{\n            font-family: 'Poppins', sans-serif;\n\
      \            line-height: 1.7;\n            color: var(--text-color);\n    \
      \        background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color)\
      \ 100%);\n            background-attachment: fixed;\n            padding: 20px;\n\
      \            font-size: 16px;\n        }}\n\n        .container {{\n       \
      \     max-width: 1000px;\n            margin: 20px auto;\n            background-color:\
      \ rgba(255, 255, 255, 0.85);\n            padding: 30px 40px;\n            border-radius:\
      \ var(--border-radius);\n            box-shadow: 0 10px 30px var(--shadow-medium);\n\
      \        }}\n\n        h1, h2, h3 {{\n            margin-bottom: 0.8em;\n  \
      \          color: var(--primary-color);\n            font-weight: 600;\n   \
      \     }}\n\n        h1 {{\n            font-size: 2.5em;\n            text-align:\
      \ center;\n            margin-bottom: 1em;\n            color: var(--secondary-color);\
      \ /* Different color for main title */\n            text-shadow: 1px 1px 2px\
      \ var(--shadow-light);\n        }}\n\n        h2 {{\n            font-size:\
      \ 1.8em;\n            border-bottom: 2px solid var(--secondary-color);\n   \
      \         padding-bottom: 0.3em;\n            margin-top: 1.8em;\n        }}\n\
      \n        h3 {{\n            font-size: 1.3em;\n            margin-top: 1.5em;\n\
      \            color: var(--primary-color);\n        }}\n\n        p {{\n    \
      \        margin-bottom: 1em;\n        }}\n\n        a {{\n            color:\
      \ var(--secondary-color);\n            text-decoration: none;\n        }}\n\n\
      \        a:hover {{\n            text-decoration: underline;\n        }}\n\n\
      \        .card {{\n            background-color: var(--card-bg);\n         \
      \   border-radius: var(--border-radius);\n            padding: 25px;\n     \
      \       margin-bottom: 25px;\n            box-shadow: 0 5px 15px var(--shadow-light);\n\
      \            border-left: 5px solid var(--primary-color);\n            transition:\
      \ transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;\n        }}\n\n \
      \       .card:hover {{\n            transform: translateY(-3px);\n         \
      \   box-shadow: 0 8px 20px var(--shadow-medium);\n        }}\n\n        .endpoint-header\
      \ {{\n            display: flex;\n            align-items: center;\n       \
      \     gap: 15px;\n            margin-bottom: 15px;\n            flex-wrap: wrap;\n\
      \        }}\n\n        .method {{\n            display: inline-block;\n    \
      \        padding: 5px 12px;\n            border-radius: 5px;\n            color:\
      \ #fff;\n            font-weight: 700;\n            font-size: 0.9em;\n    \
      \        text-transform: uppercase;\n        }}\n\n        .method-get {{ background-color:\
      \ #2575fc; }}\n        .method-post {{ background-color: #28a745; }}\n     \
      \   .method-delete {{ background-color: #dc3545; }}\n        .method-put {{\
      \ background-color: #fd7e14; }} /* Example if needed */\n        .method-patch\
      \ {{ background-color: #ffc107; }} /* Example if needed */\n\n        .endpoint-path\
      \ {{\n            font-family: 'Courier New', Courier, monospace;\n        \
      \    font-weight: 600;\n            font-size: 1.1em;\n            color: var(--primary-color);\n\
      \            word-break: break-all;\n            background-color: var(--code-bg);\n\
      \            padding: 3px 6px;\n            border-radius: 4px;\n        }}\n\
      \n        pre {{\n            background-color: var(--code-bg);\n          \
      \  color: var(--code-text);\n            padding: 15px;\n            border-radius:\
      \ var(--border-radius);\n            overflow-x: auto;\n            margin:\
      \ 15px 0;\n            font-size: 0.95em;\n            border: 1px solid #ddd;\n\
      \        }}\n\n        code {{\n            font-family: 'Courier New', Courier,\
      \ monospace;\n        }}\n\n        .details-section {{\n            margin-top:\
      \ 15px;\n            padding-left: 10px;\n            border-left: 3px solid\
      \ #eee;\n        }}\n\n        .details-section strong {{\n            display:\
      \ block;\n            margin-bottom: 5px;\n            color: var(--primary-color);\n\
      \        }}\n\n        .details-section ul {{\n            list-style: none;\n\
      \            padding-left: 0;\n        }}\n\n         .details-section ul li\
      \ {{\n            margin-bottom: 5px;\n            font-size: 0.95em;\n    \
      \     }}\n\n        .badge {{\n            display: inline-block;\n        \
      \    padding: 3px 8px;\n            border-radius: 4px;\n            font-size:\
      \ 0.85em;\n            font-weight: 600;\n            margin-left: 5px;\n  \
      \      }}\n\n        .badge-auth {{ background-color: var(--warning-color);\
      \ color: #333; }}\n        .badge-success {{ background-color: var(--success-color);\
      \ color: white; }}\n        .badge-error {{ background-color: var(--error-color);\
      \ color: white; }}\n        .badge-ratelimit {{ background-color: var(--info-color);\
      \ color: white; }}\n\n        /* Footer */\n        footer {{\n            text-align:\
      \ center;\n            margin-top: 40px;\n            padding-top: 20px;\n \
      \           border-top: 1px solid #ccc;\n            color: #555;\n        \
      \    font-size: 0.9em;\n        }}\n\n    </style>\n</head>\n<body>\n    <div\
      \ class=\"container\">\n        <h1>\U0001F680 Message Broker API Documentation</h1>\n\
      \n        <section id=\"introduction\">\n            <h2>\U0001F44B Introduction</h2>\n\
      \            <p>Welcome to the documentation for the Message Broker API V3 (Async/SQLite).\
      \ This API allows you to manage message queues and publish/consume messages\
      \ asynchronously.</p>\n            <p><strong>Base URL:</strong> <code class=\"\
      endpoint-path\">{API_BASE_URL}</code></p>\n            <p><strong>Authentication:</strong>\
      \ Most endpoints require authentication using JSON Web Tokens (JWT). Obtain\
      \ a token via the <code>/login</code> endpoint and include it in the <code>Authorization</code>\
      \ header of subsequent requests as <code>Bearer <your_access_token></code>.</p>\n\
      \            <p><strong>Content Type:</strong> All request bodies should be\
      \ JSON (<code>Content-Type: application/json</code>). Responses are also in\
      \ JSON format.</p>\n            <p><strong>Tools:</strong> You can interact\
      \ with this API using tools like <code>curl</code>, Postman, Insomnia, or programmatically\
      \ via HTTP clients in your preferred language.</p>\n        </section>\n\n \
      \       <section id=\"authentication\">\n            <h2>\U0001F511 Authentication</h2>\n\
      \n            <div class=\"card\">\n                <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-post\">POST</span>\n    \
      \                <span class=\"endpoint-path\">/login</span>\n             \
      \   </div>\n                <p>Authenticates a user and returns JWT access and\
      \ refresh tokens.</p>\n                <div class=\"details-section\">\n   \
      \                 <strong>Authentication:</strong> None required.\n        \
      \        </div>\n                <div class=\"details-section\">\n         \
      \           <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >10 per minute</span>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Request Body:</strong>\n   \
      \                 <pre><code>{{\n    \"username\": \"your_username\", // e.g.,\
      \ \"admin\"\n    \"password\": \"your_password\"  // e.g., \"admin\"\n}}</code></pre>\n\
      \                </div>\n                <div class=\"details-section\">\n \
      \                   <strong>Success Response (200 OK):</strong>\n          \
      \          <pre><code>{{\n    \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\
      ,\n    \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\n}}</code></pre>\n\
      \                </div>\n                <div class=\"details-section\">\n \
      \                   <strong>Error Responses:</strong>\n                    <ul>\n\
      \                        <li><span class=\"badge badge-error\">400</span> Bad\
      \ Request: Invalid payload format.</li>\n                        <li><span class=\"\
      badge badge-error\">401</span> Unauthorized: Invalid username or password.</li>\n\
      \                    </ul>\n                </div>\n            </div>\n\n \
      \           <div class=\"card\">\n                <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-post\">POST</span>\n    \
      \                <span class=\"endpoint-path\">/refresh</span>\n           \
      \     </div>\n                <p>Generates a new access token using a valid\
      \ refresh token.</p>\n                 <div class=\"details-section\">\n   \
      \                 <strong>Authentication:</strong> <span class=\"badge badge-auth\"\
      >JWT Refresh Token Required</span> (Provide refresh token in <code>Authorization:\
      \ Bearer <refresh_token></code> header).\n                </div>\n         \
      \        <div class=\"details-section\">\n                    <strong>Success\
      \ Response (200 OK):</strong>\n                    <pre><code>{{\n    \"access_token\"\
      : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\" // New access token\n}}</code></pre>\n\
      \                </div>\n                 <div class=\"details-section\">\n\
      \                    <strong>Error Responses:</strong>\n                   \
      \ <ul>\n                        <li><span class=\"badge badge-error\">401</span>\
      \ Unauthorized: Invalid or expired refresh token.</li>\n                   \
      \ </ul>\n                </div>\n            </div>\n        </section>\n\n\
      \        <section id=\"queues\">\n            <h2>\U0001F4E5 Queue Management</h2>\n\
      \n            <div class=\"card\">\n                <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-post\">POST</span>\n    \
      \                <span class=\"endpoint-path\">/queues</span>\n            \
      \    </div>\n                <p>Creates a new message queue.</p>\n         \
      \        <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >60 per minute</span>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Request Body:</strong>\n   \
      \                 <pre><code>{{\n    \"name\": \"my-new-queue-name\" // String,\
      \ 1-255 chars, pattern: ^[a-zA-Z0-9_-]+$\n}}</code></pre>\n                </div>\n\
      \                <div class=\"details-section\">\n                    <strong>Success\
      \ Response (201 Created):</strong>\n                    <pre><code>{{\n    \"\
      msg\": \"Queue created\",\n    \"name\": \"my-new-queue-name\",\n    \"id\"\
      : 123 // Generated queue ID\n}}</code></pre>\n                </div>\n     \
      \            <div class=\"details-section\">\n                    <strong>Error\
      \ Responses:</strong>\n                    <ul>\n                        <li><span\
      \ class=\"badge badge-error\">400</span> Bad Request: Invalid payload (e.g.,\
      \ name format).</li>\n                        <li><span class=\"badge badge-error\"\
      >401</span> Unauthorized: Missing/Invalid JWT token.</li>\n                \
      \        <li><span class=\"badge badge-error\">409</span> Conflict: Queue name\
      \ already exists.</li>\n                        <li><span class=\"badge badge-error\"\
      >500</span> Internal Server Error: Database error.</li>\n                  \
      \  </ul>\n                </div>\n            </div>\n\n            <div class=\"\
      card\">\n                <div class=\"endpoint-header\">\n                 \
      \   <span class=\"method method-get\">GET</span>\n                    <span\
      \ class=\"endpoint-path\">/queues</span>\n                </div>\n         \
      \       <p>Lists all existing message queues.</p>\n                 <div class=\"\
      details-section\">\n                    <strong>Authentication:</strong> <span\
      \ class=\"badge badge-auth\">JWT Access Token Required</span>\n            \
      \    </div>\n                <div class=\"details-section\">\n             \
      \       <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\">100\
      \ per minute</span>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Success Response (200 OK):</strong>\n\
      \                    <pre><code>[\n    {{\n        \"id\": 1,\n        \"name\"\
      : \"email-notifications\",\n        \"created_at\": \"2025-04-03T10:00:00Z\"\
      \n    }},\n    {{\n        \"id\": 2,\n        \"name\": \"image-processing\"\
      ,\n        \"created_at\": \"2025-04-03T11:30:00Z\"\n    }}\n    // ... more\
      \ queues\n]</code></pre>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Error Responses:</strong>\n\
      \                    <ul>\n                         <li><span class=\"badge\
      \ badge-error\">401</span> Unauthorized: Missing/Invalid JWT token.</li>\n \
      \                        <li><span class=\"badge badge-error\">500</span> Internal\
      \ Server Error: Database error.</li>\n                    </ul>\n          \
      \      </div>\n            </div>\n\n            <div class=\"card\">\n    \
      \            <div class=\"endpoint-header\">\n                    <span class=\"\
      method method-get\">GET</span>\n                    <span class=\"endpoint-path\"\
      >/queues/{'{queue_name}'}</span>\n                </div>\n                <p>Retrieves\
      \ details for a specific queue, including the count of pending messages.</p>\n\
      \                 <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >100 per minute</span>\n                </div>\n                <div class=\"\
      details-section\">\n                    <strong>URL Parameters:</strong>\n \
      \                   <ul>\n                        <li><code>queue_name</code>\
      \ (string): The name of the queue to retrieve.</li>\n                    </ul>\n\
      \                </div>\n                 <div class=\"details-section\">\n\
      \                    <strong>Success Response (200 OK):</strong>\n         \
      \           <pre><code>{{\n    \"id\": 1,\n    \"name\": \"email-notifications\"\
      ,\n    \"created_at\": \"2025-04-03T10:00:00Z\",\n    \"pending_messages\":\
      \ 42\n}}</code></pre>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Error Responses:</strong>\n\
      \                    <ul>\n                        <li><span class=\"badge badge-error\"\
      >401</span> Unauthorized: Missing/Invalid JWT token.</li>\n                \
      \        <li><span class=\"badge badge-error\">404</span> Not Found: The specified\
      \ queue name does not exist.</li>\n                        <li><span class=\"\
      badge badge-error\">500</span> Internal Server Error: Database error.</li>\n\
      \                    </ul>\n                </div>\n            </div>\n\n \
      \           <div class=\"card\">\n                <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-delete\">DELETE</span>\n\
      \                    <span class=\"endpoint-path\">/queues/{'{queue_name}'}</span>\n\
      \                </div>\n                <p>Deletes a specific queue and all\
      \ messages within it.</p>\n                 <div class=\"details-section\">\n\
      \                    <strong>Authentication:</strong> <span class=\"badge badge-auth\"\
      >JWT Access Token Required</span>\n                </div>\n                <div\
      \ class=\"details-section\">\n                    <strong>Rate Limit:</strong>\
      \ <span class=\"badge badge-ratelimit\">30 per minute</span>\n             \
      \   </div>\n                 <div class=\"details-section\">\n             \
      \       <strong>URL Parameters:</strong>\n                    <ul>\n       \
      \                 <li><code>queue_name</code> (string): The name of the queue\
      \ to delete.</li>\n                    </ul>\n                </div>\n     \
      \            <div class=\"details-section\">\n                    <strong>Success\
      \ Response (200 OK):</strong>\n                    <pre><code>{{\n    \"msg\"\
      : \"Queue 'queue-to-delete' deleted\"\n}}</code></pre>\n                </div>\n\
      \                 <div class=\"details-section\">\n                    <strong>Error\
      \ Responses:</strong>\n                    <ul>\n                        <li><span\
      \ class=\"badge badge-error\">401</span> Unauthorized: Missing/Invalid JWT token.</li>\n\
      \                        <li><span class=\"badge badge-error\">404</span> Not\
      \ Found: The specified queue name does not exist.</li>\n                   \
      \     <li><span class=\"badge badge-error\">500</span> Internal Server Error:\
      \ Database error.</li>\n                    </ul>\n                </div>\n\
      \            </div>\n        </section>\n\n        <section id=\"messages\"\
      >\n            <h2>✉️ Message Handling</h2>\n\n             <div class=\"card\"\
      >\n                <div class=\"endpoint-header\">\n                    <span\
      \ class=\"method method-post\">POST</span>\n                    <span class=\"\
      endpoint-path\">/queues/{'{queue_name}'}/messages</span>\n                </div>\n\
      \                <p>Publishes a new message to the specified queue.</p>\n  \
      \               <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >500 per minute</span>\n                </div>\n                <div class=\"\
      details-section\">\n                    <strong>URL Parameters:</strong>\n \
      \                   <ul>\n                        <li><code>queue_name</code>\
      \ (string): The name of the target queue.</li>\n                    </ul>\n\
      \                </div>\n                 <div class=\"details-section\">\n\
      \                    <strong>Request Body:</strong>\n                    <pre><code>{{\n\
      \    \"content\": {{ // Can be JSON object, string, or array\n        \"user_id\"\
      : 123,\n        \"task\": \"send_welcome_email\",\n        \"template\": \"\
      welcome_v1\"\n    }}\n}}</code></pre>\n                    <pre><code>{{\n \
      \   \"content\": \"Simple string message\"\n}}</code></pre>\n              \
      \      <pre><code>{{\n    \"content\": [\"item1\", \"item2\", 123]\n}}</code></pre>\n\
      \                </div>\n                <div class=\"details-section\">\n \
      \                   <strong>Success Response (201 Created):</strong>\n     \
      \               <pre><code>{{\n    \"msg\": \"Message published\",\n    \"message_id\"\
      : 5678 // Generated message ID\n}}</code></pre>\n                </div>\n  \
      \               <div class=\"details-section\">\n                    <strong>Error\
      \ Responses:</strong>\n                    <ul>\n                        <li><span\
      \ class=\"badge badge-error\">400</span> Bad Request: Invalid payload format.</li>\n\
      \                        <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                        <li><span class=\"\
      badge badge-error\">404</span> Not Found: The specified queue name does not\
      \ exist.</li>\n                        <li><span class=\"badge badge-error\"\
      >500</span> Internal Server Error: Database error.</li>\n                  \
      \  </ul>\n                </div>\n                 <div class=\"details-section\"\
      >\n                     <strong>Note:</strong> On success, an SSE event is published\
      \ to the queue's channel.\n                 </div>\n            </div>\n\n \
      \            <div class=\"card\">\n                <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-get\">GET</span>\n      \
      \              <span class=\"endpoint-path\">/queues/{'{queue_name}'}/messages</span>\n\
      \                </div>\n                <p>Consumes the oldest pending message\
      \ from the specified queue. Marks the message as 'processing'.</p>\n       \
      \          <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >200 per minute</span>\n                </div>\n                <div class=\"\
      details-section\">\n                    <strong>URL Parameters:</strong>\n \
      \                   <ul>\n                        <li><code>queue_name</code>\
      \ (string): The name of the queue to consume from.</li>\n                  \
      \  </ul>\n                </div>\n                 <div class=\"details-section\"\
      >\n                    <strong>Success Response (200 OK):</strong>\n       \
      \             <pre><code>{{\n    \"message_id\": 5678,\n    \"queue\": \"email-notifications\"\
      ,\n    \"content\": {{ // The original message content\n        \"user_id\"\
      : 123,\n        \"task\": \"send_welcome_email\",\n        \"template\": \"\
      welcome_v1\"\n    }},\n    \"status\": \"processing\",\n    \"retrieved_at\"\
      : \"2025-04-03T12:00:00Z\"\n}}</code></pre>\n                </div>\n      \
      \           <div class=\"details-section\">\n                    <strong>Success\
      \ Response (204 No Content):</strong>\n                    <ul>\n          \
      \               <li>Returned when there are no pending messages in the queue.\
      \ The response body is empty.</li>\n                    </ul>\n            \
      \     </div>\n                 <div class=\"details-section\">\n           \
      \         <strong>Error Responses:</strong>\n                    <ul>\n    \
      \                    <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                        <li><span class=\"\
      badge badge-error\">404</span> Not Found: The specified queue name does not\
      \ exist.</li>\n                        <li><span class=\"badge badge-error\"\
      >500</span> Internal Server Error: Database error during consumption.</li>\n\
      \                    </ul>\n                 </div>\n                  <div\
      \ class=\"details-section\">\n                     <strong>Important:</strong>\
      \ After successfully processing the message content received here, you <strong>must</strong>\
      \ call the ACK (<code>/messages/{'{message_id}'}/ack</code>) or NACK (<code>/messages/{'{message_id}'}/nack</code>)\
      \ endpoint. Failure to do so will leave the message in the 'processing' state\
      \ indefinitely.\n                 </div>\n            </div>\n\n           \
      \  <div class=\"card\">\n                <div class=\"endpoint-header\">\n \
      \                   <span class=\"method method-post\">POST</span>\n       \
      \             <span class=\"endpoint-path\">/messages/{'{message_id}'}/ack</span>\n\
      \                </div>\n                <p>✅ Acknowledges successful processing\
      \ of a message. Marks the message as 'processed'.</p>\n                 <div\
      \ class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >200 per minute</span>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>URL Parameters:</strong>\n \
      \                   <ul>\n                        <li><code>message_id</code>\
      \ (integer): The ID of the message to acknowledge.</li>\n                  \
      \  </ul>\n                </div>\n                <div class=\"details-section\"\
      >\n                    <strong>Request Body:</strong> None.\n              \
      \  </div>\n                 <div class=\"details-section\">\n              \
      \      <strong>Success Response (200 OK):</strong>\n                    <pre><code>{{\n\
      \    \"msg\": \"Message 5678 acknowledged\"\n}}</code></pre>\n             \
      \   </div>\n                 <div class=\"details-section\">\n             \
      \       <strong>Error Responses:</strong>\n                    <ul>\n      \
      \                  <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                        <li><span class=\"\
      badge badge-error\">403</span> Forbidden: You are not the consumer who retrieved\
      \ this message.</li>\n                        <li><span class=\"badge badge-error\"\
      >404</span> Not Found: The specified message ID does not exist.</li>\n     \
      \                   <li><span class=\"badge badge-error\">409</span> Conflict:\
      \ The message is not in the 'processing' state.</li>\n                     \
      \   <li><span class=\"badge badge-error\">500</span> Internal Server Error:\
      \ Database error.</li>\n                    </ul>\n                </div>\n\
      \                 <div class=\"details-section\">\n                     <strong>Note:</strong>\
      \ On success, an SSE event is published to the message's queue channel.\n  \
      \               </div>\n            </div>\n\n             <div class=\"card\"\
      >\n                <div class=\"endpoint-header\">\n                    <span\
      \ class=\"method method-post\">POST</span>\n                    <span class=\"\
      endpoint-path\">/messages/{'{message_id}'}/nack</span>\n                </div>\n\
      \                <p>❌ Negatively acknowledges a message, indicating processing\
      \ failed. Marks the message as 'failed'. (No automatic retry or DLQ implemented\
      \ in this version).</p>\n                 <div class=\"details-section\">\n\
      \                    <strong>Authentication:</strong> <span class=\"badge badge-auth\"\
      >JWT Access Token Required</span>\n                </div>\n                <div\
      \ class=\"details-section\">\n                    <strong>Rate Limit:</strong>\
      \ <span class=\"badge badge-ratelimit\">200 per minute</span>\n            \
      \    </div>\n                 <div class=\"details-section\">\n            \
      \        <strong>URL Parameters:</strong>\n                    <ul>\n      \
      \                  <li><code>message_id</code> (integer): The ID of the message\
      \ to NACK.</li>\n                    </ul>\n                </div>\n       \
      \          <div class=\"details-section\">\n                    <strong>Request\
      \ Body:</strong> None. (Optionally could accept a 'reason').\n             \
      \   </div>\n                 <div class=\"details-section\">\n             \
      \       <strong>Success Response (200 OK):</strong>\n                    <pre><code>{{\n\
      \    \"msg\": \"Message 5678 marked as failed (NACK)\"\n}}</code></pre>\n  \
      \              </div>\n                 <div class=\"details-section\">\n  \
      \                  <strong>Error Responses:</strong>\n                    <ul>\n\
      \                        <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                        <li><span class=\"\
      badge badge-error\">403</span> Forbidden: You are not the consumer who retrieved\
      \ this message.</li>\n                        <li><span class=\"badge badge-error\"\
      >404</span> Not Found: The specified message ID does not exist.</li>\n     \
      \                   <li><span class=\"badge badge-error\">409</span> Conflict:\
      \ The message is not in the 'processing' state.</li>\n                     \
      \   <li><span class=\"badge badge-error\">500</span> Internal Server Error:\
      \ Database error.</li>\n                    </ul>\n                </div>\n\
      \                 <div class=\"details-section\">\n                     <strong>Note:</strong>\
      \ On success, an SSE event is published to the message's queue channel.\n  \
      \               </div>\n            </div>\n        </section>\n\n        <section\
      \ id=\"stats\">\n            <h2>\U0001F4CA Statistics</h2>\n            <div\
      \ class=\"card\">\n                <div class=\"endpoint-header\">\n       \
      \             <span class=\"method method-get\">GET</span>\n               \
      \     <span class=\"endpoint-path\">/stats</span>\n                </div>\n\
      \                <p>Retrieves detailed statistics about the broker and the system\
      \ it's running on.</p>\n                <div class=\"details-section\">\n  \
      \                  <strong>Authentication:</strong> <span class=\"badge badge-auth\"\
      >JWT Access Token Required</span>\n                </div>\n                <div\
      \ class=\"details-section\">\n                    <strong>Rate Limit:</strong>\
      \ <span class=\"badge badge-ratelimit\">30 per minute</span>\n             \
      \   </div>\n                <div class=\"details-section\">\n              \
      \      <strong>Success Response (200 OK):</strong>\n                    <pre><code>{{\n\
      \    \"start_time\": \"2025-04-03T09:00:00Z\",\n    \"requests_total\": 1500,\n\
      \    \"requests_by_route\": {{\n        \"/queues\": {{ \"GET\": 500, \"POST\"\
      : 100 }},\n        \"/queues/<string:queue_name>/messages\": {{ \"GET\": 400,\
      \ \"POST\": 450 }},\n        // ... other routes\n    }},\n    \"requests_by_status\"\
      : {{\n        \"200\": 1200, \"201\": 100, \"204\": 50, \"404\": 100, \"401\"\
      : 50\n    }},\n    \"queues_total\": 5,\n    \"messages_total\": 10000,\n  \
      \  \"messages_pending\": 2000,\n    \"messages_processing\": 50,\n    \"messages_processed\"\
      : 7800,\n    \"messages_failed\": 150,\n    \"last_error\": null, // or \"DB\
      \ Stats Update Failed: 2025-04-03T13:00:00Z\"\n    \"system\": {{\n        \"\
      python_version\": \"3.10.x\",\n        \"platform\": \"Linux\", // or \"Windows\"\
      , \"Darwin\"\n        \"platform_release\": \"5.15.0-...\",\n        \"architecture\"\
      : \"x86_64\",\n        \"cpu_percent\": 15.5,\n        \"memory_total_gb\":\
      \ 15.6,\n        \"memory_available_gb\": 8.2,\n        \"memory_used_gb\":\
      \ 7.4,\n        \"memory_percent\": 47.4,\n        \"disk_usage\": {{\n    \
      \        \"/\": {{ \"total_gb\": 99.5, \"used_gb\": 40.2, \"free_gb\": 59.3,\
      \ \"percent\": 40.4 }}\n            // ... other mounted disks\n        }},\n\
      \        \"process_memory_mb\": 120.5,\n        \"process_cpu_percent\": 2.1\n\
      \    }},\n    \"broker_specific\": {{\n        \"db_engine\": \"sqlite (aiosqlite)\"\
      ,\n        \"auth_method\": \"jwt (access+refresh)\",\n        \"notification\"\
      : \"sse (redis)\",\n        \"rate_limit\": \"redis\"\n    }},\n    \"uptime_seconds\"\
      : 36000.5,\n    \"uptime_human\": \"10:00:00\"\n}}</code></pre>\n          \
      \      </div>\n                 <div class=\"details-section\">\n          \
      \          <strong>Error Responses:</strong>\n                    <ul>\n   \
      \                     <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                        <li><span class=\"\
      badge badge-error\">500</span> Internal Server Error: If stats collection fails\
      \ badly.</li>\n                    </ul>\n                </div>\n         \
      \   </div>\n        </section>\n\n         <section id=\"logs\">\n         \
      \   <h2>\U0001F4C4 Log Viewing</h2>\n             <p>These endpoints allow viewing\
      \ the JSON log files generated by the broker.</p>\n\n            <div class=\"\
      card\">\n                <div class=\"endpoint-header\">\n                 \
      \   <span class=\"method method-get\">GET</span>\n                    <span\
      \ class=\"endpoint-path\">/logs</span>\n                </div>\n           \
      \     <p>Lists the available JSON log files, sorted newest first.</p>\n    \
      \            <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ <span class=\"badge badge-auth\">JWT Access Token Required</span>\n      \
      \          </div>\n                <div class=\"details-section\">\n       \
      \             <strong>Rate Limit:</strong> <span class=\"badge badge-ratelimit\"\
      >10 per minute</span>\n                </div>\n                <div class=\"\
      details-section\">\n                    <strong>Success Response (200 OK):</strong>\n\
      \                    <pre><code>{{\n    \"log_files\": [\n        \"broker_log_20250403_140000_abcdef12.json\"\
      ,\n        \"broker_log_20250403_130000_fedcba98.json\"\n        // ... other\
      \ log files\n    ]\n}}</code></pre>\n                </div>\n              \
      \   <div class=\"details-section\">\n                    <strong>Error Responses:</strong>\n\
      \                    <ul>\n                        <li><span class=\"badge badge-error\"\
      >401</span> Unauthorized: Missing/Invalid JWT token.</li>\n                \
      \        <li><span class=\"badge badge-error\">500</span> Internal Server Error:\
      \ Error reading log directory.</li>\n                    </ul>\n           \
      \     </div>\n            </div>\n\n             <div class=\"card\">\n    \
      \            <div class=\"endpoint-header\">\n                    <span class=\"\
      method method-get\">GET</span>\n                    <span class=\"endpoint-path\"\
      >/logs/{'{filename}'}</span>\n                </div>\n                <p>Retrieves\
      \ the content of a specific log file.</p>\n                <div class=\"details-section\"\
      >\n                    <strong>Authentication:</strong> <span class=\"badge\
      \ badge-auth\">JWT Access Token Required</span>\n                </div>\n  \
      \              <div class=\"details-section\">\n                    <strong>Rate\
      \ Limit:</strong> <span class=\"badge badge-ratelimit\">60 per minute</span>\n\
      \                </div>\n                 <div class=\"details-section\">\n\
      \                    <strong>URL Parameters:</strong>\n                    <ul>\n\
      \                        <li><code>filename</code> (string): The exact name\
      \ of the JSON log file to retrieve (e.g., <code>broker_log_20250403_140000_abcdef12.json</code>).</li>\n\
      \                    </ul>\n                </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Query Parameters (Optional):</strong>\n\
      \                    <ul>\n                        <li><code>start</code> (integer):\
      \ Line number to start reading from (1-based index).</li>\n                \
      \        <li><code>end</code> (integer): Line number to stop reading at (inclusive).</li>\n\
      \                        <li><code>tail</code> (integer): Retrieve only the\
      \ last N lines. (Takes precedence over start/end if provided).</li>\n      \
      \              </ul>\n                 </div>\n                 <div class=\"\
      details-section\">\n                    <strong>Success Response (200 OK):</strong>\
      \ Returns a JSON array, where each element is a parsed JSON log entry from a\
      \ line in the file.\n                    <pre><code>[\n    {{ // Log Entry 1\n\
      \        \"timestamp\": \"2025-04-03T14:00:01.123Z\",\n        \"level\": \"\
      INFO\",\n        \"name\": \"MessageBrokerV3\",\n        \"pid\": 12345,\n \
      \       \"thread\": \"MainThread\",\n        \"message\": \"\U0001F680 Initializing\
      \ Flask Application...\",\n        \"icon_type\": \"INFO\"\n    }},\n    {{\
      \ // Log Entry 2\n        \"timestamp\": \"2025-04-03T14:00:05.456Z\",\n   \
      \     \"level\": \"ERROR\",\n        \"name\": \"MessageBrokerV3\",\n      \
      \  \"pid\": 12345,\n        \"thread\": \"Thread-2\",\n        \"message\":\
      \ \"Database error consuming message from 'image-processing' (SQLite): database\
      \ is locked\",\n        \"icon_type\": \"DB\",\n        \"exception\": \"Traceback\
      \ (most recent call last):...\",\n        \"traceback\": [ \"...\" ] // Full\
      \ traceback array if exception occurred\n    }}\n    // ... more log entries\n\
      ]</code></pre>\n                 <p>If a line in the log file is not valid JSON,\
      \ it will be represented as:</p>\n                 <pre><code>{{\n    \"_error\"\
      : \"Invalid JSON\",\n    \"_line\": 15, // The line number where the error occurred\n\
      \    \"_raw\": \"This was not json {{ \"maybe\" }}\n}}\n</code></pre>\n    \
      \            </div>\n                 <div class=\"details-section\">\n    \
      \                <strong>Error Responses:</strong>\n                    <ul>\n\
      \                        <li><span class=\"badge badge-error\">400</span> Bad\
      \ Request: Invalid filename.</li>\n                        <li><span class=\"\
      badge badge-error\">401</span> Unauthorized: Missing/Invalid JWT token.</li>\n\
      \                        <li><span class=\"badge badge-error\">404</span> Not\
      \ Found: The specified log file does not exist.</li>\n                     \
      \   <li><span class=\"badge badge-error\">500</span> Internal Server Error:\
      \ Error reading the log file.</li>\n                    </ul>\n            \
      \    </div>\n            </div>\n        </section>\n\n         <section id=\"\
      graphql\">\n            <h2>\U0001F347 GraphQL API</h2>\n\n            <div\
      \ class=\"card\">\n                <div class=\"endpoint-header\">\n       \
      \             <span class=\"method method-post\">POST</span> / <span class=\"\
      method method-get\">GET</span>\n                    <span class=\"endpoint-path\"\
      >/graphql</span>\n                </div>\n                <p>Provides a GraphQL\
      \ endpoint for querying queues and messages.</p>\n                 <div class=\"\
      details-section\">\n                    <strong>Authentication:</strong> <span\
      \ class=\"badge badge-auth\">JWT Access Token Required</span>\n            \
      \    </div>\n                <div class=\"details-section\">\n             \
      \       <strong>Interface:</strong> Supports GET requests for introspection\
      \ and POST requests for queries/mutations (though only queries are defined here).\
      \ Accessing this endpoint in a browser typically shows the GraphiQL interface\
      \ for interactive exploration.\n                </div>\n                 <div\
      \ class=\"details-section\">\n                     <strong>Available Queries:</strong>\n\
      \                     <ul>\n                        <li><code>allQueues</code>:\
      \ Retrieves a list of all queues (supports pagination/sorting via Relay connections).</li>\n\
      \                        <li><code>queueByName(name: String!)</code>: Retrieves\
      \ a single queue by its exact name.</li>\n                        <li><code>messagesInQueue(queueName:\
      \ String!, status: String, limit: Int)</code>: Retrieves messages for a specific\
      \ queue, optionally filtering by status ('pending', 'processing', 'processed',\
      \ 'failed') and limiting the result count (default 100).</li>\n            \
      \         </ul>\n                 </div>\n                <div class=\"details-section\"\
      >\n                    <strong>Example GraphQL Query (POST Request Body):</strong>\n\
      \                    <pre><code>{{\n    \"query\": \\\"\\\"\\\"\n        query\
      \ {{\n          q1: queueByName(name: \"email-notifications\") {{\n        \
      \    id\n            name\n            createdAt\n          }}\n          pendingMessages:\
      \ messagesInQueue(queueName: \"image-processing\", status: \"pending\", limit:\
      \ 5) {{\n            edges {{\n              node {{\n                id\n \
      \               status\n                createdAt\n                content\n\
      \              }}\n            }}\n          }}\n        }}\n    \\\"\\\"\\\"\
      \n}}</code></pre>\n                </div>\n                <div class=\"details-section\"\
      >\n                    <strong>Success Response (200 OK):</strong> The structure\
      \ mirrors the GraphQL query.\n                    <pre><code>{{\n    \"data\"\
      : {{\n        \"q1\": {{\n            \"id\": \"UXVldWVPYmplY3Q6MQ==\", // Base64\
      \ encoded Relay ID\n            \"name\": \"email-notifications\",\n       \
      \     \"createdAt\": \"2025-04-03T10:00:00+00:00\"\n        }},\n        \"\
      pendingMessages\": {{\n             \"edges\": [\n                {{ \"node\"\
      : {{ \"id\": \"TWVzc2FnZU9iamVjdDo1...\", \"status\": \"pending\", ... }} }},\n\
      \                {{ \"node\": {{ \"id\": \"TWVzc2FnZU9iamVjdDo2...\", \"status\"\
      : \"pending\", ... }} }}\n             ]\n        }}\n    }}\n}}</code></pre>\n\
      \                </div>\n                 <div class=\"details-section\">\n\
      \                    <strong>Error Responses:</strong> GraphQL has its own error\
      \ reporting format within the JSON response, typically under an \"errors\" key.\
      \ HTTP status is usually 200 even if the query fails, unless there's an authentication\
      \ issue (401) or server error (500).\n                    <ul>\n           \
      \             <li><span class=\"badge badge-error\">401</span> Unauthorized:\
      \ Missing/Invalid JWT token.</li>\n                    </ul>\n             \
      \    </div>\n            </div>\n        </section>\n\n        <section id=\"\
      sse\">\n            <h2>\U0001F4E1 Server-Sent Events (SSE)</h2>\n\n       \
      \     <div class=\"card\">\n                 <div class=\"endpoint-header\"\
      >\n                    <span class=\"method method-get\">GET</span>\n      \
      \              <span class=\"endpoint-path\">/stream</span>\n              \
      \   </div>\n                 <p>Establishes a Server-Sent Events connection\
      \ to receive real-time notifications about message events.</p>\n           \
      \       <div class=\"details-section\">\n                    <strong>Authentication:</strong>\
      \ None required for the stream connection itself (consider adding JWT query\
      \ param auth if needed for specific use cases).\n                  </div>\n\
      \                  <div class=\"details-section\">\n                    <strong>How\
      \ it Works:</strong>\n                    <ul>\n                        <li>Clients\
      \ connect to this endpoint using the standard <code>EventSource</code> API in\
      \ JavaScript (or equivalent in other languages).</li>\n                    \
      \    <li>The server keeps this connection open and pushes events as they happen.</li>\n\
      \                        <li>Events are published by the server when:\n    \
      \                        <ul>\n                                <li>A new message\
      \ is published (event type: <code>message</code>, channel: <code>queue_name</code>).</li>\n\
      \                                <li>A message is acknowledged (event type:\
      \ <code>message</code>, channel: <code>queue_name</code>).</li>\n          \
      \                      <li>A message is NACKed (event type: <code>message</code>,\
      \ channel: <code>queue_name</code>).</li>\n                            </ul>\n\
      \                        </li>\n                        <li>The <code>channel</code>\
      \ parameter in the SSE URL (e.g., <code>/stream?channel=my-queue</code>) is\
      \ used by the <em>client</em> library (like the official `flask-sse` JS client)\
      \ to filter messages client-side if needed, but the Python backend currently\
      \ publishes events with the queue name embedded in the data and tagged with\
      \ the queue name as the channel type. You might need a specific client library\
      \ or custom JS to listen only to specific queue channels effectively based on\
      \ the pushed data or type.</li>\n                    </ul>\n               \
      \   </div>\n                  <div class=\"details-section\">\n            \
      \          <strong>Example JavaScript Client:</strong>\n                   \
      \   <pre><code>// Assuming you want events for the 'email-notifications' queue\n\
      const eventSource = new EventSource(\"{API_BASE_URL}/stream\"); // Connect to\
      \ the main stream\n\neventSource.onmessage = function(event) {{\n    console.log(\"\
      Raw message received:\", event.data);\n    try {{\n        const data = JSON.parse(event.data);\n\
      \n        // Check if the message is for the queue we care about\n        if\
      \ (data.queue === \"email-notifications\") {{\n            console.log(`Event\
      \ for email-notifications:`, data);\n            // Handle the event (e.g.,\
      \ update UI)\n            if (data.event === \"new_message\") {{\n         \
      \       console.log(`New message ${'{data.message_id}'} published.`);\n    \
      \        }} else if (data.event === \"message_acked\") {{\n                \
      \ console.log(`Message ${'{data.message_id}'} acknowledged.`);\n           \
      \ }} else if (data.event === \"message_nacked\") {{\n                 console.log(`Message\
      \ ${'{data.message_id}'} failed.`);\n            }}\n        }}\n    }} catch\
      \ (e) {{\n        console.error(\"Failed to parse SSE data:\", e);\n    }}\n\
      }};\n\neventSource.onerror = function(err) {{\n    console.error(\"EventSource\
      \ failed:\", err);\n    // Handle errors, maybe attempt reconnection\n}};\n\n\
      // To close the connection:\n// eventSource.close();\n</code></pre>\n      \
      \            </div>\n                  <div class=\"details-section\">\n   \
      \                 <strong>Event Data Format:</strong> The data field of each\
      \ SSE message is a JSON string like:\n                    <pre><code>{{\n  \
      \  \"queue\": \"queue_name\",      // Name of the queue the event pertains to\n\
      \    \"message_id\": 12345,        // ID of the relevant message\n    \"event\"\
      : \"new_message\"      // Type of event (\"new_message\", \"message_acked\"\
      , \"message_nacked\")\n}}</code></pre>\n                  </div>\n         \
      \   </div>\n        </section>\n\n        <footer>\n            Message Broker\
      \ API V3 Docs - Served by Flask\n        </footer>\n    </div>\n</body>\n</html>\n\
      \"\"\"\n\n@app.route('/')\ndef serve_documentation():\n    \"\"\"Serves the\
      \ main HTML documentation page.\"\"\"\n    # Use Response object for explicit\
      \ content type and status\n    return Response(HTML_CONTENT, mimetype='text/html',\
      \ status=200)\n\nif __name__ == '__main__':\n    print(f\" * Starting documentation\
      \ server on http://localhost:{DOC_SERVER_PORT}\")\n    # Use waitress or gunicorn\
      \ in production instead of Flask's development server\n    # For simplicity\
      \ here, we use the built-in server.\n    # Use host='0.0.0.0' to make it accessible\
      \ from other devices on the network\n    app.run(host='0.0.0.0', port=DOC_SERVER_PORT,\
      \ debug=False)"
    tamanho: 0.04 MB
.\certs_v3:
  cert.pem:
    caminho_completo: .\certs_v3\cert.pem
    numero_de_linhas: 21
    tamanho: 0.00 MB
  key_nopass.pem:
    caminho_completo: .\certs_v3\key_nopass.pem
    numero_de_linhas: 28
    tamanho: 0.00 MB
.\databases:
  message_broker_v3.db:
    caminho_completo: .\databases\message_broker_v3.db
    numero_de_linhas: -1
    sqlite_info:
      messages:
        columns:
        - !!python/tuple
          - id
          - INTEGER
        - !!python/tuple
          - queue_id
          - INTEGER
        - !!python/tuple
          - content
          - JSON
        - !!python/tuple
          - status
          - VARCHAR(50)
        - !!python/tuple
          - created_at
          - DATETIME
        - !!python/tuple
          - processed_at
          - DATETIME
        - !!python/tuple
          - consumer_id
          - VARCHAR(255)
        - !!python/tuple
          - updated_at
          - DATETIME
        rows:
        - !!python/tuple
          - 1
          - 1
          - hello world
          - pending
          - '2025-04-03 04:19:34.913039+00:00'
          - null
          - null
          - '2025-04-03 04:19:34.913039+00:00'
        - !!python/tuple
          - 2
          - 2
          - 'Performance Test Msg #1 (2025-04-03T01:21:38.755527)'
          - pending
          - '2025-04-03 04:21:40.810530+00:00'
          - null
          - null
          - '2025-04-03 04:21:40.810530+00:00'
        - !!python/tuple
          - 3
          - 2
          - 'Performance Test Msg #2 (2025-04-03T01:21:40.921526)'
          - pending
          - '2025-04-03 04:21:42.981144+00:00'
          - null
          - null
          - '2025-04-03 04:21:42.981144+00:00'
        - !!python/tuple
          - 4
          - 2
          - 'Performance Test Msg #3 (2025-04-03T01:21:43.092140)'
          - pending
          - '2025-04-03 04:21:45.153592+00:00'
          - null
          - null
          - '2025-04-03 04:21:45.153592+00:00'
        - !!python/tuple
          - 5
          - 2
          - 'Performance Test Msg #4 (2025-04-03T01:21:45.260588)'
          - pending
          - '2025-04-03 04:21:47.317485+00:00'
          - null
          - null
          - '2025-04-03 04:21:47.317485+00:00'
      queues:
        columns:
        - !!python/tuple
          - id
          - INTEGER
        - !!python/tuple
          - name
          - VARCHAR(255)
        - !!python/tuple
          - created_at
          - DATETIME
        - !!python/tuple
          - updated_at
          - DATETIME
        rows:
        - !!python/tuple
          - 1
          - minha-fila-teste
          - '2025-04-03 04:18:14.283280+00:00'
          - '2025-04-03 04:18:14.283280+00:00'
        - !!python/tuple
          - 2
          - minha-fila-teste-perf
          - '2025-04-03 04:21:38.246755+00:00'
          - '2025-04-03 04:21:38.246755+00:00'
        - !!python/tuple
          - 3
          - minha-fila-teste-stress
          - '2025-04-03 04:25:41.874533+00:00'
          - '2025-04-03 04:25:41.874533+00:00'
    tamanho: 23.06 MB
  message_broker_v3.db-shm:
    caminho_completo: .\databases\message_broker_v3.db-shm
    numero_de_linhas: -1
    tamanho: -0.00 MB
  message_broker_v3.db-wal:
    caminho_completo: .\databases\message_broker_v3.db-wal
    numero_de_linhas: -1
    tamanho: -0.00 MB
.\logs_v3:
  broker_log_20250403_023313_f153a3a3.json:
    caminho_completo: .\logs_v3\broker_log_20250403_023313_f153a3a3.json
    json_info:
      numero_de_linhas: 150298
      tamanho: 50.88 MB
    numero_de_linhas: 150298
    tamanho: 50.88 MB
.\test-json-data-collector-validation: {}
